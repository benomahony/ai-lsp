{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"4a41b40a96e5444ea105a5b04890e708"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"84068"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a794644d9b6edde49dfd799164ee4","spanId":"5d39c62de80e6d5f","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762971239641474000","endTimeUnixNano":"1762971239641474000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"77"}}],"status":{}},{"traceId":"019a79464696ae8ac54cf80a48966bf1","spanId":"0911b51f31ed32fd","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762971240085992000","endTimeUnixNano":"1762971240085992000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"115"}}],"status":{}},{"traceId":"019a794646af7322ca6d0470c8cbf161","spanId":"5ab69ae7a44ef971","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762971240111188000","endTimeUnixNano":"1762971240111188000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79464a9b94e1672695f2ab86afb1","spanId":"b71aeb1b7bf4d0db","parentSpanId":"aa38013c77d7efc1","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762971241116096000","endTimeUnixNano":"1762971241116096000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7946500c88b20b1d8c951012caa7","spanId":"9de8269d252a452d","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762971242508076000","endTimeUnixNano":"1762971242508076000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79464a9b94e1672695f2ab86afb1","spanId":"aa38013c77d7efc1","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762971241115616000","endTimeUnixNano":"1762971242515718000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762971242511840000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79465014e80e7a31a138a0a89069","spanId":"94e71703d5fdf578","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762971242516719000","endTimeUnixNano":"1762971242516719000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7946509e9181b8f530b28f5e5163","spanId":"2225a4d22a632f17","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762971242654758000","endTimeUnixNano":"1762971242654758000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7946509fbe131632c392672b5f8b","spanId":"a88c6c2ec3092ba4","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762971242655051000","endTimeUnixNano":"1762971242655051000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a794651ed07e3b6035246fd48d528","spanId":"e9d80909cae35b14","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762971242989864000","endTimeUnixNano":"1762971242989864000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a794651eedc2d7701ea9433b436f7","spanId":"6f987651fcd96c16","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762971242990220000","endTimeUnixNano":"1762971242990220000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a794654870b69b0ab35edf90e4767","spanId":"1f675a1b4079b79c","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762971243655241000","endTimeUnixNano":"1762971243655241000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a794654871f2b5f5d42bad0697597","spanId":"bfe304446bf34735","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762971243655579000","endTimeUnixNano":"1762971243655579000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79464a9b94e1672695f2ab86afb1","spanId":"a1b08c28befc4aac","parentSpanId":"6a627c700d1408cf","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762971241118964000","endTimeUnixNano":"1762971242508544000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79464a9b94e1672695f2ab86afb1","spanId":"6a627c700d1408cf","parentSpanId":"aa38013c77d7efc1","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762971241117073000","endTimeUnixNano":"1762971242508735000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"4a41b40a96e5444ea105a5b04890e708"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"84068"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a794658708d0e64ba1ed92eb4480c","spanId":"e49416ced173a42b","parentSpanId":"56ea6310c854b7a3","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762971244657449000","endTimeUnixNano":"1762971244657449000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7946588d67664405e1137ce1c5f0","spanId":"45f6902f1f53ed38","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762971244685865000","endTimeUnixNano":"1762971244685865000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a794658708d0e64ba1ed92eb4480c","spanId":"56ea6310c854b7a3","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762971244656885000","endTimeUnixNano":"1762971244694184000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"d6ae5738e90564b59bad5ea502d5749f68a582be0f2c9b9bde5866723629d7bc"}}],"events":[{"timeUnixNano":"1762971244690741000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        req, traces=traces, timeout=real_timeout\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<7 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1268, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohappyeyeballs/impl.py\", line 87, in start_connection\n    sock, _, _ = await _staggered.staggered_race(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<13 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohappyeyeballs/_staggered.py\", line 165, in staggered_race\n    done = await _wait_one(\n           ^^^^^^^^^^^^^^^^\n        (*tasks, start_next) if start_next else tasks, loop\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohappyeyeballs/_staggered.py\", line 46, in _wait_one\n    return await wait_next\n           ^^^^^^^^^^^^^^^\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a794658966dd14bd1a3c0dcb2f3af","spanId":"e55f874603b05a2b","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762971244694386000","endTimeUnixNano":"1762971244694386000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a794658a63e65ca564d618d98187d","spanId":"ce3dd7fd0877a169","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762971244710061000","endTimeUnixNano":"1762971244710061000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a794658a63f93df524c5ddf11f06c","spanId":"fed60874f3017c4e","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762971244710307000","endTimeUnixNano":"1762971244710307000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79465c8e3b7b4f44a5b8eea497b3","spanId":"210f3cdb7a61ff92","parentSpanId":"df05a48cf566b422","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762971245710826000","endTimeUnixNano":"1762971245710826000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a794658708d0e64ba1ed92eb4480c","spanId":"b89dfd0f0273f96b","parentSpanId":"1019305335e5c47d","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762971244659621000","endTimeUnixNano":"1762971244686492000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a794658708d0e64ba1ed92eb4480c","spanId":"1019305335e5c47d","parentSpanId":"56ea6310c854b7a3","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762971244658802000","endTimeUnixNano":"1762971244686754000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\n        \\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"4a41b40a96e5444ea105a5b04890e708"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"84068"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79465c8e3b7b4f44a5b8eea497b3","spanId":"3ecfd3addd37fa9d","parentSpanId":"df6280551e39e8ba","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762971245711730000","endTimeUnixNano":"1762971266679019000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4882"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"347"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"3310"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4882"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_d6a1e992ee63478ba348e0003f75fb8b\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"warning\", \"message\": \"Repeatedly calling `split('\\\\n')` on a growing string slice within a loop to calculate line and column numbers is inefficient for large documents or many snippet occurrences. This leads to quadratic time complexity in the worst case for text processing. Pre-calculating line start offsets or iterating line-by-line would be more performant to avoid redundant work.\", \"issue_snippet\": \"text[:start_pos].split(\\\"\\\\n\\\")\"}, {\"issue_snippet\": \"positions[0]\", \"severity\": \"info\", \"message\": \"The `find_snippet_in_text` method correctly identifies all occurrences of an `issue_snippet`, but only the first occurrence (`positions[0]`) is used to generate a diagnostic. If the same semantic issue appears multiple times in a document, only the first instance will be flagged by the AI LSP, potentially missing other relevant issues for the user. Consider iterating over all `positions` to generate diagnostics for every occurrence.\"}, {\"message\": \"Accessing `server.lsp.diagnostics` relies on an internal, potentially private, attribute of the `pygls` LanguageServer implementation. This approach is fragile and might break with future updates to `pygls`, coupling your code tightly to its internal structure. It's generally safer to interact with LSP features through documented public APIs or to manage diagnostic state more explicitly within `AILanguageServer` if direct access is unavoidable.\", \"issue_snippet\": \"getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\", \"severity\": \"warning\"}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79465c8e3b7b4f44a5b8eea497b3","spanId":"df6280551e39e8ba","parentSpanId":"df05a48cf566b422","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762971245711310000","endTimeUnixNano":"1762971266689977000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"text[:start_pos].split(\\\"\\\\n\\\")\", \"severity\": \"warning\", \"message\": \"Repeatedly calling `split('\\\\n')` on a growing string slice within a loop to calculate line and column numbers is inefficient for large documents or many snippet occurrences. This leads to quadratic time complexity in the worst case for text processing. Pre-calculating line start offsets or iterating line-by-line would be more performant to avoid redundant work.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"positions[0]\", \"severity\": \"info\", \"message\": \"The `find_snippet_in_text` method correctly identifies all occurrences of an `issue_snippet`, but only the first occurrence (`positions[0]`) is used to generate a diagnostic. If the same semantic issue appears multiple times in a document, only the first instance will be flagged by the AI LSP, potentially missing other relevant issues for the user. Consider iterating over all `positions` to generate diagnostics for every occurrence.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\", \"severity\": \"warning\", \"message\": \"Accessing `server.lsp.diagnostics` relies on an internal, potentially private, attribute of the `pygls` LanguageServer implementation. This approach is fragile and might break with future updates to `pygls`, coupling your code tightly to its internal structure. It's generally safer to interact with LSP features through documented public APIs or to manage diagnostic state more explicitly within `AILanguageServer` if direct access is unavoidable.\", \"suggested_fixes\": null}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4882"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"347"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"3310"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4882"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_d6a1e992ee63478ba348e0003f75fb8b\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"warning\", \"message\": \"Repeatedly calling `split('\\\\n')` on a growing string slice within a loop to calculate line and column numbers is inefficient for large documents or many snippet occurrences. This leads to quadratic time complexity in the worst case for text processing. Pre-calculating line start offsets or iterating line-by-line would be more performant to avoid redundant work.\", \"issue_snippet\": \"text[:start_pos].split(\\\"\\\\n\\\")\"}, {\"issue_snippet\": \"positions[0]\", \"severity\": \"info\", \"message\": \"The `find_snippet_in_text` method correctly identifies all occurrences of an `issue_snippet`, but only the first occurrence (`positions[0]`) is used to generate a diagnostic. If the same semantic issue appears multiple times in a document, only the first instance will be flagged by the AI LSP, potentially missing other relevant issues for the user. Consider iterating over all `positions` to generate diagnostics for every occurrence.\"}, {\"message\": \"Accessing `server.lsp.diagnostics` relies on an internal, potentially private, attribute of the `pygls` LanguageServer implementation. This approach is fragile and might break with future updates to `pygls`, coupling your code tightly to its internal structure. It's generally safer to interact with LSP features through documented public APIs or to manage diagnostic state more explicitly within `AILanguageServer` if direct access is unavoidable.\", \"issue_snippet\": \"getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\", \"severity\": \"warning\"}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_d6a1e992ee63478ba348e0003f75fb8b\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79465c8e3b7b4f44a5b8eea497b3","spanId":"b4daeae84f4d507b","parentSpanId":"df05a48cf566b422","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762971266699744000","endTimeUnixNano":"1762971266699744000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 3 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"3"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79465c8e3b7b4f44a5b8eea497b3","spanId":"9ee1a4068ccd0e57","parentSpanId":"df05a48cf566b422","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762971266702317000","endTimeUnixNano":"1762971266702317000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'text[:start_pos].spl...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"text[:start_pos].spl"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79465c8e3b7b4f44a5b8eea497b3","spanId":"bb78514918f1331a","parentSpanId":"df05a48cf566b422","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762971266702733000","endTimeUnixNano":"1762971266702733000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 3 occurrences of 'positions[0]...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"3"}},{"key":"snippet[:20]","value":{"stringValue":"positions[0]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79465c8e3b7b4f44a5b8eea497b3","spanId":"1e84950b0fdf1ef1","parentSpanId":"df05a48cf566b422","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762971266703007000","endTimeUnixNano":"1762971266703007000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 2 occurrences of 'getattr(server.lsp, ...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"2"}},{"key":"snippet[:20]","value":{"stringValue":"getattr(server.lsp, "}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79465c8e3b7b4f44a5b8eea497b3","spanId":"df05a48cf566b422","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762971245710588000","endTimeUnixNano":"1762971266703140000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a7946ae94206b44af565d03b4950e","spanId":"df0d27c80fc1fb4d","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762971266708640000","endTimeUnixNano":"1762971266708640000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 3 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"3"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"035681d08ccb448eaf20f77e9e2c5347"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"93334"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79772185e88760f062713e46e542","spanId":"20ba7a579ff6a901","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762974441861141000","endTimeUnixNano":"1762974441861141000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"77"}}],"status":{}},{"traceId":"019a797723b028de245ac511c4ba0c57","spanId":"c87c14fdbe684b56","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762974442416087000","endTimeUnixNano":"1762974442416087000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"115"}}],"status":{}},{"traceId":"019a797723cf10607d61fef513d867a8","spanId":"98d6edb7c5c6452d","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762974442447358000","endTimeUnixNano":"1762974442447358000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a797727bd9d53891d7c107f460c11","spanId":"fa442aab7daad255","parentSpanId":"8e94a4a11e08bb5f","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762974443454552000","endTimeUnixNano":"1762974443454552000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"035681d08ccb448eaf20f77e9e2c5347"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"93334"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79778332f3f91449c56012ef21b7","spanId":"fba342a263955875","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762974466866727000","endTimeUnixNano":"1762974466866727000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a797727bd9d53891d7c107f460c11","spanId":"8e94a4a11e08bb5f","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762974443453064000","endTimeUnixNano":"1762974466880992000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762974466871909000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79778342b193ef49ea74de2ebf5a","spanId":"689ff3c34a381535","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762974466882024000","endTimeUnixNano":"1762974466882024000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7977871c9ed8dad684c699b0034a","spanId":"1c88cccf7321bbc8","parentSpanId":"417a54ffd0eeb365","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762974467869116000","endTimeUnixNano":"1762974467869116000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a797727bd9d53891d7c107f460c11","spanId":"21c909c119429a2c","parentSpanId":"96faf400c81a9d20","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762974443456842000","endTimeUnixNano":"1762974466867335000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a797727bd9d53891d7c107f460c11","spanId":"96faf400c81a9d20","parentSpanId":"8e94a4a11e08bb5f","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762974443455737000","endTimeUnixNano":"1762974466867577000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"035681d08ccb448eaf20f77e9e2c5347"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"93334"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7977871c9ed8dad684c699b0034a","spanId":"417a54ffd0eeb365","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762974467868594000","endTimeUnixNano":"1762974481742128000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a7977871c9ed8dad684c699b0034a","spanId":"b387caade76856f7","parentSpanId":"417a54ffd0eeb365","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762974467870189000","endTimeUnixNano":"1762974481742352000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a7977871c9ed8dad684c699b0034a","spanId":"255d1fca2af65e18","parentSpanId":"b387caade76856f7","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762974467871170000","endTimeUnixNano":"1762974481742591000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"a410cd2d65084a57911648777d4de572"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"99136"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7978be4ac12d8fe8b6defccb29c6","spanId":"f0ffe9e821d49ad4","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762974547530171000","endTimeUnixNano":"1762974547530171000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"77"}}],"status":{}},{"traceId":"019a7978bfda37cb406b0ebad9f96985","spanId":"9cfac55b83c6b307","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762974547930556000","endTimeUnixNano":"1762974547930556000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"115"}}],"status":{}},{"traceId":"019a7978bfed956ea68ef96d865e51f0","spanId":"7bb7ee885ba817f3","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762974547949945000","endTimeUnixNano":"1762974547949945000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/.config/nvim/lua/config/keymaps.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/config/keymaps.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7978c3d84001e3b0e570fb463448","spanId":"8557ed0a0b94a1e2","parentSpanId":"85d0ab07cab90a2d","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762974548952826000","endTimeUnixNano":"1762974548952826000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/config/keymaps.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/config/keymaps.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7978c3d84001e3b0e570fb463448","spanId":"85d0ab07cab90a2d","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762974548952401000","endTimeUnixNano":"1762974549254479000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/config/keymaps.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a7978c3d84001e3b0e570fb463448","spanId":"5b2d909f854e33f7","parentSpanId":"85d0ab07cab90a2d","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762974548953338000","endTimeUnixNano":"1762974549254561000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a7978c3d84001e3b0e570fb463448","spanId":"44b9a2f7b7d99527","parentSpanId":"5b2d909f854e33f7","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762974548953843000","endTimeUnixNano":"1762974549254593000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"a8808ef98af74577b449d2cb7ef32c77"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"5317"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7978e18e84738aa00634db29cf53","spanId":"f44ae5c14f8d4d7d","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762974556558930000","endTimeUnixNano":"1762974556558930000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"77"}}],"status":{}},{"traceId":"019a7978e2e5975ce940737fad8dd13f","spanId":"c89cab7361d7e2f4","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762974556901952000","endTimeUnixNano":"1762974556901952000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"115"}}],"status":{}},{"traceId":"019a7978e2f975b12a9d85a53c68a329","spanId":"45b37f07b396f283","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762974556921452000","endTimeUnixNano":"1762974556921452000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"f6e71058edd527ca","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762974557924993000","endTimeUnixNano":"1762974557924993000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"a8808ef98af74577b449d2cb7ef32c77"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"5317"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"16342131937e7fa4","parentSpanId":"db5ae19c3fb9a61f","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762974557926430000","endTimeUnixNano":"1762974607190206000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"9365"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"1854"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"7914"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"9365"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal get_buffer_globals = function()\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_a076092a98f949a7a3fdf876818b46b9\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"suggested_fixes\": [{\"title\": \"Escape user command arguments for 'UVAddPackage'.\", \"target_snippet\": \"opts.args\", \"replacement_snippet\": \"vim.fn.shellescape(opts.args)\"}, {\"replacement_snippet\": \"vim.fn.shellescape(opts.args)\", \"target_snippet\": \"opts.args\", \"title\": \"Escape user command arguments for 'UVRemovePackage'.\"}], \"message\": \"User input from `opts.args` in user commands (`UVAddPackage`, `UVRemovePackage`) is directly concatenated into shell commands without being escaped. A malicious user could inject arbitrary shell commands (e.g., `my_package; rm -rf /`), leading to a critical remote code execution vulnerability.\", \"severity\": \"error\", \"issue_snippet\": \"opts.args\"}, {\"suggested_fixes\": [{\"replacement_snippet\": \"local actual_cmd = cmd:gsub(\\\"%[(.-)%]\\\", vim.fn.shellescape(input))\", \"title\": \"Escape user input for Snacks picker commands.\", \"target_snippet\": \"local actual_cmd = cmd:gsub(\\\"%[(.-)%]\\\", input)\"}, {\"title\": \"Escape user input for Telescope picker commands.\", \"target_snippet\": \"local cmd = selection.cmd .. input\", \"replacement_snippet\": \"local cmd = selection.cmd .. vim.fn.shellescape(input)\"}, {\"replacement_snippet\": \"require('uv').run_command('uv add ' .. vim.fn.shellescape(input))\", \"title\": \"Escape user input for 'add package' keymap.\", \"target_snippet\": \"require('uv').run_command('uv add ' .. input)\"}, {\"title\": \"Escape user input for 'remove package' keymap.\", \"target_snippet\": \"require('uv').run_command('uv remove ' .. input)\", \"replacement_snippet\": \"require('uv').run_command('uv remove ' .. vim.fn.shellescape(input))\"}], \"severity\": \"error\", \"issue_snippet\": \"input\", \"message\": \"User input obtained via `vim.ui.input` (e.g., for adding/removing packages from pickers or keymaps) is directly concatenated into shell commands without being escaped. This is a critical remote code execution vulnerability, as an attacker can inject malicious shell commands (e.g., `my_package; rm -rf /`). This pattern is repeated in Snacks picker, Telescope picker, and direct keymaps for `uv add` and `uv remove`.\"}, {\"message\": \"The `source` command is executed in a subshell via `vim.fn.jobstart`. The `source` command is a shell built-in that modifies the *current* shell's environment. Running it in a subshell means its effects on environment variables are confined to that subshell and do not propagate back to the Neovim process. While `vim.env.VIRTUAL_ENV` and `vim.env.PATH` are manually updated, this might not fully replicate all environment changes typically made by a complete virtual environment activation script, potentially leading to an incomplete setup for subsequent Python commands.\", \"severity\": \"warning\", \"issue_snippet\": \"local command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\", \"suggested_fixes\": null}, {\"message\": \"The logic to extract global variables (`get_buffer_globals`) relies on simplistic regex matching and indentation checks. It might inaccurately classify local variables within top-level functions or complex assignments as globals, especially in non-trivial Python codebases. This can lead to `NameError` or unexpected behavior when the selected Python code is executed in isolation, as its dependencies might not be correctly provided. A more robust solution would involve proper Python AST parsing.\", \"issue_snippet\": \"if not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\", \"suggested_fixes\": null, \"severity\": \"warning\"}, {\"issue_snippet\": \"print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\", \"severity\": \"warning\", \"message\": \"When automatically wrapping selected expressions for printing, inserting the expression directly into an f-string can lead to `SyntaxError` if the expression itself contains f-string syntax (e.g., `{}`) or unescaped quotes. A safer approach is to assign the expression to a temporary variable and then print its `str()` representation.\", \"suggested_fixes\": [{\"replacement_snippet\": \"local cleaned_selection = selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\")\\nfile:write(\\\"_uv_expr_result = (\\\" .. cleaned_selection .. \\\")\\\\n\\\")\\nfile:write('print(\\\"Expression result: \\\" + str(_uv_expr_result))\\\\n')\", \"target_snippet\": \"file:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\", \"title\": \"Use a temporary variable and `str()` for safer expression printing.\"}]}, {\"severity\": \"warning\", \"message\": \"The logic for `M.run_python_selection` wraps all indented selected code in a new function (`def run_selection():`). However, if the selected code itself is an indented function or class definition (e.g., a method inside a class), this wrapping will result in a `SyntaxError` (e.g., `def` nested at the same level) or incorrect execution context. The checks for `is_function_def` and `is_class_def` should explicitly prevent this wrapping.\", \"suggested_fixes\": [{\"target_snippet\": \"if is_all_indented then\", \"title\": \"Avoid redundant function wrapping for indented function/class definitions.\", \"replacement_snippet\": \"if is_all_indented and not is_function_def and not is_class_def then\"}], \"issue_snippet\": \"if is_all_indented then\"}, {\"message\": \"The plugin creates temporary Python files for executing selections and functions in `~/.cache/nvim/uv_run`. There is no explicit mechanism to clean up these temporary files after execution, which can lead to accumulation and unnecessary disk space usage over time.\", \"suggested_fixes\": null, \"issue_snippet\": \"local temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\", \"severity\": \"info\"}, {\"severity\": \"info\", \"issue_snippet\": \"_G.run_command = M.run_command\", \"message\": \"Exposing `M.run_command` globally as `_G.run_command` pollutes the global namespace. While the comment suggests it 'can be removed if not needed', its presence can lead to naming conflicts with other plugins or scripts and makes the module less encapsulated. It is generally better to explicitly require the module to access its functions.\", \"suggested_fixes\": [{\"replacement_snippet\": \"-- _G.run_command = M.run_command -- Removed global exposure to prevent namespace pollution\", \"title\": \"Remove global exposure if not strictly necessary.\", \"target_snippet\": \"_G.run_command = M.run_command\"}]}, {\"issue_snippet\": \"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\"\", \"severity\": \"info\", \"message\": \"When running a specific Python function (`M.run_python_function`), the temporary script imports the entire current Python file as a module. This means all top-level code in that module will be executed upon import, including `if __name__ == \\\"__main__\\\":` blocks, global variable assignments, and any other statements outside of function definitions. This can lead to unintended side effects, inefficient execution, or incorrect behavior if the module's top-level code relies on a specific execution context.\", \"suggested_fixes\": null}, {\"message\": \"The logic for integrating with `Snacks` and `Telescope` pickers contains significant duplication in defining command items and the `on_select`/`confirm` logic. This violates the DRY (Don't Repeat Yourself) principle, making the code harder to maintain and update. Refactoring into shared data structures and helper functions would improve maintainability.\", \"severity\": \"info\", \"issue_snippet\": \"finder = function()\", \"suggested_fixes\": null}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"db5ae19c3fb9a61f","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762974557925728000","endTimeUnixNano":"1762974607207268000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"opts.args\", \"severity\": \"error\", \"message\": \"User input from `opts.args` in user commands (`UVAddPackage`, `UVRemovePackage`) is directly concatenated into shell commands without being escaped. A malicious user could inject arbitrary shell commands (e.g., `my_package; rm -rf /`), leading to a critical remote code execution vulnerability.\", \"suggested_fixes\": [{\"title\": \"Escape user command arguments for 'UVAddPackage'.\", \"target_snippet\": \"opts.args\", \"replacement_snippet\": \"vim.fn.shellescape(opts.args)\"}, {\"title\": \"Escape user command arguments for 'UVRemovePackage'.\", \"target_snippet\": \"opts.args\", \"replacement_snippet\": \"vim.fn.shellescape(opts.args)\"}]}, {\"issue_snippet\": \"input\", \"severity\": \"error\", \"message\": \"User input obtained via `vim.ui.input` (e.g., for adding/removing packages from pickers or keymaps) is directly concatenated into shell commands without being escaped. This is a critical remote code execution vulnerability, as an attacker can inject malicious shell commands (e.g., `my_package; rm -rf /`). This pattern is repeated in Snacks picker, Telescope picker, and direct keymaps for `uv add` and `uv remove`.\", \"suggested_fixes\": [{\"title\": \"Escape user input for Snacks picker commands.\", \"target_snippet\": \"local actual_cmd = cmd:gsub(\\\"%[(.-)%]\\\", input)\", \"replacement_snippet\": \"local actual_cmd = cmd:gsub(\\\"%[(.-)%]\\\", vim.fn.shellescape(input))\"}, {\"title\": \"Escape user input for Telescope picker commands.\", \"target_snippet\": \"local cmd = selection.cmd .. input\", \"replacement_snippet\": \"local cmd = selection.cmd .. vim.fn.shellescape(input)\"}, {\"title\": \"Escape user input for 'add package' keymap.\", \"target_snippet\": \"require('uv').run_command('uv add ' .. input)\", \"replacement_snippet\": \"require('uv').run_command('uv add ' .. vim.fn.shellescape(input))\"}, {\"title\": \"Escape user input for 'remove package' keymap.\", \"target_snippet\": \"require('uv').run_command('uv remove ' .. input)\", \"replacement_snippet\": \"require('uv').run_command('uv remove ' .. vim.fn.shellescape(input))\"}]}, {\"issue_snippet\": \"local command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\", \"severity\": \"warning\", \"message\": \"The `source` command is executed in a subshell via `vim.fn.jobstart`. The `source` command is a shell built-in that modifies the *current* shell's environment. Running it in a subshell means its effects on environment variables are confined to that subshell and do not propagate back to the Neovim process. While `vim.env.VIRTUAL_ENV` and `vim.env.PATH` are manually updated, this might not fully replicate all environment changes typically made by a complete virtual environment activation script, potentially leading to an incomplete setup for subsequent Python commands.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"if not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\", \"severity\": \"warning\", \"message\": \"The logic to extract global variables (`get_buffer_globals`) relies on simplistic regex matching and indentation checks. It might inaccurately classify local variables within top-level functions or complex assignments as globals, especially in non-trivial Python codebases. This can lead to `NameError` or unexpected behavior when the selected Python code is executed in isolation, as its dependencies might not be correctly provided. A more robust solution would involve proper Python AST parsing.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\", \"severity\": \"warning\", \"message\": \"When automatically wrapping selected expressions for printing, inserting the expression directly into an f-string can lead to `SyntaxError` if the expression itself contains f-string syntax (e.g., `{}`) or unescaped quotes. A safer approach is to assign the expression to a temporary variable and then print its `str()` representation.\", \"suggested_fixes\": [{\"title\": \"Use a temporary variable and `str()` for safer expression printing.\", \"target_snippet\": \"file:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\", \"replacement_snippet\": \"local cleaned_selection = selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\")\\nfile:write(\\\"_uv_expr_result = (\\\" .. cleaned_selection .. \\\")\\\\n\\\")\\nfile:write('print(\\\"Expression result: \\\" + str(_uv_expr_result))\\\\n')\"}]}, {\"issue_snippet\": \"if is_all_indented then\", \"severity\": \"warning\", \"message\": \"The logic for `M.run_python_selection` wraps all indented selected code in a new function (`def run_selection():`). However, if the selected code itself is an indented function or class definition (e.g., a method inside a class), this wrapping will result in a `SyntaxError` (e.g., `def` nested at the same level) or incorrect execution context. The checks for `is_function_def` and `is_class_def` should explicitly prevent this wrapping.\", \"suggested_fixes\": [{\"title\": \"Avoid redundant function wrapping for indented function/class definitions.\", \"target_snippet\": \"if is_all_indented then\", \"replacement_snippet\": \"if is_all_indented and not is_function_def and not is_class_def then\"}]}, {\"issue_snippet\": \"local temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\", \"severity\": \"info\", \"message\": \"The plugin creates temporary Python files for executing selections and functions in `~/.cache/nvim/uv_run`. There is no explicit mechanism to clean up these temporary files after execution, which can lead to accumulation and unnecessary disk space usage over time.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"_G.run_command = M.run_command\", \"severity\": \"info\", \"message\": \"Exposing `M.run_command` globally as `_G.run_command` pollutes the global namespace. While the comment suggests it 'can be removed if not needed', its presence can lead to naming conflicts with other plugins or scripts and makes the module less encapsulated. It is generally better to explicitly require the module to access its functions.\", \"suggested_fixes\": [{\"title\": \"Remove global exposure if not strictly necessary.\", \"target_snippet\": \"_G.run_command = M.run_command\", \"replacement_snippet\": \"-- _G.run_command = M.run_command -- Removed global exposure to prevent namespace pollution\"}]}, {\"issue_snippet\": \"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\"\", \"severity\": \"info\", \"message\": \"When running a specific Python function (`M.run_python_function`), the temporary script imports the entire current Python file as a module. This means all top-level code in that module will be executed upon import, including `if __name__ == \\\"__main__\\\":` blocks, global variable assignments, and any other statements outside of function definitions. This can lead to unintended side effects, inefficient execution, or incorrect behavior if the module's top-level code relies on a specific execution context.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"finder = function()\", \"severity\": \"info\", \"message\": \"The logic for integrating with `Snacks` and `Telescope` pickers contains significant duplication in defining command items and the `on_select`/`confirm` logic. This violates the DRY (Don't Repeat Yourself) principle, making the code harder to maintain and update. Refactoring into shared data structures and helper functions would improve maintainability.\", \"suggested_fixes\": null}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"9365"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"1854"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"7914"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"9365"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal get_buffer_globals = function()\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_a076092a98f949a7a3fdf876818b46b9\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"suggested_fixes\": [{\"title\": \"Escape user command arguments for 'UVAddPackage'.\", \"target_snippet\": \"opts.args\", \"replacement_snippet\": \"vim.fn.shellescape(opts.args)\"}, {\"replacement_snippet\": \"vim.fn.shellescape(opts.args)\", \"target_snippet\": \"opts.args\", \"title\": \"Escape user command arguments for 'UVRemovePackage'.\"}], \"message\": \"User input from `opts.args` in user commands (`UVAddPackage`, `UVRemovePackage`) is directly concatenated into shell commands without being escaped. A malicious user could inject arbitrary shell commands (e.g., `my_package; rm -rf /`), leading to a critical remote code execution vulnerability.\", \"severity\": \"error\", \"issue_snippet\": \"opts.args\"}, {\"suggested_fixes\": [{\"replacement_snippet\": \"local actual_cmd = cmd:gsub(\\\"%[(.-)%]\\\", vim.fn.shellescape(input))\", \"title\": \"Escape user input for Snacks picker commands.\", \"target_snippet\": \"local actual_cmd = cmd:gsub(\\\"%[(.-)%]\\\", input)\"}, {\"title\": \"Escape user input for Telescope picker commands.\", \"target_snippet\": \"local cmd = selection.cmd .. input\", \"replacement_snippet\": \"local cmd = selection.cmd .. vim.fn.shellescape(input)\"}, {\"replacement_snippet\": \"require('uv').run_command('uv add ' .. vim.fn.shellescape(input))\", \"title\": \"Escape user input for 'add package' keymap.\", \"target_snippet\": \"require('uv').run_command('uv add ' .. input)\"}, {\"title\": \"Escape user input for 'remove package' keymap.\", \"target_snippet\": \"require('uv').run_command('uv remove ' .. input)\", \"replacement_snippet\": \"require('uv').run_command('uv remove ' .. vim.fn.shellescape(input))\"}], \"severity\": \"error\", \"issue_snippet\": \"input\", \"message\": \"User input obtained via `vim.ui.input` (e.g., for adding/removing packages from pickers or keymaps) is directly concatenated into shell commands without being escaped. This is a critical remote code execution vulnerability, as an attacker can inject malicious shell commands (e.g., `my_package; rm -rf /`). This pattern is repeated in Snacks picker, Telescope picker, and direct keymaps for `uv add` and `uv remove`.\"}, {\"message\": \"The `source` command is executed in a subshell via `vim.fn.jobstart`. The `source` command is a shell built-in that modifies the *current* shell's environment. Running it in a subshell means its effects on environment variables are confined to that subshell and do not propagate back to the Neovim process. While `vim.env.VIRTUAL_ENV` and `vim.env.PATH` are manually updated, this might not fully replicate all environment changes typically made by a complete virtual environment activation script, potentially leading to an incomplete setup for subsequent Python commands.\", \"severity\": \"warning\", \"issue_snippet\": \"local command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\", \"suggested_fixes\": null}, {\"message\": \"The logic to extract global variables (`get_buffer_globals`) relies on simplistic regex matching and indentation checks. It might inaccurately classify local variables within top-level functions or complex assignments as globals, especially in non-trivial Python codebases. This can lead to `NameError` or unexpected behavior when the selected Python code is executed in isolation, as its dependencies might not be correctly provided. A more robust solution would involve proper Python AST parsing.\", \"issue_snippet\": \"if not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\", \"suggested_fixes\": null, \"severity\": \"warning\"}, {\"issue_snippet\": \"print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\", \"severity\": \"warning\", \"message\": \"When automatically wrapping selected expressions for printing, inserting the expression directly into an f-string can lead to `SyntaxError` if the expression itself contains f-string syntax (e.g., `{}`) or unescaped quotes. A safer approach is to assign the expression to a temporary variable and then print its `str()` representation.\", \"suggested_fixes\": [{\"replacement_snippet\": \"local cleaned_selection = selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\")\\nfile:write(\\\"_uv_expr_result = (\\\" .. cleaned_selection .. \\\")\\\\n\\\")\\nfile:write('print(\\\"Expression result: \\\" + str(_uv_expr_result))\\\\n')\", \"target_snippet\": \"file:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\", \"title\": \"Use a temporary variable and `str()` for safer expression printing.\"}]}, {\"severity\": \"warning\", \"message\": \"The logic for `M.run_python_selection` wraps all indented selected code in a new function (`def run_selection():`). However, if the selected code itself is an indented function or class definition (e.g., a method inside a class), this wrapping will result in a `SyntaxError` (e.g., `def` nested at the same level) or incorrect execution context. The checks for `is_function_def` and `is_class_def` should explicitly prevent this wrapping.\", \"suggested_fixes\": [{\"target_snippet\": \"if is_all_indented then\", \"title\": \"Avoid redundant function wrapping for indented function/class definitions.\", \"replacement_snippet\": \"if is_all_indented and not is_function_def and not is_class_def then\"}], \"issue_snippet\": \"if is_all_indented then\"}, {\"message\": \"The plugin creates temporary Python files for executing selections and functions in `~/.cache/nvim/uv_run`. There is no explicit mechanism to clean up these temporary files after execution, which can lead to accumulation and unnecessary disk space usage over time.\", \"suggested_fixes\": null, \"issue_snippet\": \"local temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\", \"severity\": \"info\"}, {\"severity\": \"info\", \"issue_snippet\": \"_G.run_command = M.run_command\", \"message\": \"Exposing `M.run_command` globally as `_G.run_command` pollutes the global namespace. While the comment suggests it 'can be removed if not needed', its presence can lead to naming conflicts with other plugins or scripts and makes the module less encapsulated. It is generally better to explicitly require the module to access its functions.\", \"suggested_fixes\": [{\"replacement_snippet\": \"-- _G.run_command = M.run_command -- Removed global exposure to prevent namespace pollution\", \"title\": \"Remove global exposure if not strictly necessary.\", \"target_snippet\": \"_G.run_command = M.run_command\"}]}, {\"issue_snippet\": \"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\"\", \"severity\": \"info\", \"message\": \"When running a specific Python function (`M.run_python_function`), the temporary script imports the entire current Python file as a module. This means all top-level code in that module will be executed upon import, including `if __name__ == \\\"__main__\\\":` blocks, global variable assignments, and any other statements outside of function definitions. This can lead to unintended side effects, inefficient execution, or incorrect behavior if the module's top-level code relies on a specific execution context.\", \"suggested_fixes\": null}, {\"message\": \"The logic for integrating with `Snacks` and `Telescope` pickers contains significant duplication in defining command items and the `on_select`/`confirm` logic. This violates the DRY (Don't Repeat Yourself) principle, making the code harder to maintain and update. Refactoring into shared data structures and helper functions would improve maintainability.\", \"severity\": \"info\", \"issue_snippet\": \"finder = function()\", \"suggested_fixes\": null}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_a076092a98f949a7a3fdf876818b46b9\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"7d64967998c02b81","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762974607222742000","endTimeUnixNano":"1762974607222742000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 10 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"10"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"6c15f6e4e58c2125","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762974607224363000","endTimeUnixNano":"1762974607224363000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 2 occurrences of 'opts.args...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"2"}},{"key":"snippet[:20]","value":{"stringValue":"opts.args"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"0a4c20342a6db280","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762974607225697000","endTimeUnixNano":"1762974607225697000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 25 occurrences of 'input...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"25"}},{"key":"snippet[:20]","value":{"stringValue":"input"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"ad00c7a4e7ebd986","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762974607225880000","endTimeUnixNano":"1762974607225880000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'local command = \"sou...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"local command = \"sou"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"dcd05fe146e58015","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762974607226212000","endTimeUnixNano":"1762974607226212000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'if not in_class and ...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"if not in_class and "}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"0e2e7f97a9e0eb71","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762974607226426000","endTimeUnixNano":"1762974607226426000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'print(f\"Expression r...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"print(f\"Expression r"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"14697f8f658516d7","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762974607226578000","endTimeUnixNano":"1762974607226578000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'if is_all_indented t...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"if is_all_indented t"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"edda00f471a66c76","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762974607226741000","endTimeUnixNano":"1762974607226741000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 2 occurrences of 'local temp_dir = vim...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"2"}},{"key":"snippet[:20]","value":{"stringValue":"local temp_dir = vim"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"a0edc121765b4210","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762974607226965000","endTimeUnixNano":"1762974607226965000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of '_G.run_command = M.r...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"_G.run_command = M.r"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"a6f613b9567885bd","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762974607227112000","endTimeUnixNano":"1762974607227112000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'import \" .. module_n...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"import \" .. module_n"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"e5e7c0fe7a462a88","parentSpanId":"3c8ba40f54e1ba14","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762974607227300000","endTimeUnixNano":"1762974607227300000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 2 occurrences of 'finder = function()...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"2"}},{"key":"snippet[:20]","value":{"stringValue":"finder = function()"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7978e6e42cc65aa1d08c679faa3e","spanId":"3c8ba40f54e1ba14","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762974557924410000","endTimeUnixNano":"1762974607227383000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a7979a77d0bd13c375c94921b6e7b","spanId":"ad617b71d6622037","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762974607229951000","endTimeUnixNano":"1762974607229951000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 10 diagnostics for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"10"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"4d3973eb13704a2baf171123e754ae3a"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"24647"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a798fbb88a919f3c3089dd7054411","spanId":"cc8952aad35fcdad","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762976054152599000","endTimeUnixNano":"1762976054152599000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"77"}}],"status":{}},{"traceId":"019a798fbcf8b1dec00ea004921eaf27","spanId":"16d14c44624b5b49","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762976054520151000","endTimeUnixNano":"1762976054520151000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"115"}}],"status":{}},{"traceId":"019a798fbd0a0143db8850d67863507b","spanId":"675822e2b6da6784","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976054538883000","endTimeUnixNano":"1762976054538883000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fc0f880203d2d4989e0101174","spanId":"a96a94121f8a5038","parentSpanId":"5a33357839670ef0","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976055545406000","endTimeUnixNano":"1762976055545406000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fccbe2b9fe52196124bd4b5a0","spanId":"2603b72a484626a2","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976058558112000","endTimeUnixNano":"1762976058558112000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fc0f880203d2d4989e0101174","spanId":"5a33357839670ef0","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976055544357000","endTimeUnixNano":"1762976058569515000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976058559371000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a798fcccaa4d53359fc9b729b4d67","spanId":"90b28ce48e90eae1","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976058570770000","endTimeUnixNano":"1762976058570770000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a798fc0f880203d2d4989e0101174","spanId":"5c6c0503ff8bbf21","parentSpanId":"4766ebe448fad6b3","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976055548181000","endTimeUnixNano":"1762976058558667000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a798fc0f880203d2d4989e0101174","spanId":"4766ebe448fad6b3","parentSpanId":"5a33357839670ef0","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976055546978000","endTimeUnixNano":"1762976058558865000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  \\\"benomahony/uv.nvim\\\",\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"4d3973eb13704a2baf171123e754ae3a"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"24647"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a798fd0a8a932ea7506f47147665a","spanId":"bca6d947cd1c383e","parentSpanId":"6b0c20c5df58703c","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976059561015000","endTimeUnixNano":"1762976059561015000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fd2d631cda1157eebb9c34281","spanId":"a784effe909048f7","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976060118570000","endTimeUnixNano":"1762976060118570000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fd0a8a932ea7506f47147665a","spanId":"6b0c20c5df58703c","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976059560138000","endTimeUnixNano":"1762976060124691000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976060120649000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a798fd2ddf1e284bf0f4be8642089","spanId":"a98ac155251a4189","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976060125059000","endTimeUnixNano":"1762976060125059000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fd58def7d92f476dfbd3d1a4f","spanId":"37ae63d1818a3928","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976060813543000","endTimeUnixNano":"1762976060813543000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fd58ecaf2beaef50659c0b61e","spanId":"21be526ba5ee44e5","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976060814326000","endTimeUnixNano":"1762976060814326000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fd66ad84406768f8362d41143","spanId":"70c1139f0d603f1a","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976061034036000","endTimeUnixNano":"1762976061034036000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fd66ba112d08eee30832767a1","spanId":"7e30570810a26f0a","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976061035170000","endTimeUnixNano":"1762976061035170000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fd80b7742a565a6aa4a300660","spanId":"1e4855c890d4a55d","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976061451403000","endTimeUnixNano":"1762976061451403000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fd80c063e001c1fd77db428fa","spanId":"38e43655650eda6e","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976061452047000","endTimeUnixNano":"1762976061452047000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fd989f8fd57681ecb569ae338","spanId":"57d61bd28608a50a","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976061833354000","endTimeUnixNano":"1762976061833354000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fd9891034d5c50fba50b3f190","spanId":"5d6c553efeb4e0fb","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976061833906000","endTimeUnixNano":"1762976061833906000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fda1de7e2fdb73a9e630d455b","spanId":"7e8b063cc61e983b","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976061981944000","endTimeUnixNano":"1762976061981944000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fda1e26b7035f70ff87589fd9","spanId":"131d8d75ff04c3c0","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976061982664000","endTimeUnixNano":"1762976061982664000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fdcfc7af0d68fe660c154b51f","spanId":"386f44f96a874b7f","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976062716974000","endTimeUnixNano":"1762976062716974000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fdcfee8f1278b00c8d15f622c","spanId":"b9135c38f6b4222e","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976062718054000","endTimeUnixNano":"1762976062718054000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe0e7d4b04b18fa2b1294b9f0","spanId":"7c11dfc6942747fe","parentSpanId":"7c37360b41df4da0","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976063719471000","endTimeUnixNano":"1762976063719471000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe255b4c79734e17af22c65f6","spanId":"765ccd3fdc5fc4d0","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976064085721000","endTimeUnixNano":"1762976064085721000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe0e7d4b04b18fa2b1294b9f0","spanId":"7c37360b41df4da0","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976063719071000","endTimeUnixNano":"1762976064092026000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976064088187000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a798fe25cbd575eb2eceb94827df8","spanId":"225488ba5af78b09","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976064092498000","endTimeUnixNano":"1762976064092498000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a798fd0a8a932ea7506f47147665a","spanId":"c807657b4b699072","parentSpanId":"730ce59b483b492f","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976059564917000","endTimeUnixNano":"1762976060119568000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a798fd0a8a932ea7506f47147665a","spanId":"730ce59b483b492f","parentSpanId":"6b0c20c5df58703c","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976059562973000","endTimeUnixNano":"1762976060119938000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  -- \\\"benomahony/uv.nvim\\\",\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a798fe0e7d4b04b18fa2b1294b9f0","spanId":"54d563795d7a86c2","parentSpanId":"5d322cec96197368","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976063720977000","endTimeUnixNano":"1762976064087290000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a798fe0e7d4b04b18fa2b1294b9f0","spanId":"5d322cec96197368","parentSpanId":"7c37360b41df4da0","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976063720284000","endTimeUnixNano":"1762976064087558000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  -- \\\"benomahony/uv.nvim\\\",\\n  dir\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"4d3973eb13704a2baf171123e754ae3a"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"24647"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a798fe28c70ff2197c6407e3e8f84","spanId":"3416ef332b64a31e","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976064140555000","endTimeUnixNano":"1762976064140555000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe28d14fe52f7a51278087421","spanId":"d7857fabb0b21d3b","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976064141109000","endTimeUnixNano":"1762976064141109000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe44b3e66e00bc786ec11972e","spanId":"1280fb44d2c0e6c2","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976064587768000","endTimeUnixNano":"1762976064587768000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe44cb40199afa79a157d193a","spanId":"fa2d06b4e81cd971","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976064588780000","endTimeUnixNano":"1762976064588780000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe54907789c904b12afb1d38c","spanId":"9330af3c56c3b3c5","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976064841804000","endTimeUnixNano":"1762976064841804000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe54a139f6ad16db0c0467eb2","spanId":"aca16ab696362fc3","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976064842574000","endTimeUnixNano":"1762976064842574000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe5db7690a54e425bfd41ff0d","spanId":"a96977d2f2d36edd","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976064987301000","endTimeUnixNano":"1762976064987301000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe5db5619f802b9b106a75a4f","spanId":"51399f8e53011263","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976064987768000","endTimeUnixNano":"1762976064987768000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe670efcbd5106ee7b17fc4ec","spanId":"a7891408d0082b35","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976065136857000","endTimeUnixNano":"1762976065136857000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe671e2c9ecb89678d7a9c52f","spanId":"9cac1e43ad0a5eb0","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976065137295000","endTimeUnixNano":"1762976065137295000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe707c61c9a499a6dd35b1f2e","spanId":"a0e8f27e8edcc37a","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976065287525000","endTimeUnixNano":"1762976065287525000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe7079c37f2280672c50e9ba6","spanId":"3cb428db0a187448","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976065287962000","endTimeUnixNano":"1762976065287962000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe79c5d4efb77182ff7bbf96e","spanId":"3fa5f82eb9e9b56f","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976065436767000","endTimeUnixNano":"1762976065436767000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe79e9559413817b18dfd28f5","spanId":"3e6254ec82a695d0","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976065438129000","endTimeUnixNano":"1762976065438129000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe83135add8e3d77561658267","spanId":"90b7f89bbcb934d0","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976065585366000","endTimeUnixNano":"1762976065585366000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe8312bc5c7bcd9c656686168","spanId":"851de2e070fb345b","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976065585634000","endTimeUnixNano":"1762976065585634000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe92519325af9f1f0c02558ec","spanId":"87feedc9b1401f00","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976065829767000","endTimeUnixNano":"1762976065829767000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe92731666e2120dd1ff035f3","spanId":"1d16c86ca35f0d86","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976065831809000","endTimeUnixNano":"1762976065831809000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe9b842d7414382d70c663e57","spanId":"00ed1aa7ed557276","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976065976019000","endTimeUnixNano":"1762976065976019000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fe9b82e8e4e1074cabf08dfba","spanId":"aae56c5af0c78c88","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976065976441000","endTimeUnixNano":"1762976065976441000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fea4eecfc984844b2882c01a9","spanId":"99fc210ef7c7174c","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976066126216000","endTimeUnixNano":"1762976066126216000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fea4e31a912fce1ee4f52c675","spanId":"1b93819c1d1cbbc5","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976066126677000","endTimeUnixNano":"1762976066126677000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798feae4436d56931b08eb684fa7","spanId":"097f9142da8cf191","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976066276590000","endTimeUnixNano":"1762976066276590000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798feae58b51485d2cfcd73df3f3","spanId":"b22870aa7f1153b6","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976066277247000","endTimeUnixNano":"1762976066277247000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798febd8c32a2f3877e00bd10ae7","spanId":"2ed443538ad654fc","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976066520665000","endTimeUnixNano":"1762976066520665000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798febda34a52cb5f0fb3e8aa503","spanId":"adb2e95416bcbba3","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976066522102000","endTimeUnixNano":"1762976066522102000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798fec79edc404be95c64591e41e","spanId":"8a3484f2438809c4","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976066681640000","endTimeUnixNano":"1762976066681640000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798fec7a2ef3990f46fab0fa768a","spanId":"b7b6ea915ff49bb0","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976066682352000","endTimeUnixNano":"1762976066682352000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff063602e40eeaa87802334bb","spanId":"f47ae77956b450c9","parentSpanId":"773a6774f66afe69","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976067683946000","endTimeUnixNano":"1762976067683946000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff3105bf99f747998271afd69","spanId":"bb94aeddc3ba18a9","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976068368185000","endTimeUnixNano":"1762976068368185000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff063602e40eeaa87802334bb","spanId":"773a6774f66afe69","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976067683516000","endTimeUnixNano":"1762976068374964000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976068370945000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a798ff317f18c1c64c40b98c3495e","spanId":"ecdb1f8d911861e8","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976068375322000","endTimeUnixNano":"1762976068375322000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff3a04dc9eded013f6bba751b","spanId":"cceec2f73579d1e9","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976068512680000","endTimeUnixNano":"1762976068512680000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff3a1a2e0037c896b89f986f1","spanId":"40944217b636c9b9","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976068513062000","endTimeUnixNano":"1762976068513062000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff43987f8f50ba1d9c36e5cbb","spanId":"2281fb2e4194c2e9","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976068665130000","endTimeUnixNano":"1762976068665130000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff439a24d4288fc115dc033f7","spanId":"1bc7d9b062f246e2","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976068665545000","endTimeUnixNano":"1762976068665545000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a798ff063602e40eeaa87802334bb","spanId":"079a218309378731","parentSpanId":"d3e646baf3088e5c","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976067685614000","endTimeUnixNano":"1762976068369656000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a798ff063602e40eeaa87802334bb","spanId":"d3e646baf3088e5c","parentSpanId":"773a6774f66afe69","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976067684813000","endTimeUnixNano":"1762976068370096000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  -- \\\"benomahony/uv.nvim\\\",\\n  dir = \\\"/Users/benomahony/\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"4d3973eb13704a2baf171123e754ae3a"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"24647"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a798ff75257574c8b238dca80f7ac","spanId":"4f7377b96beef469","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976069458070000","endTimeUnixNano":"1762976069458070000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff7538977dc56ac5f5be490d4","spanId":"b13ac29dcbdb285f","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976069459150000","endTimeUnixNano":"1762976069459150000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff8035992067203c4693262ef","spanId":"77d660c1db6fb2b1","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976069635328000","endTimeUnixNano":"1762976069635328000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff803f0e1382e701d1d2d6134","spanId":"5ad59130508da1bd","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976069635955000","endTimeUnixNano":"1762976069635955000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff8b31e3f62af0c39318f2c12","spanId":"e4c318cfc076e506","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976069811874000","endTimeUnixNano":"1762976069811874000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff8b48e45573c8a0f278e2223","spanId":"e5dfbd2edfc163b5","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976069812637000","endTimeUnixNano":"1762976069812637000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff9810e591bc359e9ff571c4a","spanId":"560dc8aa485a4f52","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976070017162000","endTimeUnixNano":"1762976070017162000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798ff98217de8500711ebcf5a521","spanId":"e0696b1473797ae5","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976070017996000","endTimeUnixNano":"1762976070017996000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798ffa1bfb7cbc24248d22e276ee","spanId":"7d9ba012e7f1b991","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976070171788000","endTimeUnixNano":"1762976070171788000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798ffa1c10071be4aa01755d4711","spanId":"f0bb88083578331b","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976070172165000","endTimeUnixNano":"1762976070172165000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798ffdc52cdc6739c32eee2b665e","spanId":"03c43f8f3392364f","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976071109850000","endTimeUnixNano":"1762976071109850000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798ffdc6deaa4cbf50214fcb48a6","spanId":"32e1b4ed2885ebd1","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976071110844000","endTimeUnixNano":"1762976071110844000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a798ffdf96cb0d3571611181aea57","spanId":"588e9813395f338b","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976071161562000","endTimeUnixNano":"1762976071161562000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a798ffdfafa11bdd87d7d34bfb7f5","spanId":"a1985028bd1de283","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976071162269000","endTimeUnixNano":"1762976071162269000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7990010b2c5686c59afad5d457d6","spanId":"c088de08c07e6af7","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976071947484000","endTimeUnixNano":"1762976071947484000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990010d3e3f0fe42943429861fc","spanId":"4da26c3568e016b3","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976071949053000","endTimeUnixNano":"1762976071949053000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79900204828fe5b6579091f51581","spanId":"dd41f151c10c01d3","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976072196482000","endTimeUnixNano":"1762976072196482000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79900206ed38e69f6a90eefc84ef","spanId":"c0dc07321e32fea6","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976072198076000","endTimeUnixNano":"1762976072198076000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7990029614d06aa2347388230947","spanId":"b75177335e1e20f2","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976072342874000","endTimeUnixNano":"1762976072342874000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79900297627ae25c651199aa768a","spanId":"775f8afee1345b58","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976072343366000","endTimeUnixNano":"1762976072343366000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799002c9fc68d4143a6a26a5b09c","spanId":"36052dc78ec8f2c2","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976072393463000","endTimeUnixNano":"1762976072393463000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a799002c95db1f6517e7625182a72","spanId":"100aa1f35d246cef","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976072393915000","endTimeUnixNano":"1762976072393915000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7990035e183e4e7c54a8e37a8751","spanId":"3343ae8b59ad91a0","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976072542784000","endTimeUnixNano":"1762976072542784000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990035f2999b2afde9276510286","spanId":"2b5446fa87939caa","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976072543241000","endTimeUnixNano":"1762976072543241000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799003f6e3d89239fe90ad05374d","spanId":"f0f79106e2401288","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976072694768000","endTimeUnixNano":"1762976072694768000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a799003f7a870bf6e18b45760045c","spanId":"44d78df68d94d277","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976072695356000","endTimeUnixNano":"1762976072695356000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7990049dbf3e4d2bc758d3cdf850","spanId":"67c39afc0f747adf","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976072861297000","endTimeUnixNano":"1762976072861297000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990049d378a59a60336247171b2","spanId":"bf46b2f0c3168fdd","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976072861917000","endTimeUnixNano":"1762976072861917000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79900564ea6cf8b213c6821644b8","spanId":"6903a1c64511435c","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976073060096000","endTimeUnixNano":"1762976073060096000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a799005640a34ec8090177ea2fede","spanId":"f5960c4988d23a20","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976073060704000","endTimeUnixNano":"1762976073060704000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7990069ff05caa7f1d87c83de702","spanId":"0d44ab8f7efc1d6b","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976073375368000","endTimeUnixNano":"1762976073375368000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a799006a08e20c2331ccf6505a90b","spanId":"db9c2eb5b99491ed","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976073376034000","endTimeUnixNano":"1762976073376034000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"4d3973eb13704a2baf171123e754ae3a"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"24647"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79900a89d20be36f64ba22a2347f","spanId":"0adaa57a9c3eaef1","parentSpanId":"1b6f4215df5e4743","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976074377475000","endTimeUnixNano":"1762976074377475000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79900c35f29b0b2629f62bb01edc","spanId":"fc0350a4e04bceec","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976074805260000","endTimeUnixNano":"1762976074805260000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79900a89d20be36f64ba22a2347f","spanId":"1b6f4215df5e4743","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976074377080000","endTimeUnixNano":"1762976074809566000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976074806750000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79900c3989c74729917d5053f89f","spanId":"7c3569c1f99d5498","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976074809919000","endTimeUnixNano":"1762976074809919000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79900d4f95265da3b7d8fec630f5","spanId":"a226d0878e4bbc28","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976075087665000","endTimeUnixNano":"1762976075087665000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79900d50ce2651545d5185c1c3c4","spanId":"8136c6aed3633b4a","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976075088203000","endTimeUnixNano":"1762976075088203000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79900d50884ea9a5693c21877543","spanId":"55aceacd56c533ef","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976075088960000","endTimeUnixNano":"1762976075088960000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79900d51651e19d456e96c2c25a8","spanId":"e7bfc17f7dda18ee","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976075089267000","endTimeUnixNano":"1762976075089267000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7990113a7bf526364a82a8dfc154","spanId":"691ab79bdadeea06","parentSpanId":"db0b158b28f31437","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976076091212000","endTimeUnixNano":"1762976076091212000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799014251e5b3c391451d4813b51","spanId":"f961241ecbc765ab","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976076837365000","endTimeUnixNano":"1762976076837365000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990113a7bf526364a82a8dfc154","spanId":"db0b158b28f31437","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976076090616000","endTimeUnixNano":"1762976076842456000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976076839336000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a7990142ac2223fa336e92dddf333","spanId":"0f93d5f4aacc5b7f","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976076842690000","endTimeUnixNano":"1762976076842690000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79901639d7ffd33d61cce8198d77","spanId":"7c2f4f0781276130","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976077369219000","endTimeUnixNano":"1762976077369219000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79901639c686d75890963a93b7e4","spanId":"50aae9d0d6360434","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976077369726000","endTimeUnixNano":"1762976077369726000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79901723af233c3c65afee5d59ab","spanId":"6ed22b6594ecb90f","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976077603867000","endTimeUnixNano":"1762976077603867000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79901724f533e5de22b3c4333570","spanId":"344be0621ab14a59","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976077604355000","endTimeUnixNano":"1762976077604355000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79901af6a6fdbdba86184272cada","spanId":"d33b5c1e2e369ce5","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976078581989000","endTimeUnixNano":"1762976078581989000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79901af633e32cf381c14071a592","spanId":"21ad5dbdac853bfe","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976078582846000","endTimeUnixNano":"1762976078582846000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79900a89d20be36f64ba22a2347f","spanId":"e3a12c5eccac886f","parentSpanId":"601e4fe2f6068d76","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976074379140000","endTimeUnixNano":"1762976074806019000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79900a89d20be36f64ba22a2347f","spanId":"601e4fe2f6068d76","parentSpanId":"1b6f4215df5e4743","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976074378310000","endTimeUnixNano":"1762976074806246000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  -- \\\"benomahony/uv.nvim\\\",\\n  dir = \\\"/Users/benomahony/C\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a7990113a7bf526364a82a8dfc154","spanId":"25843594e156870e","parentSpanId":"7b864760a8cc067c","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976076093490000","endTimeUnixNano":"1762976076838340000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a7990113a7bf526364a82a8dfc154","spanId":"7b864760a8cc067c","parentSpanId":"db0b158b28f31437","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976076092485000","endTimeUnixNano":"1762976076838645000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  -- \\\"benomahony/uv.nvim\\\",\\n  dir = \\\"/Users/benomahony/Code/\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"4d3973eb13704a2baf171123e754ae3a"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"24647"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79901ee0216e8d0ffac72c7f898a","spanId":"1e6b88a7f8509730","parentSpanId":"960921be3af247c5","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976079585054000","endTimeUnixNano":"1762976079585054000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79901fa200ee34dc04762fbc97e7","spanId":"1110c8646d51bab9","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976079778298000","endTimeUnixNano":"1762976079778298000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79901ee0216e8d0ffac72c7f898a","spanId":"960921be3af247c5","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976079584152000","endTimeUnixNano":"1762976079785922000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976079781761000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79901faa2511bece1087ad802fbf","spanId":"f43b4a6a24693dbb","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976079786284000","endTimeUnixNano":"1762976079786284000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7990203ba56b147cdd37af5f9bfd","spanId":"168c5596b4b206f5","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976079931022000","endTimeUnixNano":"1762976079931022000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990203ca649bcc41c06caf90c58","spanId":"428905a14f6d02f0","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976079932480000","endTimeUnixNano":"1762976079932480000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799023234ff31c26fc774992536a","spanId":"3a6b870677447f13","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976080675321000","endTimeUnixNano":"1762976080675321000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79902324c4bb3e1ce274bebdcc4f","spanId":"2098220a44226f88","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976080676052000","endTimeUnixNano":"1762976080676052000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7990244a8cfe382b97b0f575c954","spanId":"cf62092274e3b94c","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976080970901000","endTimeUnixNano":"1762976080970901000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990244b362f9dd6f49a4f07d311","spanId":"247d2b884d32c07f","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976080971394000","endTimeUnixNano":"1762976080971394000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7990244c7ceb94543d792f923567","spanId":"f7f0059c29d19003","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976080972036000","endTimeUnixNano":"1762976080972036000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990244c1c9302603c7cf40563e2","spanId":"406aa3ddb5f0de43","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976080972345000","endTimeUnixNano":"1762976080972345000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79902835b85f1137da9ce192ddf2","spanId":"15692968c0edc38b","parentSpanId":"fd80ad927fb2f2fc","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976081974683000","endTimeUnixNano":"1762976081974683000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79901ee0216e8d0ffac72c7f898a","spanId":"ff5f658eaa87c31c","parentSpanId":"f0987ce8e3cc5199","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976079588674000","endTimeUnixNano":"1762976079780295000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79901ee0216e8d0ffac72c7f898a","spanId":"f0987ce8e3cc5199","parentSpanId":"960921be3af247c5","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976079586904000","endTimeUnixNano":"1762976079780883000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  -- \\\"benomahony/uv.nvim\\\",\\n  dir = \\\"/Users/benomahony/Code/open_source/\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"4d3973eb13704a2baf171123e754ae3a"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"24647"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79902835b85f1137da9ce192ddf2","spanId":"bbd8c8c3e30aa768","parentSpanId":"c417802df7d9eda3","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976081977849000","endTimeUnixNano":"1762976084454227000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"595"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"14"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"413"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"595"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  -- \\\"benomahony/uv.nvim\\\",\\n  dir = \\\"/Users/benomahony/Code/open_source/uv.nvim-amirreza/\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_24909437135a417fa363a62cd7e6f305\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": []}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79902835b85f1137da9ce192ddf2","spanId":"c417802df7d9eda3","parentSpanId":"fd80ad927fb2f2fc","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976081976469000","endTimeUnixNano":"1762976084456151000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": []}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"595"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"14"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"413"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"595"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  -- \\\"benomahony/uv.nvim\\\",\\n  dir = \\\"/Users/benomahony/Code/open_source/uv.nvim-amirreza/\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_24909437135a417fa363a62cd7e6f305\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": []}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_24909437135a417fa363a62cd7e6f305\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79902835b85f1137da9ce192ddf2","spanId":"544e1089f913fc01","parentSpanId":"fd80ad927fb2f2fc","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762976084457981000","endTimeUnixNano":"1762976084457981000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 0 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"0"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79902835b85f1137da9ce192ddf2","spanId":"fd80ad927fb2f2fc","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976081973825000","endTimeUnixNano":"1762976084458241000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a799031ee05f874f3f054dcf5f9dd","spanId":"a966555585f0f8c7","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762976084462583000","endTimeUnixNano":"1762976084462583000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 0 diagnostics for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"0"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799037eb864289db6729ebc77abc","spanId":"58bc571ab3f001a5","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976085995495000","endTimeUnixNano":"1762976085995495000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c133a345f3d34040b8bd471efc181fca"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"28607"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79904bf04c98a7e80367c5131fbd","spanId":"43fdb8b4a8c85e7b","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762976091120959000","endTimeUnixNano":"1762976091120959000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"77"}}],"status":{}},{"traceId":"019a79904d4373b40862f24d992c8553","spanId":"a7acbc5ff12244c5","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762976091459171000","endTimeUnixNano":"1762976091459171000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"115"}}],"status":{}},{"traceId":"019a79904d558847e7af6e0000ab738f","spanId":"86f00e2e7293d99e","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976091477611000","endTimeUnixNano":"1762976091477611000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990514001c12bb5993693dea18a","spanId":"5f2ad7d852719f77","parentSpanId":"ad09dfeb68d9e71e","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976092481532000","endTimeUnixNano":"1762976092481532000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799051a752ea28362b19d46e330c","spanId":"edac784cd4c5f51e","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976092583278000","endTimeUnixNano":"1762976092583278000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990514001c12bb5993693dea18a","spanId":"ad09dfeb68d9e71e","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976092480880000","endTimeUnixNano":"1762976092589714000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976092584508000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a799051af29439499967bfd8ac0be","spanId":"4cce3f51994529bd","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976092591171000","endTimeUnixNano":"1762976092591171000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7990559127dbc2467ebd779a9796","spanId":"f7753cefebc876ce","parentSpanId":"d3c9f7d0558695e2","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976093586444000","endTimeUnixNano":"1762976093586444000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7990569e4596606dce6b19bcade0","spanId":"c6eaf60af8be380c","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976093854056000","endTimeUnixNano":"1762976093854056000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990559127dbc2467ebd779a9796","spanId":"d3c9f7d0558695e2","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976093585387000","endTimeUnixNano":"1762976093861789000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976093857971000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a799056a634863a9d812b56872e30","spanId":"2c97c1e61c706b2f","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976093862150000","endTimeUnixNano":"1762976093862150000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799058751028f71fb8e9c8ff6acc","spanId":"5bc2e0293facc81d","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976094325138000","endTimeUnixNano":"1762976094325138000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a799058767730d3745f8c8c976fbc","spanId":"f6af6f7f602c1aef","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976094326277000","endTimeUnixNano":"1762976094326277000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799059a3dcefec5581bb86883872","spanId":"dd8c223326c36425","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976094627887000","endTimeUnixNano":"1762976094627887000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a799059a4108dce375e06289380cf","spanId":"e5dd44a3ad60a519","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976094628405000","endTimeUnixNano":"1762976094628405000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79905bebe8da5b7429acbe52d235","spanId":"7201410cb408d827","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976095211040000","endTimeUnixNano":"1762976095211040000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79905bebe5ae3a00f09f4805f9f5","spanId":"ac2b3c73872ece75","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976095211484000","endTimeUnixNano":"1762976095211484000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79905d18e1768af63f1ec6d592d7","spanId":"e4d8672166882e7e","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976095512594000","endTimeUnixNano":"1762976095512594000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79905d19c5b734c1a2415e3ccc3e","spanId":"a99d144fed7252f1","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976095513556000","endTimeUnixNano":"1762976095513556000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a7990514001c12bb5993693dea18a","spanId":"878179223c0255b0","parentSpanId":"a32dcc1ab9ad2944","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976092483052000","endTimeUnixNano":"1762976092583787000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a7990514001c12bb5993693dea18a","spanId":"a32dcc1ab9ad2944","parentSpanId":"ad09dfeb68d9e71e","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976092482348000","endTimeUnixNano":"1762976092583985000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  -- \\\"benomahony/uv.nvim\\\",\\n  dir = \\\"/Users/benomahony/Code/open_source/uv.nvim-amirreza/\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a7990559127dbc2467ebd779a9796","spanId":"cb11a012f9bc2b17","parentSpanId":"fd4e78281a6dc2e1","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976093589633000","endTimeUnixNano":"1762976093856622000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a7990559127dbc2467ebd779a9796","spanId":"fd4e78281a6dc2e1","parentSpanId":"d3c9f7d0558695e2","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976093588185000","endTimeUnixNano":"1762976093857189000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  -- \\\"benomahony/uv.nvim\\\",\\n  dir = \\\"/Users/benomahony/Code/open_source/uv.nvim-amirreza/\\\"\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c133a345f3d34040b8bd471efc181fca"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"28607"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7990610143f2e5cb4bddf2157c4f","spanId":"9840c83e4feac870","parentSpanId":"2c9b139d19643b80","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976096514545000","endTimeUnixNano":"1762976096514545000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799066451d0a1b72eaf908379a65","spanId":"1baea8779b1ee103","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976097861613000","endTimeUnixNano":"1762976097861613000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990610143f2e5cb4bddf2157c4f","spanId":"2c9b139d19643b80","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976096513857000","endTimeUnixNano":"1762976097865058000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976097862885000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79906649a5fb89d65ddb993b8f13","spanId":"2c98d30fce82dd6b","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976097865276000","endTimeUnixNano":"1762976097865276000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/uv.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a7990610143f2e5cb4bddf2157c4f","spanId":"d13ccc3bc4dc315f","parentSpanId":"c79c29a22aa154e1","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976096517105000","endTimeUnixNano":"1762976097862204000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a7990610143f2e5cb4bddf2157c4f","spanId":"c79c29a22aa154e1","parentSpanId":"2c9b139d19643b80","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976096515981000","endTimeUnixNano":"1762976097862428000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\nreturn {\\n  -- \\\"benomahony/uv.nvim\\\",\\n  dir = \\\"/Users/benomahony/Code/open_source/uv.nvim-amirreza/\\\",\\n  opts = {\\n    notify_activate_venv = false,\\n    picker_integration = true,\\n  },\\n}\\n\\n```\\n\\nFile: uv.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"5562677e4a744d0e9658b60527233708"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"44772"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7990af2470dae68c9de0c0d10ce8","spanId":"b317c953660d696b","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762976116516564000","endTimeUnixNano":"1762976116516564000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"77"}}],"status":{}},{"traceId":"019a7990b08bb4932eebfacf9e48d7fb","spanId":"5bb3348a307813af","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762976116875605000","endTimeUnixNano":"1762976116875605000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"115"}}],"status":{}},{"traceId":"019a7990b09d0051eae307e33aabba68","spanId":"9b357a72cbc6387c","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976116893853000","endTimeUnixNano":"1762976116893853000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7990b4887589dc76e2c292c3a523","spanId":"9b371776a940a093","parentSpanId":"a267ec6c991a8868","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976117897181000","endTimeUnixNano":"1762976117897181000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"17d077fec1c94224ab0750c330085ee7"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"45173"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79917e6424dc0de3f40b8b8531b7","spanId":"1fead71c49f03d4b","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762976169572395000","endTimeUnixNano":"1762976169572395000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"77"}}],"status":{}},{"traceId":"019a79917fde0d6797a41e5e4de09ded","spanId":"516afb77c1b9c250","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762976169950593000","endTimeUnixNano":"1762976169950593000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"115"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"5562677e4a744d0e9658b60527233708"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"44772"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a7990b4887589dc76e2c292c3a523","spanId":"a0e771158faf4c54","parentSpanId":"ddad674d7fe40f25","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976117898906000","endTimeUnixNano":"1762976182600193000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4882"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"895"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"11613"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4882"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_5e40d40a737047b5a62b48ad02faf511\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"warning\", \"message\": \"The `find_snippet_in_text` function strips leading and trailing whitespace from the provided snippet before searching. If the AI provides an `issue_snippet` or `target_snippet` that intentionally includes specific whitespace (e.g., indentation or leading/trailing spaces as part of the match), this stripping could lead to a failure to locate the snippet or an incorrect match. This may result in diagnostics or code actions not being applied as intended. It's generally safer to search for the exact snippet provided by the AI without modification.\", \"suggested_fixes\": [{\"target_snippet\": \"snippet.strip()\", \"replacement_snippet\": \"snippet\", \"title\": \"Remove snippet stripping\"}], \"issue_snippet\": \"snippet.strip()\"}, {\"message\": \"The `analyze_document`, `code_actions`, and `regenerate_fix` functions consistently use `positions[0]` from `find_snippet_in_text`. This means that if an `issue_snippet` or `target_snippet` (as provided by the AI) appears multiple times in the document, diagnostics and code actions will always be applied to the *first* occurrence found. This is an architectural limitation because the AI's current output format lacks the explicit range information needed to pinpoint a specific instance of a repeated code pattern, potentially leading to confusing diagnostics and incorrect application of fixes.\", \"severity\": \"error\", \"issue_snippet\": \"positions[0]\"}, {\"message\": \"The `max_cache_size` setting is defined in the `Settings` class but is not referenced or utilized anywhere within the provided `AILanguageServer` implementation. This indicates a potentially unimplemented feature, a forgotten setting, or dead code. It's good practice to either implement the associated caching logic (e.g., for `_pending_tasks` or `analyze_document` results) or remove the setting to avoid confusion and maintain a clean configuration.\", \"suggested_fixes\": [{\"replacement_snippet\": \"# max_cache_size: int = Field(default=50) # Setting currently unused.\", \"target_snippet\": \"max_cache_size: int = Field(default=50)\", \"title\": \"Remove unused setting\"}], \"severity\": \"info\", \"issue_snippet\": \"max_cache_size\"}, {\"message\": \"When regenerating a fix, the `regenerate_fix` command sends the entire document's content (`doc.source`) to the AI for analysis. While providing full context can be beneficial, for a focused regeneration of a specific snippet, sending the full file can be inefficient (higher token usage, slower response) and raises data privacy concerns if the code contains sensitive information. It would be more efficient and privacy-preserving to send only a contextual window of code around the problematic snippet instead of the entire document.\", \"issue_snippet\": \"doc.source\", \"severity\": \"warning\"}, {\"suggested_fixes\": [{\"replacement_snippet\": \"# To ensure OpenTelemetry components pick up the endpoint consistently,\\n# this line should ideally be moved to the very top of the file,\\n# before any imports that initialize tracing or logging libraries.\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\", \"title\": \"Suggest moving environment variable setting for early initialization\", \"target_snippet\": \"os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\"}], \"issue_snippet\": \"os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\", \"message\": \"The environment variable `OTEL_EXPORTER_OTLP_ENDPOINT` is set using `os.environ` after `logfire` has already been imported. While `logfire.configure` is called later, some underlying OpenTelemetry components might read environment variables during initial import. To guarantee that all parts of the tracing system correctly pick up the endpoint, it is best practice to set this environment variable at the very top of the script or externally, before any modules that rely on it are imported or initialized.\", \"severity\": \"warning\"}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a7990b4887589dc76e2c292c3a523","spanId":"ddad674d7fe40f25","parentSpanId":"a267ec6c991a8868","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976117897890000","endTimeUnixNano":"1762976182609243000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"snippet.strip()\", \"severity\": \"warning\", \"message\": \"The `find_snippet_in_text` function strips leading and trailing whitespace from the provided snippet before searching. If the AI provides an `issue_snippet` or `target_snippet` that intentionally includes specific whitespace (e.g., indentation or leading/trailing spaces as part of the match), this stripping could lead to a failure to locate the snippet or an incorrect match. This may result in diagnostics or code actions not being applied as intended. It's generally safer to search for the exact snippet provided by the AI without modification.\", \"suggested_fixes\": [{\"title\": \"Remove snippet stripping\", \"target_snippet\": \"snippet.strip()\", \"replacement_snippet\": \"snippet\"}]}, {\"issue_snippet\": \"positions[0]\", \"severity\": \"error\", \"message\": \"The `analyze_document`, `code_actions`, and `regenerate_fix` functions consistently use `positions[0]` from `find_snippet_in_text`. This means that if an `issue_snippet` or `target_snippet` (as provided by the AI) appears multiple times in the document, diagnostics and code actions will always be applied to the *first* occurrence found. This is an architectural limitation because the AI's current output format lacks the explicit range information needed to pinpoint a specific instance of a repeated code pattern, potentially leading to confusing diagnostics and incorrect application of fixes.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"max_cache_size\", \"severity\": \"info\", \"message\": \"The `max_cache_size` setting is defined in the `Settings` class but is not referenced or utilized anywhere within the provided `AILanguageServer` implementation. This indicates a potentially unimplemented feature, a forgotten setting, or dead code. It's good practice to either implement the associated caching logic (e.g., for `_pending_tasks` or `analyze_document` results) or remove the setting to avoid confusion and maintain a clean configuration.\", \"suggested_fixes\": [{\"title\": \"Remove unused setting\", \"target_snippet\": \"max_cache_size: int = Field(default=50)\", \"replacement_snippet\": \"# max_cache_size: int = Field(default=50) # Setting currently unused.\"}]}, {\"issue_snippet\": \"doc.source\", \"severity\": \"warning\", \"message\": \"When regenerating a fix, the `regenerate_fix` command sends the entire document's content (`doc.source`) to the AI for analysis. While providing full context can be beneficial, for a focused regeneration of a specific snippet, sending the full file can be inefficient (higher token usage, slower response) and raises data privacy concerns if the code contains sensitive information. It would be more efficient and privacy-preserving to send only a contextual window of code around the problematic snippet instead of the entire document.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\", \"severity\": \"warning\", \"message\": \"The environment variable `OTEL_EXPORTER_OTLP_ENDPOINT` is set using `os.environ` after `logfire` has already been imported. While `logfire.configure` is called later, some underlying OpenTelemetry components might read environment variables during initial import. To guarantee that all parts of the tracing system correctly pick up the endpoint, it is best practice to set this environment variable at the very top of the script or externally, before any modules that rely on it are imported or initialized.\", \"suggested_fixes\": [{\"title\": \"Suggest moving environment variable setting for early initialization\", \"target_snippet\": \"os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\", \"replacement_snippet\": \"# To ensure OpenTelemetry components pick up the endpoint consistently,\\n# this line should ideally be moved to the very top of the file,\\n# before any imports that initialize tracing or logging libraries.\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\"}]}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4882"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"895"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"11613"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4882"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_5e40d40a737047b5a62b48ad02faf511\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"warning\", \"message\": \"The `find_snippet_in_text` function strips leading and trailing whitespace from the provided snippet before searching. If the AI provides an `issue_snippet` or `target_snippet` that intentionally includes specific whitespace (e.g., indentation or leading/trailing spaces as part of the match), this stripping could lead to a failure to locate the snippet or an incorrect match. This may result in diagnostics or code actions not being applied as intended. It's generally safer to search for the exact snippet provided by the AI without modification.\", \"suggested_fixes\": [{\"target_snippet\": \"snippet.strip()\", \"replacement_snippet\": \"snippet\", \"title\": \"Remove snippet stripping\"}], \"issue_snippet\": \"snippet.strip()\"}, {\"message\": \"The `analyze_document`, `code_actions`, and `regenerate_fix` functions consistently use `positions[0]` from `find_snippet_in_text`. This means that if an `issue_snippet` or `target_snippet` (as provided by the AI) appears multiple times in the document, diagnostics and code actions will always be applied to the *first* occurrence found. This is an architectural limitation because the AI's current output format lacks the explicit range information needed to pinpoint a specific instance of a repeated code pattern, potentially leading to confusing diagnostics and incorrect application of fixes.\", \"severity\": \"error\", \"issue_snippet\": \"positions[0]\"}, {\"message\": \"The `max_cache_size` setting is defined in the `Settings` class but is not referenced or utilized anywhere within the provided `AILanguageServer` implementation. This indicates a potentially unimplemented feature, a forgotten setting, or dead code. It's good practice to either implement the associated caching logic (e.g., for `_pending_tasks` or `analyze_document` results) or remove the setting to avoid confusion and maintain a clean configuration.\", \"suggested_fixes\": [{\"replacement_snippet\": \"# max_cache_size: int = Field(default=50) # Setting currently unused.\", \"target_snippet\": \"max_cache_size: int = Field(default=50)\", \"title\": \"Remove unused setting\"}], \"severity\": \"info\", \"issue_snippet\": \"max_cache_size\"}, {\"message\": \"When regenerating a fix, the `regenerate_fix` command sends the entire document's content (`doc.source`) to the AI for analysis. While providing full context can be beneficial, for a focused regeneration of a specific snippet, sending the full file can be inefficient (higher token usage, slower response) and raises data privacy concerns if the code contains sensitive information. It would be more efficient and privacy-preserving to send only a contextual window of code around the problematic snippet instead of the entire document.\", \"issue_snippet\": \"doc.source\", \"severity\": \"warning\"}, {\"suggested_fixes\": [{\"replacement_snippet\": \"# To ensure OpenTelemetry components pick up the endpoint consistently,\\n# this line should ideally be moved to the very top of the file,\\n# before any imports that initialize tracing or logging libraries.\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\", \"title\": \"Suggest moving environment variable setting for early initialization\", \"target_snippet\": \"os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\"}], \"issue_snippet\": \"os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\", \"message\": \"The environment variable `OTEL_EXPORTER_OTLP_ENDPOINT` is set using `os.environ` after `logfire` has already been imported. While `logfire.configure` is called later, some underlying OpenTelemetry components might read environment variables during initial import. To guarantee that all parts of the tracing system correctly pick up the endpoint, it is best practice to set this environment variable at the very top of the script or externally, before any modules that rely on it are imported or initialized.\", \"severity\": \"warning\"}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_5e40d40a737047b5a62b48ad02faf511\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7990b4887589dc76e2c292c3a523","spanId":"96d9f6b0f21f29c0","parentSpanId":"a267ec6c991a8868","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762976182617855000","endTimeUnixNano":"1762976182617855000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 5 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"5"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a7990b4887589dc76e2c292c3a523","spanId":"979e46ec775c8caf","parentSpanId":"a267ec6c991a8868","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976182619675000","endTimeUnixNano":"1762976182619675000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'snippet.strip()...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"snippet.strip()"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7990b4887589dc76e2c292c3a523","spanId":"00baa4605b96f951","parentSpanId":"a267ec6c991a8868","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976182620085000","endTimeUnixNano":"1762976182620085000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 3 occurrences of 'positions[0]...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"3"}},{"key":"snippet[:20]","value":{"stringValue":"positions[0]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7990b4887589dc76e2c292c3a523","spanId":"a98fcfdb500e6e6d","parentSpanId":"a267ec6c991a8868","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976182620267000","endTimeUnixNano":"1762976182620267000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'max_cache_size...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"max_cache_size"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7990b4887589dc76e2c292c3a523","spanId":"848050cc160ef8e1","parentSpanId":"a267ec6c991a8868","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976182620543000","endTimeUnixNano":"1762976182620543000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 5 occurrences of 'doc.source...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"5"}},{"key":"snippet[:20]","value":{"stringValue":"doc.source"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7990b4887589dc76e2c292c3a523","spanId":"6b4331aed5f31833","parentSpanId":"a267ec6c991a8868","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976182620696000","endTimeUnixNano":"1762976182620696000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'os.environ[\"OTEL_EXP...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"os.environ[\"OTEL_EXP"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7990b4887589dc76e2c292c3a523","spanId":"a267ec6c991a8868","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976117896709000","endTimeUnixNano":"1762976182620803000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a7991b1605d965ac3adc536fde864","spanId":"85a94b70ec8a3b14","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762976182624050000","endTimeUnixNano":"1762976182624050000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 5 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"5"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c511d8c928e741c8b6e3305bc85905a5"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"61044"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7991e5f35cc72273ee833946fdee","spanId":"19c3fdc57ea64486","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762976196083865000","endTimeUnixNano":"1762976196083865000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"77"}}],"status":{}},{"traceId":"019a7991e743dc406d3686c905500a24","spanId":"f3267dd98b43704a","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762976196419340000","endTimeUnixNano":"1762976196419340000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"115"}}],"status":{}},{"traceId":"019a7991e75518fdc0297dd24a4a0f6d","spanId":"9204c95404044369","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976196437950000","endTimeUnixNano":"1762976196437950000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7991eb4061f8488d91e4acf1acb5","spanId":"deaeae4cb2af0f72","parentSpanId":"1a19f18d8cf91da9","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976197440799000","endTimeUnixNano":"1762976197440799000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c511d8c928e741c8b6e3305bc85905a5"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"61044"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a7991eb4061f8488d91e4acf1acb5","spanId":"7a88a484e66142da","parentSpanId":"49d5ac67b955f87b","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976197441761000","endTimeUnixNano":"1762976218607678000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"9365"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"1148"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"2348"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"9365"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal get_buffer_globals = function()\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_ad5034e3dc664109be4eaa9fffbcf8d3\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"error\", \"message\": \"The 'source' command for activating the virtual environment is constructed but never actually executed. Manually setting VIRTUAL_ENV and PATH is an incomplete activation and will miss other environment variables or shell functions that the 'activate' script typically sets. This can lead to unexpected behavior or Python not finding correctly configured packages.\", \"suggested_fixes\": [{\"replacement_snippet\": \"local command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n-- This command needs to be run in the shell that Neovim itself runs in,\\n-- or its direct child process, which is complex. For Neovim's internal\\n-- environment, setting vim.env variables is the primary mechanism.\\n-- Consider if the 'source' command is truly necessary or achievable\\n-- given Neovim's job control model. If a full shell environment is needed,\\n-- it might require restarting Neovim within the activated shell or more advanced setup.\\n-- For most cases, setting VIRTUAL_ENV and PATH in vim.env is sufficient for Neovim's internal Python provider.\", \"target_snippet\": \"local command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\", \"title\": \"Execute the source command via a shell, if Neovim's environment can be directly influenced.\"}], \"issue_snippet\": \"source\"}, {\"issue_snippet\": \"get_buffer_globals\", \"suggested_fixes\": [{\"replacement_snippet\": \"get_buffer_globals = function() -- Consider using a more robust Python parser (if available via external tool) or adding a disclaimer about limitations.\", \"target_snippet\": \"get_buffer_globals = function()\", \"title\": \"Improve global variable detection or provide a disclaimer.\"}], \"message\": \"The `get_buffer_globals` function uses fragile heuristics (indentation-based string matching) to identify imports and global variables. This method may incorrectly classify or miss context (e.g., variables defined within top-level if/try blocks, or complex module structures), leading to runtime errors when the extracted selection is run in a temporary file without its full lexical scope.\", \"severity\": \"warning\"}, {\"issue_snippet\": \"function_name .. \\\"()\\\"\", \"severity\": \"warning\", \"message\": \"In `M.run_python_selection`, when auto-calling a detected function, it is invoked without any arguments (e.g., `my_function()`). This will cause a runtime error if the function requires mandatory arguments.\", \"suggested_fixes\": [{\"target_snippet\": \"file:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\", \"title\": \"Add a comment about the limitation or provide a mechanism for argument input.\", \"replacement_snippet\": \"file:write(\\\"    result = \\\" .. function_name .. \\\"() -- NOTE: Assumes no arguments or all arguments have defaults. May fail if mandatory args are missing.\\\\n\\\")\"}]}, {\"suggested_fixes\": [{\"replacement_snippet\": \"file:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"() -- This call assumes the function takes no arguments or all arguments have defaults. Consider adding a mechanism for user input for function arguments.\\\\n\\\")\", \"title\": \"Provide a mechanism to input function arguments.\", \"target_snippet\": \"file:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\"}], \"message\": \"In `M.run_python_function`, when calling a selected function, it is explicitly invoked without any arguments (e.g., `module.my_function()`). This is a critical limitation; if the Python function requires mandatory arguments, this will result in a runtime error, severely limiting the utility of running functions that aren't self-contained or have defaults.\", \"issue_snippet\": \"func_name .. \\\"()\\\"\", \"severity\": \"error\"}, {\"issue_snippet\": \"vim.api.nvim_set_keymap\", \"severity\": \"info\", \"message\": \"For the main UV command menu (`prefix`) and environment management (`prefix .. \\\"e\\\"`) keymaps, the code defines mappings for both Snacks and Telescope pickers if both are detected. The keymap defined last will always overwrite the previous one. This creates a 'last writer wins' scenario without explicit user preference, potentially leading to inconsistent behavior or a picker being unexpectedly overridden.\", \"suggested_fixes\": [{\"title\": \"Prioritize picker integration or allow user configuration.\", \"target_snippet\": \"if _G.Snacks and _G.Snacks.picker then\", \"replacement_snippet\": \"-- if M.config.use_snacks_picker and _G.Snacks and _G.Snacks.picker then\\nif _G.Snacks and _G.Snacks.picker then\"}, {\"target_snippet\": \"if has_telescope then\", \"replacement_snippet\": \"-- elseif M.config.use_telescope_picker and has_telescope then\\n-- Or: if not (_G.Snacks and _G.Snacks.picker) and has_telescope then (prioritize Snacks if both are present)\\nif has_telescope then\", \"title\": \"Prioritize picker integration or allow user configuration (example for Telescope).\"}]}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a7991eb4061f8488d91e4acf1acb5","spanId":"49d5ac67b955f87b","parentSpanId":"1a19f18d8cf91da9","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976197441303000","endTimeUnixNano":"1762976218627590000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"source\", \"severity\": \"error\", \"message\": \"The 'source' command for activating the virtual environment is constructed but never actually executed. Manually setting VIRTUAL_ENV and PATH is an incomplete activation and will miss other environment variables or shell functions that the 'activate' script typically sets. This can lead to unexpected behavior or Python not finding correctly configured packages.\", \"suggested_fixes\": [{\"title\": \"Execute the source command via a shell, if Neovim's environment can be directly influenced.\", \"target_snippet\": \"local command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\", \"replacement_snippet\": \"local command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n-- This command needs to be run in the shell that Neovim itself runs in,\\n-- or its direct child process, which is complex. For Neovim's internal\\n-- environment, setting vim.env variables is the primary mechanism.\\n-- Consider if the 'source' command is truly necessary or achievable\\n-- given Neovim's job control model. If a full shell environment is needed,\\n-- it might require restarting Neovim within the activated shell or more advanced setup.\\n-- For most cases, setting VIRTUAL_ENV and PATH in vim.env is sufficient for Neovim's internal Python provider.\"}]}, {\"issue_snippet\": \"get_buffer_globals\", \"severity\": \"warning\", \"message\": \"The `get_buffer_globals` function uses fragile heuristics (indentation-based string matching) to identify imports and global variables. This method may incorrectly classify or miss context (e.g., variables defined within top-level if/try blocks, or complex module structures), leading to runtime errors when the extracted selection is run in a temporary file without its full lexical scope.\", \"suggested_fixes\": [{\"title\": \"Improve global variable detection or provide a disclaimer.\", \"target_snippet\": \"get_buffer_globals = function()\", \"replacement_snippet\": \"get_buffer_globals = function() -- Consider using a more robust Python parser (if available via external tool) or adding a disclaimer about limitations.\"}]}, {\"issue_snippet\": \"function_name .. \\\"()\\\"\", \"severity\": \"warning\", \"message\": \"In `M.run_python_selection`, when auto-calling a detected function, it is invoked without any arguments (e.g., `my_function()`). This will cause a runtime error if the function requires mandatory arguments.\", \"suggested_fixes\": [{\"title\": \"Add a comment about the limitation or provide a mechanism for argument input.\", \"target_snippet\": \"file:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\", \"replacement_snippet\": \"file:write(\\\"    result = \\\" .. function_name .. \\\"() -- NOTE: Assumes no arguments or all arguments have defaults. May fail if mandatory args are missing.\\\\n\\\")\"}]}, {\"issue_snippet\": \"func_name .. \\\"()\\\"\", \"severity\": \"error\", \"message\": \"In `M.run_python_function`, when calling a selected function, it is explicitly invoked without any arguments (e.g., `module.my_function()`). This is a critical limitation; if the Python function requires mandatory arguments, this will result in a runtime error, severely limiting the utility of running functions that aren't self-contained or have defaults.\", \"suggested_fixes\": [{\"title\": \"Provide a mechanism to input function arguments.\", \"target_snippet\": \"file:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\", \"replacement_snippet\": \"file:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"() -- This call assumes the function takes no arguments or all arguments have defaults. Consider adding a mechanism for user input for function arguments.\\\\n\\\")\"}]}, {\"issue_snippet\": \"vim.api.nvim_set_keymap\", \"severity\": \"info\", \"message\": \"For the main UV command menu (`prefix`) and environment management (`prefix .. \\\"e\\\"`) keymaps, the code defines mappings for both Snacks and Telescope pickers if both are detected. The keymap defined last will always overwrite the previous one. This creates a 'last writer wins' scenario without explicit user preference, potentially leading to inconsistent behavior or a picker being unexpectedly overridden.\", \"suggested_fixes\": [{\"title\": \"Prioritize picker integration or allow user configuration.\", \"target_snippet\": \"if _G.Snacks and _G.Snacks.picker then\", \"replacement_snippet\": \"-- if M.config.use_snacks_picker and _G.Snacks and _G.Snacks.picker then\\nif _G.Snacks and _G.Snacks.picker then\"}, {\"title\": \"Prioritize picker integration or allow user configuration (example for Telescope).\", \"target_snippet\": \"if has_telescope then\", \"replacement_snippet\": \"-- elseif M.config.use_telescope_picker and has_telescope then\\n-- Or: if not (_G.Snacks and _G.Snacks.picker) and has_telescope then (prioritize Snacks if both are present)\\nif has_telescope then\"}]}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"9365"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"1148"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"2348"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"9365"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal get_buffer_globals = function()\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_ad5034e3dc664109be4eaa9fffbcf8d3\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"error\", \"message\": \"The 'source' command for activating the virtual environment is constructed but never actually executed. Manually setting VIRTUAL_ENV and PATH is an incomplete activation and will miss other environment variables or shell functions that the 'activate' script typically sets. This can lead to unexpected behavior or Python not finding correctly configured packages.\", \"suggested_fixes\": [{\"replacement_snippet\": \"local command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n-- This command needs to be run in the shell that Neovim itself runs in,\\n-- or its direct child process, which is complex. For Neovim's internal\\n-- environment, setting vim.env variables is the primary mechanism.\\n-- Consider if the 'source' command is truly necessary or achievable\\n-- given Neovim's job control model. If a full shell environment is needed,\\n-- it might require restarting Neovim within the activated shell or more advanced setup.\\n-- For most cases, setting VIRTUAL_ENV and PATH in vim.env is sufficient for Neovim's internal Python provider.\", \"target_snippet\": \"local command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\", \"title\": \"Execute the source command via a shell, if Neovim's environment can be directly influenced.\"}], \"issue_snippet\": \"source\"}, {\"issue_snippet\": \"get_buffer_globals\", \"suggested_fixes\": [{\"replacement_snippet\": \"get_buffer_globals = function() -- Consider using a more robust Python parser (if available via external tool) or adding a disclaimer about limitations.\", \"target_snippet\": \"get_buffer_globals = function()\", \"title\": \"Improve global variable detection or provide a disclaimer.\"}], \"message\": \"The `get_buffer_globals` function uses fragile heuristics (indentation-based string matching) to identify imports and global variables. This method may incorrectly classify or miss context (e.g., variables defined within top-level if/try blocks, or complex module structures), leading to runtime errors when the extracted selection is run in a temporary file without its full lexical scope.\", \"severity\": \"warning\"}, {\"issue_snippet\": \"function_name .. \\\"()\\\"\", \"severity\": \"warning\", \"message\": \"In `M.run_python_selection`, when auto-calling a detected function, it is invoked without any arguments (e.g., `my_function()`). This will cause a runtime error if the function requires mandatory arguments.\", \"suggested_fixes\": [{\"target_snippet\": \"file:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\", \"title\": \"Add a comment about the limitation or provide a mechanism for argument input.\", \"replacement_snippet\": \"file:write(\\\"    result = \\\" .. function_name .. \\\"() -- NOTE: Assumes no arguments or all arguments have defaults. May fail if mandatory args are missing.\\\\n\\\")\"}]}, {\"suggested_fixes\": [{\"replacement_snippet\": \"file:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"() -- This call assumes the function takes no arguments or all arguments have defaults. Consider adding a mechanism for user input for function arguments.\\\\n\\\")\", \"title\": \"Provide a mechanism to input function arguments.\", \"target_snippet\": \"file:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\"}], \"message\": \"In `M.run_python_function`, when calling a selected function, it is explicitly invoked without any arguments (e.g., `module.my_function()`). This is a critical limitation; if the Python function requires mandatory arguments, this will result in a runtime error, severely limiting the utility of running functions that aren't self-contained or have defaults.\", \"issue_snippet\": \"func_name .. \\\"()\\\"\", \"severity\": \"error\"}, {\"issue_snippet\": \"vim.api.nvim_set_keymap\", \"severity\": \"info\", \"message\": \"For the main UV command menu (`prefix`) and environment management (`prefix .. \\\"e\\\"`) keymaps, the code defines mappings for both Snacks and Telescope pickers if both are detected. The keymap defined last will always overwrite the previous one. This creates a 'last writer wins' scenario without explicit user preference, potentially leading to inconsistent behavior or a picker being unexpectedly overridden.\", \"suggested_fixes\": [{\"title\": \"Prioritize picker integration or allow user configuration.\", \"target_snippet\": \"if _G.Snacks and _G.Snacks.picker then\", \"replacement_snippet\": \"-- if M.config.use_snacks_picker and _G.Snacks and _G.Snacks.picker then\\nif _G.Snacks and _G.Snacks.picker then\"}, {\"target_snippet\": \"if has_telescope then\", \"replacement_snippet\": \"-- elseif M.config.use_telescope_picker and has_telescope then\\n-- Or: if not (_G.Snacks and _G.Snacks.picker) and has_telescope then (prioritize Snacks if both are present)\\nif has_telescope then\", \"title\": \"Prioritize picker integration or allow user configuration (example for Telescope).\"}]}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_ad5034e3dc664109be4eaa9fffbcf8d3\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7991eb4061f8488d91e4acf1acb5","spanId":"4342464001f486c7","parentSpanId":"1a19f18d8cf91da9","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762976218643886000","endTimeUnixNano":"1762976218643886000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 5 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"5"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a7991eb4061f8488d91e4acf1acb5","spanId":"5b92e028af31af4d","parentSpanId":"1a19f18d8cf91da9","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976218646388000","endTimeUnixNano":"1762976218646388000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 6 occurrences of 'source...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"6"}},{"key":"snippet[:20]","value":{"stringValue":"source"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7991eb4061f8488d91e4acf1acb5","spanId":"80acd8aa47e3c0be","parentSpanId":"1a19f18d8cf91da9","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976218646800000","endTimeUnixNano":"1762976218646800000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 2 occurrences of 'get_buffer_globals...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"2"}},{"key":"snippet[:20]","value":{"stringValue":"get_buffer_globals"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7991eb4061f8488d91e4acf1acb5","spanId":"0214eb00e06f6424","parentSpanId":"1a19f18d8cf91da9","flags":256,"name":"Could not find snippet '{snippet}' in text","kind":1,"startTimeUnixNano":"1762976218647310000","endTimeUnixNano":"1762976218647310000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"13"}},{"key":"logfire.msg_template","value":{"stringValue":"Could not find snippet '{snippet}' in text"}},{"key":"logfire.msg","value":{"stringValue":"Could not find snippet 'function_name .. \"()\"' in text"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"149"}},{"key":"snippet","value":{"stringValue":"function_name .. \"()\""}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"snippet\":{}}}"}}],"status":{}},{"traceId":"019a7991eb4061f8488d91e4acf1acb5","spanId":"e9b12896196e9a4b","parentSpanId":"1a19f18d8cf91da9","flags":256,"name":"Could not find snippet '{snippet}' in text","kind":1,"startTimeUnixNano":"1762976218647483000","endTimeUnixNano":"1762976218647483000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"13"}},{"key":"logfire.msg_template","value":{"stringValue":"Could not find snippet '{snippet}' in text"}},{"key":"logfire.msg","value":{"stringValue":"Could not find snippet 'func_name .. \"()\"' in text"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"149"}},{"key":"snippet","value":{"stringValue":"func_name .. \"()\""}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"snippet\":{}}}"}}],"status":{}},{"traceId":"019a7991eb4061f8488d91e4acf1acb5","spanId":"cf9571a860f86d60","parentSpanId":"1a19f18d8cf91da9","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976218648663000","endTimeUnixNano":"1762976218648663000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 14 occurrences of 'vim.api.nvim_set_key...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"14"}},{"key":"snippet[:20]","value":{"stringValue":"vim.api.nvim_set_key"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7991eb4061f8488d91e4acf1acb5","spanId":"1a19f18d8cf91da9","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976197440411000","endTimeUnixNano":"1762976218648794000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79923e1cd85890ad533a96a10360","spanId":"06731a2daf54a341","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762976218652323000","endTimeUnixNano":"1762976218652323000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 3 diagnostics for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"3"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c511d8c928e741c8b6e3305bc85905a5"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"61044"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7992bff8125e022f0d3b2b075d6b","spanId":"f0c21c17de54fce0","parentSpanId":"d4f960a2f69b7237","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976251896888000","endTimeUnixNano":"1762976251896888000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'local command = \"sou...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"local command = \"sou"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a7992bff8125e022f0d3b2b075d6b","spanId":"9125c2cc09559270","parentSpanId":"d4f960a2f69b7237","flags":256,"name":"creating_quick_fix","kind":1,"startTimeUnixNano":"1762976251897190000","endTimeUnixNano":"1762976251897379000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"332"}},{"key":"title","value":{"stringValue":"Execute the source command via a shell, if Neovim's environment can be directly influenced."}},{"key":"logfire.msg_template","value":{"stringValue":"creating_quick_fix"}},{"key":"logfire.msg","value":{"stringValue":"creating_quick_fix"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"title\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a7992bff8125e022f0d3b2b075d6b","spanId":"d4f960a2f69b7237","flags":256,"name":"code_actions","kind":1,"startTimeUnixNano":"1762976251896509000","endTimeUnixNano":"1762976251897472000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"315"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"code_actions"}},{"key":"logfire.msg","value":{"stringValue":"code_actions"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a7992cd4df6752e7bb8df8b03878e","spanId":"90d7a3e8cc0a480f","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976255309712000","endTimeUnixNano":"1762976255309712000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c511d8c928e741c8b6e3305bc85905a5"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"61044"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7992d137adeb26806e54d71d5a20","spanId":"0b34e805d7176b65","parentSpanId":"85379cb0c6d7fbda","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976256312535000","endTimeUnixNano":"1762976256312535000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7992d5e76fdc8d03ce9ea29953ff","spanId":"b724c5cc968eb90e","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976257511135000","endTimeUnixNano":"1762976257511135000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7992d137adeb26806e54d71d5a20","spanId":"85379cb0c6d7fbda","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976256311561000","endTimeUnixNano":"1762976257534131000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976257522179000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a7992d5fff4b3e43b6b3a5085cffa","spanId":"d1a8444086d43fcd","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976257535372000","endTimeUnixNano":"1762976257535372000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7992d9d0c22ac6a3e062c1a002a9","spanId":"e0861a2cef113088","parentSpanId":"7de2285dd99fc87a","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976258513305000","endTimeUnixNano":"1762976258513305000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a7992d137adeb26806e54d71d5a20","spanId":"7130a120b52ed69d","parentSpanId":"bb5562151f69f024","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976256317374000","endTimeUnixNano":"1762976257511907000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a7992d137adeb26806e54d71d5a20","spanId":"bb5562151f69f024","parentSpanId":"85379cb0c6d7fbda","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976256315640000","endTimeUnixNano":"1762976257512476000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n-- This command needs to be run in the shell that Neovim itself runs in,\\n-- or its direct child process, which is complex. For Neovim's internal\\n-- environment, setting vim.env variables is the primary mechanism.\\n-- Consider if the 'source' command is truly necessary or achievable\\n-- given Neovim's job control model. If a full shell environment is needed,\\n-- it might require restarting Neovim within the activated shell or more advanced setup.\\n-- For most cases, setting VIRTUAL_ENV and PATH in vim.env is sufficient for Neovim's internal Python provider.\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal get_buffer_globals = function()\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c511d8c928e741c8b6e3305bc85905a5"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"61044"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79930cddb12af4b4bafdf3d6828b","spanId":"6ca03034e847f807","parentSpanId":"dc877c673c09ac94","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976271581475000","endTimeUnixNano":"1762976271581475000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'get_buffer_globals =...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"get_buffer_globals ="}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79930cddb12af4b4bafdf3d6828b","spanId":"39b04fa7221c2d5e","parentSpanId":"dc877c673c09ac94","flags":256,"name":"creating_quick_fix","kind":1,"startTimeUnixNano":"1762976271581639000","endTimeUnixNano":"1762976271581770000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"332"}},{"key":"title","value":{"stringValue":"Improve global variable detection or provide a disclaimer."}},{"key":"logfire.msg_template","value":{"stringValue":"creating_quick_fix"}},{"key":"logfire.msg","value":{"stringValue":"creating_quick_fix"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"title\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79930cddb12af4b4bafdf3d6828b","spanId":"dc877c673c09ac94","flags":256,"name":"code_actions","kind":1,"startTimeUnixNano":"1762976271581146000","endTimeUnixNano":"1762976271581834000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"315"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"code_actions"}},{"key":"logfire.msg","value":{"stringValue":"code_actions"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79931260f548a70bb5c3b3fcaee4","spanId":"edd119acacb4c933","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976272992562000","endTimeUnixNano":"1762976272992562000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7992d9d0c22ac6a3e062c1a002a9","spanId":"7de2285dd99fc87a","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976258512963000","endTimeUnixNano":"1762976273001315000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976272999414000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79931269327c5bc4d68a0276044b","spanId":"2959c41df28221e7","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976273001495000","endTimeUnixNano":"1762976273001495000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7993164ad975275850b2f5a9db23","spanId":"69fd3ddad1dfde00","parentSpanId":"33719589e5eeb918","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976273994671000","endTimeUnixNano":"1762976273994671000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a7992d9d0c22ac6a3e062c1a002a9","spanId":"119ddc265c31f69b","parentSpanId":"aea5bc6b185b8639","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976258514466000","endTimeUnixNano":"1762976272992993000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a7992d9d0c22ac6a3e062c1a002a9","spanId":"aea5bc6b185b8639","parentSpanId":"7de2285dd99fc87a","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976258513940000","endTimeUnixNano":"1762976272993223000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal get_buffer_globals = function()\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c511d8c928e741c8b6e3305bc85905a5"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"61044"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79932d7150b56cac11e2b16e3243","spanId":"59d563c10b5ceef8","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976279921809000","endTimeUnixNano":"1762976279921809000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7993164ad975275850b2f5a9db23","spanId":"33719589e5eeb918","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976273994236000","endTimeUnixNano":"1762976279936189000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976279933701000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79932d800821e0683961141a925f","spanId":"9316692a72454748","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976279936408000","endTimeUnixNano":"1762976279936408000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7993315c9c41b736ecbc6b2ccb44","spanId":"25438de0bcfa6761","parentSpanId":"550e2de3540266d9","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976280926015000","endTimeUnixNano":"1762976280926015000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a7993164ad975275850b2f5a9db23","spanId":"7466ae4623eaed17","parentSpanId":"e3c93283bc84b8af","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976273996070000","endTimeUnixNano":"1762976279923407000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a7993164ad975275850b2f5a9db23","spanId":"e3c93283bc84b8af","parentSpanId":"33719589e5eeb918","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976273995462000","endTimeUnixNano":"1762976279923907000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal get_buffer_globals = function() -- Consider using a more robust Python parser (if available via external tool) or adding a disclaimer about limitations.\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c511d8c928e741c8b6e3305bc85905a5"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"61044"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a799350b0ac6121e1116984fe1134","spanId":"e44ccc8bc0a950ba","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976288944789000","endTimeUnixNano":"1762976288944789000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7993315c9c41b736ecbc6b2ccb44","spanId":"550e2de3540266d9","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976280924916000","endTimeUnixNano":"1762976288955013000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976288953275000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a799350bba5084306da28b1d1d306","spanId":"91a2b9461ec25966","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976288955189000","endTimeUnixNano":"1762976288955189000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a7993549b8291b7082827b06faf93","spanId":"24438a890c8c217b","parentSpanId":"dbf1d8d5b0951067","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976289948480000","endTimeUnixNano":"1762976289948480000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799354f83953afd0d5e669c127e6","spanId":"0e24282e27f66795","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976290040861000","endTimeUnixNano":"1762976290040861000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a7993549b8291b7082827b06faf93","spanId":"dbf1d8d5b0951067","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976289947622000","endTimeUnixNano":"1762976290050616000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976290048664000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79935502420515f4ff325b4d9cf9","spanId":"bf7e0ea9b3793076","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976290050802000","endTimeUnixNano":"1762976290050802000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799358e35e2d8f7adcb3e78a18ef","spanId":"92643f81bda77c54","parentSpanId":"7842d85ebfa626f4","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976291044111000","endTimeUnixNano":"1762976291044111000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a7993315c9c41b736ecbc6b2ccb44","spanId":"89188f6c834f9543","parentSpanId":"c40ad79c357f8e20","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976280931548000","endTimeUnixNano":"1762976288945624000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a7993315c9c41b736ecbc6b2ccb44","spanId":"c40ad79c357f8e20","parentSpanId":"550e2de3540266d9","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976280928450000","endTimeUnixNano":"1762976288946037000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal get_buffer_globals = function()\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a7993549b8291b7082827b06faf93","spanId":"3e628549a10d59bf","parentSpanId":"bf59872d9948c3b8","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976289951268000","endTimeUnixNano":"1762976290041442000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a7993549b8291b7082827b06faf93","spanId":"bf59872d9948c3b8","parentSpanId":"dbf1d8d5b0951067","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976289950056000","endTimeUnixNano":"1762976290041718000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal wget_buffer_globals = function()\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c511d8c928e741c8b6e3305bc85905a5"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"61044"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a799359bafafc370ca6511bc0c6d8","spanId":"a326d50776b97e23","parentSpanId":"1d52486aea3a7139","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976291258499000","endTimeUnixNano":"1762976291258499000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'get_buffer_globals =...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"get_buffer_globals ="}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a799359bafafc370ca6511bc0c6d8","spanId":"b52ade61752319b7","parentSpanId":"1d52486aea3a7139","flags":256,"name":"creating_quick_fix","kind":1,"startTimeUnixNano":"1762976291258663000","endTimeUnixNano":"1762976291258774000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"332"}},{"key":"title","value":{"stringValue":"Improve global variable detection or provide a disclaimer."}},{"key":"logfire.msg_template","value":{"stringValue":"creating_quick_fix"}},{"key":"logfire.msg","value":{"stringValue":"creating_quick_fix"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"title\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a799359bafafc370ca6511bc0c6d8","spanId":"1d52486aea3a7139","flags":256,"name":"code_actions","kind":1,"startTimeUnixNano":"1762976291258172000","endTimeUnixNano":"1762976291258834000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"315"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"code_actions"}},{"key":"logfire.msg","value":{"stringValue":"code_actions"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79936049001d04db7f086b4f9c68","spanId":"1ba283d927554e41","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976292937627000","endTimeUnixNano":"1762976292937627000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a799358e35e2d8f7adcb3e78a18ef","spanId":"7842d85ebfa626f4","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976291042977000","endTimeUnixNano":"1762976292949328000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976292946612000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a799360559e359275b103b90ed45a","spanId":"9721c40bc531787d","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976292949637000","endTimeUnixNano":"1762976292949637000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799364326cee488560e7eb7ab3d1","spanId":"35e901ac017bf5fa","parentSpanId":"dab51a9ad7dfa04c","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976293938854000","endTimeUnixNano":"1762976293938854000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a799358e35e2d8f7adcb3e78a18ef","spanId":"ebdac6bd7a3c326b","parentSpanId":"f553bdb1627acdd7","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976291047895000","endTimeUnixNano":"1762976292938170000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a799358e35e2d8f7adcb3e78a18ef","spanId":"f553bdb1627acdd7","parentSpanId":"7842d85ebfa626f4","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976291046452000","endTimeUnixNano":"1762976292938527000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal get_buffer_globals = function()\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c511d8c928e741c8b6e3305bc85905a5"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"61044"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a7993700395295bd8b39d46d24d78","spanId":"dc06e6140d1f790d","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976296963585000","endTimeUnixNano":"1762976296963585000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a799364326cee488560e7eb7ab3d1","spanId":"dab51a9ad7dfa04c","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976293938554000","endTimeUnixNano":"1762976296971412000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976296969794000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a7993700bd6f0446cd8987b0f1a1c","spanId":"eff100b3bfe69f79","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976296971564000","endTimeUnixNano":"1762976296971564000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a799373eddc0fe8ee93bcc78d477e","spanId":"8c7bad5cb82316fc","parentSpanId":"9ef187a778fbafed","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976297966451000","endTimeUnixNano":"1762976297966451000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a799364326cee488560e7eb7ab3d1","spanId":"433c10b14d3429f7","parentSpanId":"2771dac5583c392e","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976293940035000","endTimeUnixNano":"1762976296963976000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a799364326cee488560e7eb7ab3d1","spanId":"2771dac5583c392e","parentSpanId":"dab51a9ad7dfa04c","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976293939468000","endTimeUnixNano":"1762976296964184000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal get_buffer_globals = function() -- Consider using a more robust Python parser (if available via external tool) or adding a disclaimer about limitations.\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c511d8c928e741c8b6e3305bc85905a5"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"61044"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a799386966f4e605d867ddffbd7ff","spanId":"336014b8882ee4c1","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976302742477000","endTimeUnixNano":"1762976302742477000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a799373eddc0fe8ee93bcc78d477e","spanId":"9ef187a778fbafed","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976297965521000","endTimeUnixNano":"1762976302752353000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762976302750239000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a799386a095eab712cb77eb0ef7df","spanId":"0198d52faf8634b4","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762976302752547000","endTimeUnixNano":"1762976302752547000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79938a808eec8f491953fb623024","spanId":"d16680ac332d5b8b","parentSpanId":"e5e7e1831fbcef48","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976303744628000","endTimeUnixNano":"1762976303744628000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a799373eddc0fe8ee93bcc78d477e","spanId":"2e875b7043f85199","parentSpanId":"8353c60911f349aa","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976297968714000","endTimeUnixNano":"1762976302743041000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a799373eddc0fe8ee93bcc78d477e","spanId":"8353c60911f349aa","parentSpanId":"9ef187a778fbafed","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976297967830000","endTimeUnixNano":"1762976302743325000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this lua code for issues:\\n\\n```lua\\n-- uv.nvim - Neovim plugin for uv Python package management integration\\n-- Author: Ben O'Mahony\\n-- License: MIT\\n\\nlocal M = {}\\n\\n-- Default configuration\\nM.config = {\\n\\t-- Auto-activate virtual environments when found\\n\\tauto_activate_venv = true,\\n\\tnotify_activate_venv = true,\\n\\n\\t-- Auto commands for directory changes\\n\\tauto_commands = true,\\n\\n\\t-- Integration with picker (like Telescope or other UI components)\\n\\tpicker_integration = true,\\n\\n\\t-- Keymaps to register (set to false to disable)\\n\\tkeymaps = {\\n\\t\\tprefix = \\\"<leader>x\\\", -- Main prefix for UV commands\\n\\t\\tcommands = true, -- Show UV commands menu (<leader>x)\\n\\t\\trun_file = true, -- Run current file (<leader>xr)\\n\\t\\trun_selection = true, -- Run selection (<leader>xs)\\n\\t\\trun_function = true, -- Run function (<leader>xf)\\n\\t\\tvenv = true, -- Environment management (<leader>xe)\\n\\t\\tinit = true, -- Initialize UV project (<leader>xi)\\n\\t\\tadd = true, -- Add a package (<leader>xa)\\n\\t\\tremove = true, -- Remove a package (<leader>xd)\\n\\t\\tsync = true, -- Sync packages (<leader>xc)\\n\\t\\tsync_all = true, -- uv sync --all-extras --all-groups --all-packages (<leader>xC)\\n\\t},\\n\\n\\t-- Execution options\\n\\texecution = {\\n\\t\\t-- Python run command template\\n\\t\\trun_command = \\\"uv run python\\\",\\n\\n\\t\\t-- Show output in notifications\\n\\t\\tnotify_output = true,\\n\\n\\t\\t-- Notification timeout in ms\\n\\t\\tnotification_timeout = 10000,\\n\\t},\\n}\\n\\n-- Command runner - runs shell commands and captures output\\nfunction M.run_command(cmd)\\n\\t-- Run command in background and capture output\\n\\tvim.fn.jobstart(cmd, {\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Command completed successfully: \\\" .. cmd, vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Command failed: \\\" .. cmd, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Only show meaningful output (not empty lines)\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t-- Show errors\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.WARN)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Virtual environment activation\\nfunction M.activate_venv(venv_path)\\n\\t-- For Mac, run the source command to apply to the current shell\\n\\tlocal command = \\\"source \\\" .. venv_path .. \\\"/bin/activate\\\"\\n\\t-- Set environment variables for the current Neovim instance\\n\\tvim.env.VIRTUAL_ENV = venv_path\\n\\tvim.env.PATH = venv_path .. \\\"/bin:\\\" .. vim.env.PATH\\n\\t-- Notify user\\n\\tif M.config.notify_activate_venv then\\n\\t\\tvim.notify(\\\"Activated virtual environment: \\\" .. venv_path, vim.log.levels.INFO)\\n\\tend\\nend\\n\\n-- Auto-activate the .venv if it exists at the project root\\nfunction M.auto_activate_venv()\\n\\tlocal venv_path = vim.fn.getcwd() .. \\\"/.venv\\\"\\n\\tif vim.fn.isdirectory(venv_path) == 1 then\\n\\t\\tM.activate_venv(venv_path)\\n\\t\\treturn true\\n\\tend\\n\\treturn false\\nend\\n\\n-- Function to create a temporary file with the necessary context and selected code\\nfunction M.run_python_selection()\\n\\t-- Get visual selection\\n\\tlocal get_visual_selection = function()\\n\\t\\tlocal start_pos = vim.fn.getpos(\\\"'<\\\")\\n\\t\\tlocal end_pos = vim.fn.getpos(\\\"'>\\\")\\n\\t\\tlocal lines = vim.fn.getline(start_pos[2], end_pos[2])\\n\\n\\t\\tif #lines == 0 then\\n\\t\\t\\treturn \\\"\\\"\\n\\t\\tend\\n\\n\\t\\t-- Adjust last line to end at the column position of end_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[#lines] = lines[#lines]:sub(1, end_pos[3])\\n\\t\\tend\\n\\n\\t\\t-- Adjust first line to start at the column position of start_pos\\n\\t\\tif #lines > 0 then\\n\\t\\t\\tlines[1] = lines[1]:sub(start_pos[3])\\n\\t\\tend\\n\\n\\t\\treturn table.concat(lines, \\\"\\\\n\\\")\\n\\tend\\n\\n\\t-- Get current buffer content to extract imports and global variables\\n\\tlocal get_buffer_globals = function() -- Consider using a more robust Python parser (if available via external tool) or adding a disclaimer about limitations.\\n\\t\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\t\\tlocal imports = {}\\n\\t\\tlocal globals = {}\\n\\t\\tlocal in_class = false\\n\\t\\tlocal class_indent = 0\\n\\n\\t\\tfor _, line in ipairs(lines) do\\n\\t\\t\\t-- Detect imports\\n\\t\\t\\tif line:match(\\\"^%s*import \\\") or line:match(\\\"^%s*from .+ import\\\") then\\n\\t\\t\\t\\ttable.insert(imports, line)\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect class definitions to skip class variables\\n\\t\\t\\tif line:match(\\\"^%s*class \\\") then\\n\\t\\t\\t\\tin_class = true\\n\\t\\t\\t\\tclass_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Check if we're exiting a class block\\n\\t\\t\\tif in_class and line:match(\\\"^%s*[^%s#]\\\") then\\n\\t\\t\\t\\tlocal current_indent = line:match(\\\"^(%s*)\\\"):len()\\n\\t\\t\\t\\tif current_indent <= class_indent then\\n\\t\\t\\t\\t\\tin_class = false\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\n\\t\\t\\t-- Detect global variable assignments (not in class, not inside functions)\\n\\t\\t\\tif not in_class and not line:match(\\\"^%s*def \\\") and line:match(\\\"^%s*[%w_]+ *=\\\") then\\n\\t\\t\\t\\t-- Check if it's not indented (global scope)\\n\\t\\t\\t\\tif not line:match(\\\"^%s%s+\\\") then\\n\\t\\t\\t\\t\\ttable.insert(globals, line)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend\\n\\n\\t\\treturn imports, globals\\n\\tend\\n\\n\\t-- Get selected code\\n\\tlocal selection = get_visual_selection()\\n\\tif selection == \\\"\\\" then\\n\\t\\tvim.notify(\\\"No code selected\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Get imports and globals\\n\\tlocal imports, globals = get_buffer_globals()\\n\\n\\t-- Create temp file\\n\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\tlocal temp_file = temp_dir .. \\\"/run_selection.py\\\"\\n\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\tif not file then\\n\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Write imports\\n\\tfor _, imp in ipairs(imports) do\\n\\t\\tfile:write(imp .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write globals\\n\\tfor _, glob in ipairs(globals) do\\n\\t\\tfile:write(glob .. \\\"\\\\n\\\")\\n\\tend\\n\\tfile:write(\\\"\\\\n\\\")\\n\\n\\t-- Write selected code\\n\\tfile:write(\\\"# SELECTED CODE\\\\n\\\")\\n\\n\\t-- Check if the selection is all indented (which would cause syntax errors)\\n\\tlocal is_all_indented = true\\n\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tif not line:match(\\\"^%s+\\\") and line ~= \\\"\\\" then\\n\\t\\t\\tis_all_indented = false\\n\\t\\t\\tbreak\\n\\t\\tend\\n\\tend\\n\\n\\t-- Process the selection to determine what type of code it is\\n\\tlocal is_function_def = selection:match(\\\"^%s*def%s+[%w_]+%s*%(\\\")\\n\\tlocal is_class_def = selection:match(\\\"^%s*class%s+[%w_]+\\\")\\n\\tlocal has_print = selection:match(\\\"print%s*%(\\\")\\n\\tlocal is_expression = not is_function_def\\n\\t\\tand not is_class_def\\n\\t\\tand not selection:match(\\\"=\\\")\\n\\t\\tand not selection:match(\\\"%s*for%s+\\\")\\n\\t\\tand not selection:match(\\\"%s*if%s+\\\")\\n\\t\\tand not has_print\\n\\n\\t-- If the selection is all indented, we need to dedent it or wrap it in a function\\n\\tif is_all_indented then\\n\\t\\tfile:write(\\\"def run_selection():\\\\n\\\")\\n\\t\\t-- Write the selection with original indentation\\n\\t\\tfor line in selection:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\t\\tfile:write(\\\"    \\\" .. line .. \\\"\\\\n\\\")\\n\\t\\tend\\n\\t\\tfile:write(\\\"\\\\n# Auto-call the wrapper function\\\\n\\\")\\n\\t\\tfile:write(\\\"run_selection()\\\\n\\\")\\n\\telse\\n\\t\\t-- Write the original selection\\n\\t\\tfile:write(selection .. \\\"\\\\n\\\")\\n\\n\\t\\t-- For expressions, we'll add a print statement to see the result\\n\\t\\tif is_expression then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added print for expression\\\\n\\\")\\n\\t\\t\\tfile:write('print(f\\\"Expression result: {' .. selection:gsub(\\\"^%s+\\\", \\\"\\\"):gsub(\\\"%s+$\\\", \\\"\\\") .. '}\\\")\\\\n')\\n\\t\\t-- For function definitions without calls, we'll add a call\\n\\t\\telseif is_function_def then\\n\\t\\t\\tlocal function_name = selection:match(\\\"def%s+([%w_]+)%s*%(\\\")\\n\\t\\t\\t-- Check if the function is already called in the selection\\n\\t\\t\\tif function_name and not selection:match(function_name .. \\\"%s*%(.-%)\\\") then\\n\\t\\t\\t\\tfile:write(\\\"\\\\n# Auto-added function call\\\\n\\\")\\n\\t\\t\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\t\\t\\tfile:write('    print(f\\\"Auto-executing function: ' .. function_name .. '\\\")\\\\n')\\n\\t\\t\\t\\tfile:write(\\\"    result = \\\" .. function_name .. \\\"()\\\\n\\\")\\n\\t\\t\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\t\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\t\\tend\\n\\t\\t-- If there's no print statement in the code, add an output marker\\n\\t\\telseif not has_print and not selection:match(\\\"^%s*#\\\") then\\n\\t\\t\\tfile:write(\\\"\\\\n# Auto-added execution marker\\\\n\\\")\\n\\t\\t\\tfile:write('print(\\\"Code executed successfully.\\\")\\\\n')\\n\\t\\tend\\n\\tend\\n\\n\\tfile:close()\\n\\n\\t-- Run the temp file\\n\\tvim.notify(\\\"Running selected code...\\\", vim.log.levels.INFO)\\n\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\ton_stdout = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_stderr = function(_, data)\\n\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\tvim.notify(\\\"Selected code executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\telse\\n\\t\\t\\t\\tvim.notify(\\\"Selected code execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\tend\\n\\t\\tend,\\n\\t\\tstdout_buffered = true,\\n\\t\\tstderr_buffered = true,\\n\\t})\\nend\\n\\n-- Function to run a specific Python function\\nfunction M.run_python_function()\\n\\t-- Get current buffer content\\n\\tlocal lines = vim.api.nvim_buf_get_lines(0, 0, -1, false)\\n\\tlocal buffer_content = table.concat(lines, \\\"\\\\n\\\")\\n\\n\\t-- Find all function definitions\\n\\tlocal functions = {}\\n\\tfor line in buffer_content:gmatch(\\\"[^\\\\r\\\\n]+\\\") do\\n\\t\\tlocal func_name = line:match(\\\"^def%s+([%w_]+)%s*%(\\\")\\n\\t\\tif func_name then\\n\\t\\t\\ttable.insert(functions, func_name)\\n\\t\\tend\\n\\tend\\n\\n\\tif #functions == 0 then\\n\\t\\tvim.notify(\\\"No functions found in current file\\\", vim.log.levels.WARN)\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Create temp file for function selection picker\\n\\tlocal run_function = function(func_name)\\n\\t\\t-- Create a temporary file with a main block to call the function\\n\\t\\tlocal temp_dir = vim.fn.expand(\\\"$HOME\\\") .. \\\"/.cache/nvim/uv_run\\\"\\n\\t\\tvim.fn.mkdir(temp_dir, \\\"p\\\")\\n\\t\\tlocal temp_file = temp_dir .. \\\"/run_function.py\\\"\\n\\t\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\n\\t\\t-- Create a wrapper script that imports the current file and calls the function\\n\\t\\tlocal file = io.open(temp_file, \\\"w\\\")\\n\\t\\tif not file then\\n\\t\\t\\tvim.notify(\\\"Failed to create temporary file\\\", vim.log.levels.ERROR)\\n\\t\\t\\treturn\\n\\t\\tend\\n\\n\\t\\t-- Get the module name (file name without .py)\\n\\t\\tlocal module_name = vim.fn.fnamemodify(current_file, \\\":t:r\\\")\\n\\t\\tlocal module_dir = vim.fn.fnamemodify(current_file, \\\":h\\\")\\n\\n\\t\\t-- Write imports\\n\\t\\tfile:write(\\\"import sys\\\\n\\\")\\n\\t\\tfile:write(\\\"sys.path.insert(0, \\\" .. vim.inspect(module_dir) .. \\\")\\\\n\\\")\\n\\t\\tfile:write(\\\"import \\\" .. module_name .. \\\"\\\\n\\\\n\\\")\\n\\t\\tfile:write('if __name__ == \\\"__main__\\\":\\\\n')\\n\\t\\tfile:write('    print(f\\\"Running function: ' .. func_name .. '\\\")\\\\n')\\n\\t\\tfile:write(\\\"    result = \\\" .. module_name .. \\\".\\\" .. func_name .. \\\"()\\\\n\\\")\\n\\t\\tfile:write(\\\"    if result is not None:\\\\n\\\")\\n\\t\\tfile:write('        print(f\\\"Return value: {result}\\\")\\\\n')\\n\\t\\tfile:close()\\n\\n\\t\\t-- Run the temp file\\n\\t\\tvim.notify(\\\"Running function: \\\" .. func_name, vim.log.levels.INFO)\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(temp_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Function Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function executed successfully\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Function execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR)\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\tend\\n\\n\\t-- If there's only one function, run it directly\\n\\tif #functions == 1 then\\n\\t\\trun_function(functions[1])\\n\\t\\treturn\\n\\tend\\n\\n\\t-- Otherwise, show a picker to select the function\\n\\tvim.ui.select(functions, {\\n\\t\\tprompt = \\\"Select function to run:\\\",\\n\\t\\tformat_item = function(item)\\n\\t\\t\\treturn \\\"def \\\" .. item .. \\\"()\\\"\\n\\t\\tend,\\n\\t}, function(choice)\\n\\t\\tif choice then\\n\\t\\t\\trun_function(choice)\\n\\t\\tend\\n\\tend)\\nend\\n\\n-- Run current file\\nfunction M.run_file()\\n\\tlocal current_file = vim.fn.expand(\\\"%:p\\\")\\n\\tif current_file and current_file ~= \\\"\\\" then\\n\\t\\tvim.notify(\\\"Running: \\\" .. vim.fn.expand(\\\"%:t\\\"), vim.log.levels.INFO)\\n\\t\\t-- Run python on the current file and capture output to notifications\\n\\t\\tvim.fn.jobstart(M.config.execution.run_command .. \\\" \\\" .. vim.fn.shellescape(current_file), {\\n\\t\\t\\ton_stdout = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Output\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_stderr = function(_, data)\\n\\t\\t\\t\\tif data and #data > 1 then\\n\\t\\t\\t\\t\\tlocal output = table.concat(data, \\\"\\\\n\\\")\\n\\t\\t\\t\\t\\tif output and output:match(\\\"%S\\\") then\\n\\t\\t\\t\\t\\t\\tvim.notify(output, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\t\\ttitle = \\\"Python Error\\\",\\n\\t\\t\\t\\t\\t\\t\\ttimeout = M.config.execution.notification_timeout,\\n\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\ton_exit = function(_, exit_code)\\n\\t\\t\\t\\tif exit_code == 0 then\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution completed successfully\\\", vim.log.levels.INFO, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tvim.notify(\\\"Program execution failed with exit code: \\\" .. exit_code, vim.log.levels.ERROR, {\\n\\t\\t\\t\\t\\t\\ttitle = \\\"Python Execution\\\",\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tstdout_buffered = true,\\n\\t\\t\\tstderr_buffered = true,\\n\\t\\t})\\n\\telse\\n\\t\\tvim.notify(\\\"No file is open\\\", vim.log.levels.WARN)\\n\\tend\\nend\\n\\n-- Set up command pickers for integration with UI plugins\\nfunction M.setup_pickers()\\n\\t-- Check if Snacks is available\\n\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t-- Register UV command source\\n\\t\\tSnacks.picker.sources.uv_commands = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t{ text = \\\"Run current file\\\", desc = \\\"Run current file with Python\\\", is_run_current = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run selection\\\", desc = \\\"Run selected Python code\\\", is_run_selection = true },\\n\\t\\t\\t\\t\\t{ text = \\\"Run function\\\", desc = \\\"Run specific Python function\\\", is_run_function = true },\\n\\t\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", desc = \\\"Install a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv sync\\\", desc = \\\"Sync packages from lockfile\\\" },\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\t\\tdesc = \\\"Sync all extras, groups and packages\\\",\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", desc = \\\"Remove a package\\\" },\\n\\t\\t\\t\\t\\t{ text = \\\"uv init\\\", desc = \\\"Initialize a new project\\\" },\\n\\t\\t\\t\\t}\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\treturn { { item.text .. \\\" - \\\" .. item.desc } }\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\t\\tif item.is_run_current then\\n\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_selection then\\n\\t\\t\\t\\t\\t\\t-- Check if there's a visual selection\\n\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\n\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\") -- Exit visual mode\\n\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t-- If not in visual mode, prompt the user to select text\\n\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t-- After notification, we'll set up a one-time autocmd to catch when visual mode is exited\\n\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\", -- When changing from any visual mode to normal mode\\n\\t\\t\\t\\t\\t\\t\\t\\tcallback = function(ev)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true -- Delete the autocmd after it's been triggered\\n\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\telseif item.is_run_function then\\n\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\tlocal cmd = item.text\\n\\t\\t\\t\\t\\t-- Check if command needs input\\n\\t\\t\\t\\t\\tif cmd:match(\\\"%[(.-)%]\\\") then\\n\\t\\t\\t\\t\\t\\tlocal param_name = cmd:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\tvim.ui.input({ prompt = \\\"Enter \\\" .. param_name .. \\\": \\\" }, function(input)\\n\\t\\t\\t\\t\\t\\t\\tif not input or input == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\treturn\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t-- Replace the placeholder with actual input\\n\\t\\t\\t\\t\\t\\t\\tlocal actual_cmd = cmd:gsub(\\\"%[\\\" .. param_name .. \\\"%]\\\", input)\\n\\t\\t\\t\\t\\t\\t\\tM.run_command(actual_cmd)\\n\\t\\t\\t\\t\\t\\tend)\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t-- Run the command directly\\n\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\n\\t\\t-- Register UV venv source\\n\\t\\tSnacks.picker.sources.uv_venv = {\\n\\t\\t\\tfinder = function()\\n\\t\\t\\t\\tlocal venvs = {}\\n\\t\\t\\t\\t-- Check for .venv directory (uv's default)\\n\\t\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\tif #venvs == 0 then\\n\\t\\t\\t\\t\\ttable.insert(venvs, {\\n\\t\\t\\t\\t\\t\\ttext = \\\"Create new virtual environment (uv venv)\\\",\\n\\t\\t\\t\\t\\t\\tis_create = true,\\n\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\tend\\n\\t\\t\\t\\treturn venvs\\n\\t\\t\\tend,\\n\\t\\t\\tformat = function(item)\\n\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\treturn { { \\\"+ \\\" .. item.text } }\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\tlocal icon = item.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\"\\n\\t\\t\\t\\t\\treturn { { icon .. item.text .. \\\" (Activate)\\\" } }\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t\\tconfirm = function(picker, item)\\n\\t\\t\\t\\tpicker:close()\\n\\t\\t\\t\\tif item then\\n\\t\\t\\t\\t\\tif item.is_create then\\n\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\tM.activate_venv(item.path)\\n\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\tend\\n\\t\\t\\tend,\\n\\t\\t}\\n\\tend\\n\\n\\t-- Check if Telescope is available\\n\\tlocal has_telescope, telescope = pcall(require, \\\"telescope\\\")\\n\\tif has_telescope then\\n\\t\\tlocal pickers = require(\\\"telescope.pickers\\\")\\n\\t\\tlocal finders = require(\\\"telescope.finders\\\")\\n\\t\\tlocal sorters = require(\\\"telescope.sorters\\\")\\n\\t\\tlocal actions = require(\\\"telescope.actions\\\")\\n\\t\\tlocal action_state = require(\\\"telescope.actions.state\\\")\\n\\n\\t\\t-- UV Commands picker for Telescope\\n\\t\\tM.pick_uv_commands = function()\\n\\t\\t\\tlocal items = {\\n\\t\\t\\t\\t{ text = \\\"Run current file\\\", is_run_current = true },\\n\\t\\t\\t\\t{ text = \\\"Run selection\\\", is_run_selection = true },\\n\\t\\t\\t\\t{ text = \\\"Run function\\\", is_run_function = true },\\n\\t\\t\\t\\t{ text = \\\"uv add [package]\\\", cmd = \\\"uv add \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv sync\\\", cmd = \\\"uv sync\\\" },\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\ttext = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t\\tcmd = \\\"uv sync --all-extras --all-packages --all-groups\\\",\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{ text = \\\"uv remove [package]\\\", cmd = \\\"uv remove \\\", needs_input = true },\\n\\t\\t\\t\\t{ text = \\\"uv init\\\", cmd = \\\"uv init\\\" },\\n\\t\\t\\t}\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Commands\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = entry.text,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_run_current then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_file()\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_selection then\\n\\t\\t\\t\\t\\t\\t\\t\\tlocal mode = vim.fn.mode()\\n\\t\\t\\t\\t\\t\\t\\t\\tif mode == \\\"v\\\" or mode == \\\"V\\\" or mode == \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.cmd(\\\"normal! \\\\27\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.defer_fn(function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tend, 100)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\\"Please select text first. Enter visual mode (v) and select code to run.\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.log.levels.INFO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.api.nvim_create_autocmd(\\\"ModeChanged\\\", {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpattern = \\\"[vV\\\\x16]*:n\\\",\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_selection()\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tonce = true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t})\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\telseif selection.is_run_function then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_python_function()\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tif selection.needs_input then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal placeholder = selection.text:match(\\\"%[(.-)%]\\\")\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.ui.input(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{ prompt = \\\"Enter \\\" .. (placeholder or \\\"value\\\") .. \\\": \\\" },\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfunction(input)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tif input and input ~= \\\"\\\" then\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tlocal cmd = selection.cmd .. input\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tvim.notify(\\\"Cancelled\\\", vim.log.levels.INFO)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(selection.cmd)\\n\\t\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\n\\t\\t-- UV venv picker for Telescope\\n\\t\\tM.pick_uv_venv = function()\\n\\t\\t\\tlocal items = {}\\n\\t\\t\\tif vim.fn.isdirectory(\\\".venv\\\") == 1 then\\n\\t\\t\\t\\ttable.insert(items, {\\n\\t\\t\\t\\t\\ttext = \\\".venv\\\",\\n\\t\\t\\t\\t\\tpath = vim.fn.getcwd() .. \\\"/.venv\\\",\\n\\t\\t\\t\\t\\tis_current = vim.env.VIRTUAL_ENV and vim.env.VIRTUAL_ENV:match(\\\".venv$\\\") ~= nil,\\n\\t\\t\\t\\t})\\n\\t\\t\\tend\\n\\t\\t\\tif #items == 0 then\\n\\t\\t\\t\\ttable.insert(items, { text = \\\"Create new virtual environment (uv venv)\\\", is_create = true })\\n\\t\\t\\tend\\n\\n\\t\\t\\tpickers\\n\\t\\t\\t\\t.new({}, {\\n\\t\\t\\t\\t\\tprompt_title = \\\"UV Virtual Environments\\\",\\n\\t\\t\\t\\t\\tfinder = finders.new_table({\\n\\t\\t\\t\\t\\t\\tresults = items,\\n\\t\\t\\t\\t\\t\\tentry_maker = function(entry)\\n\\t\\t\\t\\t\\t\\t\\tlocal display = entry.is_create and \\\"+ \\\" .. entry.text\\n\\t\\t\\t\\t\\t\\t\\t\\tor ((entry.is_current and \\\"\\u25cf \\\" or \\\"\\u25cb \\\") .. entry.text .. \\\" (Activate)\\\")\\n\\t\\t\\t\\t\\t\\t\\treturn {\\n\\t\\t\\t\\t\\t\\t\\t\\tvalue = entry,\\n\\t\\t\\t\\t\\t\\t\\t\\tdisplay = display,\\n\\t\\t\\t\\t\\t\\t\\t\\tordinal = display,\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t\\t}),\\n\\t\\t\\t\\t\\tsorter = sorters.get_generic_fuzzy_sorter(),\\n\\t\\t\\t\\t\\tattach_mappings = function(prompt_bufnr, map)\\n\\t\\t\\t\\t\\t\\tlocal function on_select()\\n\\t\\t\\t\\t\\t\\t\\tlocal selection = action_state.get_selected_entry().value\\n\\t\\t\\t\\t\\t\\t\\tactions.close(prompt_bufnr)\\n\\t\\t\\t\\t\\t\\t\\tif selection.is_create then\\n\\t\\t\\t\\t\\t\\t\\t\\tM.run_command(\\\"uv venv\\\")\\n\\t\\t\\t\\t\\t\\t\\telse\\n\\t\\t\\t\\t\\t\\t\\t\\tM.activate_venv(selection.path)\\n\\t\\t\\t\\t\\t\\t\\tend\\n\\t\\t\\t\\t\\t\\tend\\n\\n\\t\\t\\t\\t\\t\\tmap(\\\"i\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\tmap(\\\"n\\\", \\\"<CR>\\\", on_select)\\n\\t\\t\\t\\t\\t\\treturn true\\n\\t\\t\\t\\t\\tend,\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t:find()\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Set up user commands\\nfunction M.setup_commands()\\n\\t-- Set up UV commands\\n\\tvim.api.nvim_create_user_command(\\\"UVInit\\\", function()\\n\\t\\tM.run_command(\\\"uv init\\\")\\n\\tend, {})\\n\\n\\t-- Command to run selected code\\n\\tvim.api.nvim_create_user_command(\\\"UVRunSelection\\\", function()\\n\\t\\tM.run_python_selection()\\n\\tend, { range = true })\\n\\n\\t-- Command to run a specific function\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFunction\\\", function()\\n\\t\\tM.run_python_function()\\n\\tend, {})\\n\\n\\t-- Command to run the current file\\n\\tvim.api.nvim_create_user_command(\\\"UVRunFile\\\", function()\\n\\t\\tM.run_file()\\n\\tend, {})\\n\\n\\t-- Command to add a package\\n\\tvim.api.nvim_create_user_command(\\\"UVAddPackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv add \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\n\\n\\t-- Command to remove a package\\n\\tvim.api.nvim_create_user_command(\\\"UVRemovePackage\\\", function(opts)\\n\\t\\tM.run_command(\\\"uv remove \\\" .. opts.args)\\n\\tend, { nargs = 1 })\\nend\\n\\n-- Set up keymaps\\nfunction M.setup_keymaps()\\n\\tlocal keymaps = M.config.keymaps\\n\\tlocal prefix = keymaps.prefix or \\\"<leader>x\\\"\\n\\n\\t-- Main UV command menu\\n\\tif keymaps.commands then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua Snacks.picker.pick('uv_commands')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"v\\\",\\n\\t\\t\\t\\tprefix,\\n\\t\\t\\t\\t\\\":<C-u>lua require('uv').pick_uv_commands()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Commands (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Run current file\\n\\tif keymaps.run_file then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"r\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFile<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Current File\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run selection\\n\\tif keymaps.run_selection then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"v\\\",\\n\\t\\t\\tprefix .. \\\"s\\\",\\n\\t\\t\\t\\\":<C-u>UVRunSelection<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Selection\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Run function\\n\\tif keymaps.run_function then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"f\\\",\\n\\t\\t\\t\\\"<cmd>UVRunFunction<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Run Function\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Environment management\\n\\tif keymaps.venv then\\n\\t\\tif _G.Snacks and _G.Snacks.picker then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua Snacks.picker.pick('uv_venv')<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\t\\tlocal has_telescope_venv = pcall(require, \\\"telescope\\\")\\n\\t\\tif has_telescope_venv then\\n\\t\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\t\\\"n\\\",\\n\\t\\t\\t\\tprefix .. \\\"e\\\",\\n\\t\\t\\t\\t\\\"<cmd>lua require('uv').pick_uv_venv()<CR>\\\",\\n\\t\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Environment (Telescope)\\\" }\\n\\t\\t\\t)\\n\\t\\tend\\n\\tend\\n\\n\\t-- Initialize UV project\\n\\tif keymaps.init then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"i\\\",\\n\\t\\t\\t\\\"<cmd>UVInit<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Init\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Add a package\\n\\tif keymaps.add then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"a\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv add ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Add Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Remove a package\\n\\tif keymaps.remove then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"d\\\",\\n\\t\\t\\t\\\"<cmd>lua vim.ui.input({prompt = 'Enter package name: '}, function(input) if input and input ~= '' then require('uv').run_command('uv remove ' .. input) end end)<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Remove Package\\\" }\\n\\t\\t)\\n\\tend\\n\\n\\t-- Sync packages\\n\\tif keymaps.sync then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"c\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync Packages\\\" }\\n\\t\\t)\\n\\tend\\n\\tif keymaps.sync_all then\\n\\t\\tvim.api.nvim_set_keymap(\\n\\t\\t\\t\\\"n\\\",\\n\\t\\t\\tprefix .. \\\"C\\\",\\n\\t\\t\\t\\\"<cmd>lua require('uv').run_command('uv sync --all-extras --all-packages --all-groups')<CR>\\\",\\n\\t\\t\\t{ noremap = true, silent = true, desc = \\\"UV Sync All Extras, Groups and Packages\\\" }\\n\\t\\t)\\n\\tend\\nend\\n\\n-- Set up auto commands\\nfunction M.setup_autocommands()\\n\\tif M.config.auto_commands then\\n\\t\\t-- Auto-activate .venv if it exists\\n\\t\\tif M.config.auto_activate_venv then\\n\\t\\t\\tM.auto_activate_venv()\\n\\n\\t\\t\\t-- Also set up auto-command to check when entering a directory\\n\\t\\t\\tvim.api.nvim_create_autocmd({ \\\"DirChanged\\\" }, {\\n\\t\\t\\t\\tpattern = { \\\"global\\\" },\\n\\t\\t\\t\\tcallback = function()\\n\\t\\t\\t\\t\\tM.auto_activate_venv()\\n\\t\\t\\t\\tend,\\n\\t\\t\\t})\\n\\t\\tend\\n\\tend\\nend\\n\\n-- Main setup function\\nfunction M.setup(opts)\\n\\t-- Merge user configuration with defaults\\n\\tM.config = vim.tbl_deep_extend(\\\"force\\\", M.config, opts or {})\\n\\n\\t-- Set up commands\\n\\tM.setup_commands()\\n\\n\\t-- Set up keymaps if enabled\\n\\tif M.config.keymaps ~= false then\\n\\t\\tM.setup_keymaps()\\n\\tend\\n\\n\\t-- Set up autocommands if enabled\\n\\tif M.config.auto_commands ~= false then\\n\\t\\tM.setup_autocommands()\\n\\tend\\n\\n\\t-- Set up pickers if integration is enabled\\n\\tif M.config.picker_integration then\\n\\t\\tM.setup_pickers()\\n\\tend\\n\\n\\t-- Make run_command globally accessible (can be removed if not needed)\\n\\t_G.run_command = M.run_command\\nend\\n\\nreturn M\\n\\n```\\n\\nFile: init.lua\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"c511d8c928e741c8b6e3305bc85905a5"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"61044"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79938a808eec8f491953fb623024","spanId":"e5e7e1831fbcef48","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976303744248000","endTimeUnixNano":"1762976329275932000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/uv.nvim/lua/uv/init.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79938a808eec8f491953fb623024","spanId":"5162ff739cbd71ff","parentSpanId":"e5e7e1831fbcef48","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976303745300000","endTimeUnixNano":"1762976329276044000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79938a808eec8f491953fb623024","spanId":"38c774158a684121","parentSpanId":"5162ff739cbd71ff","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976303745833000","endTimeUnixNano":"1762976329276083000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"b8e1c89ecac846fa84becc16fb9648d1"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"73995"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a799439f77366806d8b30d62ca4b3","spanId":"d5c81defb0289392","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762976348663849000","endTimeUnixNano":"1762976348663849000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"77"}}],"status":{}},{"traceId":"019a79943b5b81050484eea41da7351e","spanId":"083dc80e61a954e3","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762976349019634000","endTimeUnixNano":"1762976349019634000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"115"}}],"status":{}},{"traceId":"019a79943b6dd5022164be1de5720765","spanId":"d33547be09433821","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976349037454000","endTimeUnixNano":"1762976349037454000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/.config/nvim/lua/plugins/custom-lsps.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/custom-lsps.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79943f5cda87ac5e24c1b7657c9c","spanId":"686169f537f7f240","parentSpanId":"c69727fa151f4f47","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976350045993000","endTimeUnixNano":"1762976350045993000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/custom-lsps.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/custom-lsps.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"b8e1c89ecac846fa84becc16fb9648d1"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"73995"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79943f5cda87ac5e24c1b7657c9c","spanId":"d4974f7d8dce4227","parentSpanId":"5531592d84b4cb8d","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976350050565000","endTimeUnixNano":"1762976358156704000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"792"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"174"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"1292"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"792"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"[Scrubbed due to 'API_KEY']\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_7e387a4432ab47b2ab4e8279398b570f\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"issue_snippet\": \"/Users/benomahony/Code/open_source/ai-lsp\", \"suggested_fixes\": [{\"target_snippet\": \"/Users/benomahony/Code/open_source/ai-lsp\", \"title\": \"Make the server path configurable or dynamic\", \"replacement_snippet\": \"vim.env.AI_LSP_SERVER_DIR or vim.fn.stdpath(\\\"data\\\") .. \\\"/ai-lsp-server\\\"\"}], \"message\": \"The path to the 'ai-lsp' server directory is hardcoded. This makes the configuration non-portable and will prevent the LSP from working for other users or if the project directory is moved. Configurations should be environment-agnostic.\", \"severity\": \"error\"}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.scrubbed","value":{"stringValue":"[{\"path\": [\"attributes\", \"events\", 1, \"content\"], \"matched_substring\": \"API_KEY\"}]"}}],"status":{}},{"traceId":"019a79943f5cda87ac5e24c1b7657c9c","spanId":"5531592d84b4cb8d","parentSpanId":"c69727fa151f4f47","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976350048536000","endTimeUnixNano":"1762976358160372000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"/Users/benomahony/Code/open_source/ai-lsp\", \"severity\": \"error\", \"message\": \"The path to the 'ai-lsp' server directory is hardcoded. This makes the configuration non-portable and will prevent the LSP from working for other users or if the project directory is moved. Configurations should be environment-agnostic.\", \"suggested_fixes\": [{\"title\": \"Make the server path configurable or dynamic\", \"target_snippet\": \"/Users/benomahony/Code/open_source/ai-lsp\", \"replacement_snippet\": \"vim.env.AI_LSP_SERVER_DIR or vim.fn.stdpath(\\\"data\\\") .. \\\"/ai-lsp-server\\\"\"}]}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"792"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"174"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"1292"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"792"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"[Scrubbed due to 'API_KEY']\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_7e387a4432ab47b2ab4e8279398b570f\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"issue_snippet\": \"/Users/benomahony/Code/open_source/ai-lsp\", \"suggested_fixes\": [{\"target_snippet\": \"/Users/benomahony/Code/open_source/ai-lsp\", \"title\": \"Make the server path configurable or dynamic\", \"replacement_snippet\": \"vim.env.AI_LSP_SERVER_DIR or vim.fn.stdpath(\\\"data\\\") .. \\\"/ai-lsp-server\\\"\"}], \"message\": \"The path to the 'ai-lsp' server directory is hardcoded. This makes the configuration non-portable and will prevent the LSP from working for other users or if the project directory is moved. Configurations should be environment-agnostic.\", \"severity\": \"error\"}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_7e387a4432ab47b2ab4e8279398b570f\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}},{"key":"logfire.scrubbed","value":{"stringValue":"[{\"path\": [\"attributes\", \"all_messages_events\", 1, \"content\"], \"matched_substring\": \"API_KEY\"}]"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79943f5cda87ac5e24c1b7657c9c","spanId":"560b9aee259ffa54","parentSpanId":"c69727fa151f4f47","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762976358163502000","endTimeUnixNano":"1762976358163502000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 1 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"1"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79943f5cda87ac5e24c1b7657c9c","spanId":"fd8ff70196de91f7","parentSpanId":"c69727fa151f4f47","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762976358166132000","endTimeUnixNano":"1762976358166132000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of '/Users/benomahony/Co...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"/Users/benomahony/Co"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79943f5cda87ac5e24c1b7657c9c","spanId":"c69727fa151f4f47","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976350044363000","endTimeUnixNano":"1762976358166385000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/custom-lsps.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79945f19e2db25ef047d3ae71b84","spanId":"4b436dcd3325c828","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762976358169915000","endTimeUnixNano":"1762976358169915000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 1 diagnostics for file:///Users/benomahony/.config/nvim/lua/plugins/custom-lsps.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"1"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/custom-lsps.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"b8e1c89ecac846fa84becc16fb9648d1"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"73995"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a799623d87c40089c8dd330b9eb7b","spanId":"be10f5ef8e882e8b","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976474072770000","endTimeUnixNano":"1762976474072770000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/.config/nvim/lua/plugins/inlay-hints.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/inlay-hints.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a799627c288df20b0ec2392560ccc","spanId":"f761f1cd1a57df07","parentSpanId":"36ee958d165d8097","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976475074826000","endTimeUnixNano":"1762976475074826000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/.config/nvim/lua/plugins/inlay-hints.lua"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/inlay-hints.lua"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"b8e1c89ecac846fa84becc16fb9648d1"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"73995"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"fee7b6632b1e5cd772ec1b55b25da31016c75755"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a799627c288df20b0ec2392560ccc","spanId":"36ee958d165d8097","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976475074459000","endTimeUnixNano":"1762976479576553000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/.config/nvim/lua/plugins/inlay-hints.lua"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a799627c288df20b0ec2392560ccc","spanId":"b6bfbec554a278a0","parentSpanId":"36ee958d165d8097","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976475075650000","endTimeUnixNano":"1762976479576646000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a799627c288df20b0ec2392560ccc","spanId":"d6e10eaa5cee33b2","parentSpanId":"b6bfbec554a278a0","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976475076331000","endTimeUnixNano":"1762976479576688000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a799db0e19375688efb850945325a","spanId":"06b16a4461de3368","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762976968929284000","endTimeUnixNano":"1762976968929284000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"77"}}],"status":{}},{"traceId":"019a799db2907a3a82104b426cad59b5","spanId":"80dea34f06987791","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762976969360647000","endTimeUnixNano":"1762976969360647000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"115"}}],"status":{}},{"traceId":"019a799db2ac5f87502199638bab4623","spanId":"8634ebfb31d5e492","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762976969388877000","endTimeUnixNano":"1762976969388877000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a799db698bb81bf4f5f3250692e8f","spanId":"5759f6e152817c50","parentSpanId":"f5e5b26d2d44f5a6","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762976970393887000","endTimeUnixNano":"1762976970393887000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a799db698bb81bf4f5f3250692e8f","spanId":"359e50a556baa948","parentSpanId":"892d6210d44c6c19","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762976970396698000","endTimeUnixNano":"1762977008045195000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4895"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"1220"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"5797"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4895"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_c62cb52921424dcfb470da94ba754725\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"error\", \"suggested_fixes\": [{\"title\": \"Implement robust snippet matching\", \"replacement_snippet\": \"def find_snippet_in_text_robust(\", \"target_snippet\": \"def find_snippet_in_text(\"}, {\"target_snippet\": \"snippet = snippet.strip()\", \"replacement_snippet\": \"# Consider fuzzy matching, diff-based matching, or AST-based matching\\nsnippet = snippet.strip()\", \"title\": \"Consider fuzzy matching or AST-based location\"}], \"issue_snippet\": \"find_snippet_in_text\", \"message\": \"The `find_snippet_in_text` method relies on exact string matching for `issue_snippet` and `target_snippet` from the AI. This approach is brittle and highly susceptible to failure due to minor differences (e.g., whitespace, line endings, or user modifications to the code). If the snippet from the AI does not exactly match the document text, diagnostics will not be placed, or quick-fixes will not be offered, despite the AI identifying a valid issue or fix. This severely impacts the reliability and user experience of the AI-LSP integration.\"}, {\"issue_snippet\": \"regenerated_issue.issue_snippet\", \"severity\": \"error\", \"suggested_fixes\": [{\"title\": \"Anchor regenerated diagnostics to original range\", \"target_snippet\": \"positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\", \"replacement_snippet\": \"# Preserve original diagnostic range or adjust relative to it.\\n# If regenerated_issue.issue_snippet is provided, search within/near the original range.\\npositions = server.find_snippet_in_text(doc.source, regenerated_issue.issue_snippet)\"}, {\"target_snippet\": \"prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\", \"replacement_snippet\": \"prompt = f\\\"\\\"\\\"Analyze this specific code issue (originally at {line}:{character}) and provide NEW suggested fixes, ideally targeting the same area or providing relative offsets.\", \"title\": \"Prompt AI for relative range or diffs\"}], \"message\": \"In the `regenerate_fix` command, the `range` of the updated diagnostic is determined by re-searching for the `regenerated_issue.issue_snippet`. If the regenerated snippet from the AI differs from the original or if the code has changed, the diagnostic's position might 'jump' to a new, potentially incorrect location or disappear entirely. This breaks the user's mental model of updating a specific issue and leads to a confusing and unstable user experience, as the diagnostic's location loses its anchor to the original problem.\"}, {\"issue_snippet\": \"server.lsp.diagnostics\", \"severity\": \"warning\", \"suggested_fixes\": [{\"title\": \"Use public pygls API for diagnostics management\", \"target_snippet\": \"current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\", \"replacement_snippet\": \"# Replace with official pygls API for managing diagnostics if available.\\n# Example (concept only): current_diags = server.workspace.get_document_diagnostics(uri)\"}, {\"title\": \"Document internal dependency\", \"target_snippet\": \"current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\", \"replacement_snippet\": \"current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, []) # WARNING: Internal pygls API, subject to change. Monitor pygls updates.\"}], \"message\": \"The `regenerate_fix` and `dismiss_suggestion` commands directly access `server.lsp.diagnostics`. This relies on an internal, undocumented attribute of `pygls` and is not part of its public API. Such direct access creates a brittle dependency that could break with future `pygls` updates, affecting the maintainability and robustness of the LSP server.\"}, {\"message\": \"The `logfire.configure` call explicitly sets `send_to_logfire=False`. However, `os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"]` is set to a specific OTLP endpoint, and `logfire.instrument_pydantic_ai()` and `logfire.instrument_httpx()` are called. Setting `send_to_logfire=False` disables Logfire's built-in OTLP exporter, preventing any telemetry generated by the instrumentation from being sent to the configured endpoint. This creates a contradictory configuration that likely results in a lack of observability despite the instrumentation being active.\", \"suggested_fixes\": [{\"target_snippet\": \"send_to_logfire=False\", \"replacement_snippet\": \"send_to_logfire=True\", \"title\": \"Enable telemetry sending for Logfire\"}, {\"replacement_snippet\": \"if settings.configure_logfire:\\n    # Telemetry sending is explicitly disabled, so OTLP endpoint setting is moot for Logfire.\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\", \"target_snippet\": \"os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\", \"title\": \"Remove unnecessary OTLP endpoint environment variable if telemetry is not desired\"}], \"severity\": \"warning\", \"issue_snippet\": \"send_to_logfire=False\"}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a799db698bb81bf4f5f3250692e8f","spanId":"892d6210d44c6c19","parentSpanId":"f5e5b26d2d44f5a6","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762976970395472000","endTimeUnixNano":"1762977008056328000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"find_snippet_in_text\", \"severity\": \"error\", \"message\": \"The `find_snippet_in_text` method relies on exact string matching for `issue_snippet` and `target_snippet` from the AI. This approach is brittle and highly susceptible to failure due to minor differences (e.g., whitespace, line endings, or user modifications to the code). If the snippet from the AI does not exactly match the document text, diagnostics will not be placed, or quick-fixes will not be offered, despite the AI identifying a valid issue or fix. This severely impacts the reliability and user experience of the AI-LSP integration.\", \"suggested_fixes\": [{\"title\": \"Implement robust snippet matching\", \"target_snippet\": \"def find_snippet_in_text(\", \"replacement_snippet\": \"def find_snippet_in_text_robust(\"}, {\"title\": \"Consider fuzzy matching or AST-based location\", \"target_snippet\": \"snippet = snippet.strip()\", \"replacement_snippet\": \"# Consider fuzzy matching, diff-based matching, or AST-based matching\\nsnippet = snippet.strip()\"}]}, {\"issue_snippet\": \"regenerated_issue.issue_snippet\", \"severity\": \"error\", \"message\": \"In the `regenerate_fix` command, the `range` of the updated diagnostic is determined by re-searching for the `regenerated_issue.issue_snippet`. If the regenerated snippet from the AI differs from the original or if the code has changed, the diagnostic's position might 'jump' to a new, potentially incorrect location or disappear entirely. This breaks the user's mental model of updating a specific issue and leads to a confusing and unstable user experience, as the diagnostic's location loses its anchor to the original problem.\", \"suggested_fixes\": [{\"title\": \"Anchor regenerated diagnostics to original range\", \"target_snippet\": \"positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\", \"replacement_snippet\": \"# Preserve original diagnostic range or adjust relative to it.\\n# If regenerated_issue.issue_snippet is provided, search within/near the original range.\\npositions = server.find_snippet_in_text(doc.source, regenerated_issue.issue_snippet)\"}, {\"title\": \"Prompt AI for relative range or diffs\", \"target_snippet\": \"prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\", \"replacement_snippet\": \"prompt = f\\\"\\\"\\\"Analyze this specific code issue (originally at {line}:{character}) and provide NEW suggested fixes, ideally targeting the same area or providing relative offsets.\"}]}, {\"issue_snippet\": \"server.lsp.diagnostics\", \"severity\": \"warning\", \"message\": \"The `regenerate_fix` and `dismiss_suggestion` commands directly access `server.lsp.diagnostics`. This relies on an internal, undocumented attribute of `pygls` and is not part of its public API. Such direct access creates a brittle dependency that could break with future `pygls` updates, affecting the maintainability and robustness of the LSP server.\", \"suggested_fixes\": [{\"title\": \"Use public pygls API for diagnostics management\", \"target_snippet\": \"current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\", \"replacement_snippet\": \"# Replace with official pygls API for managing diagnostics if available.\\n# Example (concept only): current_diags = server.workspace.get_document_diagnostics(uri)\"}, {\"title\": \"Document internal dependency\", \"target_snippet\": \"current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\", \"replacement_snippet\": \"current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, []) # WARNING: Internal pygls API, subject to change. Monitor pygls updates.\"}]}, {\"issue_snippet\": \"send_to_logfire=False\", \"severity\": \"warning\", \"message\": \"The `logfire.configure` call explicitly sets `send_to_logfire=False`. However, `os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"]` is set to a specific OTLP endpoint, and `logfire.instrument_pydantic_ai()` and `logfire.instrument_httpx()` are called. Setting `send_to_logfire=False` disables Logfire's built-in OTLP exporter, preventing any telemetry generated by the instrumentation from being sent to the configured endpoint. This creates a contradictory configuration that likely results in a lack of observability despite the instrumentation being active.\", \"suggested_fixes\": [{\"title\": \"Enable telemetry sending for Logfire\", \"target_snippet\": \"send_to_logfire=False\", \"replacement_snippet\": \"send_to_logfire=True\"}, {\"title\": \"Remove unnecessary OTLP endpoint environment variable if telemetry is not desired\", \"target_snippet\": \"os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\", \"replacement_snippet\": \"if settings.configure_logfire:\\n    # Telemetry sending is explicitly disabled, so OTLP endpoint setting is moot for Logfire.\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\"}]}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4895"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"1220"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"5797"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4895"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_c62cb52921424dcfb470da94ba754725\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"error\", \"suggested_fixes\": [{\"title\": \"Implement robust snippet matching\", \"replacement_snippet\": \"def find_snippet_in_text_robust(\", \"target_snippet\": \"def find_snippet_in_text(\"}, {\"target_snippet\": \"snippet = snippet.strip()\", \"replacement_snippet\": \"# Consider fuzzy matching, diff-based matching, or AST-based matching\\nsnippet = snippet.strip()\", \"title\": \"Consider fuzzy matching or AST-based location\"}], \"issue_snippet\": \"find_snippet_in_text\", \"message\": \"The `find_snippet_in_text` method relies on exact string matching for `issue_snippet` and `target_snippet` from the AI. This approach is brittle and highly susceptible to failure due to minor differences (e.g., whitespace, line endings, or user modifications to the code). If the snippet from the AI does not exactly match the document text, diagnostics will not be placed, or quick-fixes will not be offered, despite the AI identifying a valid issue or fix. This severely impacts the reliability and user experience of the AI-LSP integration.\"}, {\"issue_snippet\": \"regenerated_issue.issue_snippet\", \"severity\": \"error\", \"suggested_fixes\": [{\"title\": \"Anchor regenerated diagnostics to original range\", \"target_snippet\": \"positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\", \"replacement_snippet\": \"# Preserve original diagnostic range or adjust relative to it.\\n# If regenerated_issue.issue_snippet is provided, search within/near the original range.\\npositions = server.find_snippet_in_text(doc.source, regenerated_issue.issue_snippet)\"}, {\"target_snippet\": \"prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\", \"replacement_snippet\": \"prompt = f\\\"\\\"\\\"Analyze this specific code issue (originally at {line}:{character}) and provide NEW suggested fixes, ideally targeting the same area or providing relative offsets.\", \"title\": \"Prompt AI for relative range or diffs\"}], \"message\": \"In the `regenerate_fix` command, the `range` of the updated diagnostic is determined by re-searching for the `regenerated_issue.issue_snippet`. If the regenerated snippet from the AI differs from the original or if the code has changed, the diagnostic's position might 'jump' to a new, potentially incorrect location or disappear entirely. This breaks the user's mental model of updating a specific issue and leads to a confusing and unstable user experience, as the diagnostic's location loses its anchor to the original problem.\"}, {\"issue_snippet\": \"server.lsp.diagnostics\", \"severity\": \"warning\", \"suggested_fixes\": [{\"title\": \"Use public pygls API for diagnostics management\", \"target_snippet\": \"current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\", \"replacement_snippet\": \"# Replace with official pygls API for managing diagnostics if available.\\n# Example (concept only): current_diags = server.workspace.get_document_diagnostics(uri)\"}, {\"title\": \"Document internal dependency\", \"target_snippet\": \"current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\", \"replacement_snippet\": \"current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, []) # WARNING: Internal pygls API, subject to change. Monitor pygls updates.\"}], \"message\": \"The `regenerate_fix` and `dismiss_suggestion` commands directly access `server.lsp.diagnostics`. This relies on an internal, undocumented attribute of `pygls` and is not part of its public API. Such direct access creates a brittle dependency that could break with future `pygls` updates, affecting the maintainability and robustness of the LSP server.\"}, {\"message\": \"The `logfire.configure` call explicitly sets `send_to_logfire=False`. However, `os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"]` is set to a specific OTLP endpoint, and `logfire.instrument_pydantic_ai()` and `logfire.instrument_httpx()` are called. Setting `send_to_logfire=False` disables Logfire's built-in OTLP exporter, preventing any telemetry generated by the instrumentation from being sent to the configured endpoint. This creates a contradictory configuration that likely results in a lack of observability despite the instrumentation being active.\", \"suggested_fixes\": [{\"target_snippet\": \"send_to_logfire=False\", \"replacement_snippet\": \"send_to_logfire=True\", \"title\": \"Enable telemetry sending for Logfire\"}, {\"replacement_snippet\": \"if settings.configure_logfire:\\n    # Telemetry sending is explicitly disabled, so OTLP endpoint setting is moot for Logfire.\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\", \"target_snippet\": \"os.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")\", \"title\": \"Remove unnecessary OTLP endpoint environment variable if telemetry is not desired\"}], \"severity\": \"warning\", \"issue_snippet\": \"send_to_logfire=False\"}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_c62cb52921424dcfb470da94ba754725\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a799db698bb81bf4f5f3250692e8f","spanId":"45fc936ce53601ea","parentSpanId":"f5e5b26d2d44f5a6","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762977008067557000","endTimeUnixNano":"1762977008067557000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 4 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"4"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a799db698bb81bf4f5f3250692e8f","spanId":"4d89ff0097b02702","parentSpanId":"f5e5b26d2d44f5a6","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977008070831000","endTimeUnixNano":"1762977008070831000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 4 occurrences of 'find_snippet_in_text...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"4"}},{"key":"snippet[:20]","value":{"stringValue":"find_snippet_in_text"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a799db698bb81bf4f5f3250692e8f","spanId":"9a84165e3b76728f","parentSpanId":"f5e5b26d2d44f5a6","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977008071370000","endTimeUnixNano":"1762977008071370000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 2 occurrences of 'regenerated_issue.is...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"2"}},{"key":"snippet[:20]","value":{"stringValue":"regenerated_issue.is"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a799db698bb81bf4f5f3250692e8f","spanId":"a03372b93915fa47","parentSpanId":"f5e5b26d2d44f5a6","flags":256,"name":"Could not find snippet '{snippet}' in text","kind":1,"startTimeUnixNano":"1762977008072123000","endTimeUnixNano":"1762977008072123000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"13"}},{"key":"logfire.msg_template","value":{"stringValue":"Could not find snippet '{snippet}' in text"}},{"key":"logfire.msg","value":{"stringValue":"Could not find snippet 'server.lsp.diagnostics' in text"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"149"}},{"key":"snippet","value":{"stringValue":"server.lsp.diagnostics"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"snippet\":{}}}"}}],"status":{}},{"traceId":"019a799db698bb81bf4f5f3250692e8f","spanId":"69144a2ed4475727","parentSpanId":"f5e5b26d2d44f5a6","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977008072377000","endTimeUnixNano":"1762977008072377000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'send_to_logfire=Fals...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"send_to_logfire=Fals"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a799db698bb81bf4f5f3250692e8f","spanId":"f5e5b26d2d44f5a6","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762976970392956000","endTimeUnixNano":"1762977008072538000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a799e49cdc3b04e85257d9f50815f","spanId":"27bfc7e1d39e2ba8","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762977008077779000","endTimeUnixNano":"1762977008077779000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 3 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"3"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a38c2755d91ca65c05b1c43a2d","spanId":"eec289a2187be242","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977352743833000","endTimeUnixNano":"1762977352743833000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a38f6ab102bc1ffd41a175774a","spanId":"2e0bcef98ad001e4","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977353578390000","endTimeUnixNano":"1762977353578390000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a38f6c94c5c2663c7df809997d","spanId":"99a917835a46f390","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977353580758000","endTimeUnixNano":"1762977353580758000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a39354803ac82bc6b2419b0380","spanId":"0c2b56a874faeae8","parentSpanId":"1ae6edab91b70953","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977354581630000","endTimeUnixNano":"1762977354581630000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a3999e46088d9f99430a85ff88","spanId":"15c6cab652a886bc","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977356190313000","endTimeUnixNano":"1762977356190313000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a39354803ac82bc6b2419b0380","spanId":"1ae6edab91b70953","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977354580421000","endTimeUnixNano":"1762977356205112000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762977356195825000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a399adc2e4fb281b72fc87b8a6","spanId":"171ca3536a5e9185","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977356205346000","endTimeUnixNano":"1762977356205346000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a39d88bfbdd1350042e292e32a","spanId":"ada37e5846592f06","parentSpanId":"f9ce5db8c812333b","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977357192638000","endTimeUnixNano":"1762977357192638000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a3a0969ad4ab4ffe7ece68f1ad","spanId":"eac2b196dd87a369","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977357974122000","endTimeUnixNano":"1762977357974122000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a39d88bfbdd1350042e292e32a","spanId":"f9ce5db8c812333b","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977357192171000","endTimeUnixNano":"1762977357984231000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762977357981977000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a3a0a090de03a69279fdbaa7df","spanId":"ab33ab5ef443ba17","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977357984540000","endTimeUnixNano":"1762977357984540000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a3a480c0b05f46eadc658ff979","spanId":"2ded7881dbaff0fa","parentSpanId":"656de578c07140b4","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977358977612000","endTimeUnixNano":"1762977358977612000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a39354803ac82bc6b2419b0380","spanId":"ce5c1028ae2bc2d5","parentSpanId":"23e4899be04318d2","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977354586157000","endTimeUnixNano":"1762977356191062000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a39354803ac82bc6b2419b0380","spanId":"23e4899be04318d2","parentSpanId":"1ae6edab91b70953","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977354584402000","endTimeUnixNano":"1762977356191346000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a39d88bfbdd1350042e292e32a","spanId":"3221219c52efdd29","parentSpanId":"14ebc1aa900c4058","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977357194105000","endTimeUnixNano":"1762977357975495000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a39d88bfbdd1350042e292e32a","spanId":"14ebc1aa900c4058","parentSpanId":"f9ce5db8c812333b","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977357193476000","endTimeUnixNano":"1762977357976006000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a3ad9222b55e9037578205fd3c","spanId":"c59881764a2c7f1d","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977361298426000","endTimeUnixNano":"1762977361298426000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a3a480c0b05f46eadc658ff979","spanId":"656de578c07140b4","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977358976681000","endTimeUnixNano":"1762977361306706000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762977361303989000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a3ad9b2f3cddc54f17234d1136","spanId":"fea4e0efef482b0d","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977361307026000","endTimeUnixNano":"1762977361307026000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a3b17b138fcd21d2aaf9b17f00","spanId":"67de7c85212f26a0","parentSpanId":"895b8e0410741226","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977362299898000","endTimeUnixNano":"1762977362299898000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a3b3ffa29cc8463525ed36e0b6","spanId":"2536a19c10b4edd4","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977362943019000","endTimeUnixNano":"1762977362943019000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a3b17b138fcd21d2aaf9b17f00","spanId":"895b8e0410741226","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977362299538000","endTimeUnixNano":"1762977362949261000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762977362947428000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a3b40574e8956707948acdf75a","spanId":"7ec1501fe1da24f3","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977362949429000","endTimeUnixNano":"1762977362949429000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a3b7e97aa339e10c2f866cfd25","spanId":"8c84efeaba46a0d4","parentSpanId":"18d653ed69bced74","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977363946277000","endTimeUnixNano":"1762977363946277000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a3a480c0b05f46eadc658ff979","spanId":"27fdf5d8d79cfc83","parentSpanId":"69951ae7f9bc95e3","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977358980852000","endTimeUnixNano":"1762977361299111000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a3a480c0b05f46eadc658ff979","spanId":"69951ae7f9bc95e3","parentSpanId":"656de578c07140b4","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977358979450000","endTimeUnixNano":"1762977361299370000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a3b17b138fcd21d2aaf9b17f00","spanId":"7353b7ea5d879eb0","parentSpanId":"3f1f19ba8e932f3a","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977362301140000","endTimeUnixNano":"1762977362943471000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a3b17b138fcd21d2aaf9b17f00","spanId":"3f1f19ba8e932f3a","parentSpanId":"895b8e0410741226","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977362300589000","endTimeUnixNano":"1762977362943662000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a41960af971159eb29ab459b1b","spanId":"8a947c8a895b5cf5","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977388896290000","endTimeUnixNano":"1762977388896290000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a41d4aabaccf74ecb0db96e3a6","spanId":"f4629c888a27c106","parentSpanId":"0e2c4b399e1509d8","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977389899484000","endTimeUnixNano":"1762977389899484000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a41d4aabaccf74ecb0db96e3a6","spanId":"cb28cf10652047d2","parentSpanId":"0e2c4b399e1509d8","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762977390815329000","endTimeUnixNano":"1762977390815329000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 0 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"0"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79a41d4aabaccf74ecb0db96e3a6","spanId":"0e2c4b399e1509d8","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977389898339000","endTimeUnixNano":"1762977390815711000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79a420e09e87c09af8d0b89c28c9","spanId":"a72db303683e9ca8","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762977390816931000","endTimeUnixNano":"1762977390816931000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 0 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"0"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a3b7e97aa339e10c2f866cfd25","spanId":"6d9fb4dc66e324fc","parentSpanId":"18d653ed69bced74","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762977393874686000","endTimeUnixNano":"1762977393874686000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 2 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"2"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79a3b7e97aa339e10c2f866cfd25","spanId":"2766a46477f8f8ce","parentSpanId":"18d653ed69bced74","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977393875116000","endTimeUnixNano":"1762977393875116000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 3 occurrences of 'positions[0]...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"3"}},{"key":"snippet[:20]","value":{"stringValue":"positions[0]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a3b7e97aa339e10c2f866cfd25","spanId":"24ad779ca4eac352","parentSpanId":"18d653ed69bced74","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977393875417000","endTimeUnixNano":"1762977393875417000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 2 occurrences of 'getattr(server.lsp, ...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"2"}},{"key":"snippet[:20]","value":{"stringValue":"getattr(server.lsp, "}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a3b7e97aa339e10c2f866cfd25","spanId":"18d653ed69bced74","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977363945093000","endTimeUnixNano":"1762977393875547000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79a42cd316535de49314a06608a8","spanId":"965403ee1d192a20","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762977393875824000","endTimeUnixNano":"1762977393875824000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 2 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"2"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a41d4aabaccf74ecb0db96e3a6","spanId":"f3aff8df0e549aee","parentSpanId":"4dd28a1582347b72","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977389903247000","endTimeUnixNano":"1762977390812037000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"525"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"14"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"63"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"525"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n\\n\\n```\\n\\nFile: models.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_a8cfd17e301641d2b789c8694fa163d0\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": []}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a41d4aabaccf74ecb0db96e3a6","spanId":"4dd28a1582347b72","parentSpanId":"0e2c4b399e1509d8","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977389901658000","endTimeUnixNano":"1762977390814121000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": []}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"525"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"14"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"63"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"525"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n\\n\\n```\\n\\nFile: models.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_a8cfd17e301641d2b789c8694fa163d0\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": []}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_a8cfd17e301641d2b789c8694fa163d0\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a3b7e97aa339e10c2f866cfd25","spanId":"fe79a289c57e35a7","parentSpanId":"d891617c9e53e2b7","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977363950062000","endTimeUnixNano":"1762977393858005000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4895"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"444"}},{"key":"gen_ai.usage.details.cached_content_tokens","value":{"intValue":"984"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"4934"}},{"key":"gen_ai.usage.details.text_cache_tokens","value":{"intValue":"984"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4895"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_2515ded3cb9b4c2181fc15f0e10c00aa\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"message\": \"The AI analysis relies on `issue_snippet` and `target_snippet` to locate problems and apply fixes. The current implementation, however, only uses the *first* occurrence of these snippets found in the document (`positions[0]`). If the same snippet appears multiple times, the diagnostic or code action might be applied to an incorrect location, leading to misinterpretations or unintended code changes. This severely compromises the precision and reliability of the AI-powered LSP features.\", \"severity\": \"error\", \"suggested_fixes\": [{\"replacement_snippet\": \"This issue requires a fundamental change to the `DiagnosticResult` and `CodeIssue` models, as well as the AI agent's prompt, to include line and character information for `issue_snippet` and `target_snippet`. This precise location data should then be used directly to create `Range` objects, rather than relying on string searching and `positions[0]`.\", \"target_snippet\": \"positions[0]\", \"title\": \"Enhance AI output with precise location data\"}], \"issue_snippet\": \"positions[0]\"}, {\"severity\": \"warning\", \"message\": \"Accessing `server.lsp.diagnostics` directly is an architectural anti-pattern. This relies on internal implementation details of `pygls` and can lead to fragile code that breaks if the library's internals change. While `server` itself is a `LanguageServer` instance, `_diagnostics` is a private attribute. A more robust solution would involve `AILanguageServer` managing its own diagnostic state or using a public API if `pygls` were to provide one for retrieving current diagnostics.\", \"suggested_fixes\": [{\"replacement_snippet\": \"server._diagnostics.get(uri, [])\", \"title\": \"Use `server._diagnostics` directly (with caution)\", \"target_snippet\": \"getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\"}], \"issue_snippet\": \"getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\"}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a3b7e97aa339e10c2f866cfd25","spanId":"d891617c9e53e2b7","parentSpanId":"18d653ed69bced74","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977363948293000","endTimeUnixNano":"1762977393867042000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"positions[0]\", \"severity\": \"error\", \"message\": \"The AI analysis relies on `issue_snippet` and `target_snippet` to locate problems and apply fixes. The current implementation, however, only uses the *first* occurrence of these snippets found in the document (`positions[0]`). If the same snippet appears multiple times, the diagnostic or code action might be applied to an incorrect location, leading to misinterpretations or unintended code changes. This severely compromises the precision and reliability of the AI-powered LSP features.\", \"suggested_fixes\": [{\"title\": \"Enhance AI output with precise location data\", \"target_snippet\": \"positions[0]\", \"replacement_snippet\": \"This issue requires a fundamental change to the `DiagnosticResult` and `CodeIssue` models, as well as the AI agent's prompt, to include line and character information for `issue_snippet` and `target_snippet`. This precise location data should then be used directly to create `Range` objects, rather than relying on string searching and `positions[0]`.\"}]}, {\"issue_snippet\": \"getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\", \"severity\": \"warning\", \"message\": \"Accessing `server.lsp.diagnostics` directly is an architectural anti-pattern. This relies on internal implementation details of `pygls` and can lead to fragile code that breaks if the library's internals change. While `server` itself is a `LanguageServer` instance, `_diagnostics` is a private attribute. A more robust solution would involve `AILanguageServer` managing its own diagnostic state or using a public API if `pygls` were to provide one for retrieving current diagnostics.\", \"suggested_fixes\": [{\"title\": \"Use `server._diagnostics` directly (with caution)\", \"target_snippet\": \"getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\", \"replacement_snippet\": \"server._diagnostics.get(uri, [])\"}]}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4895"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"444"}},{"key":"gen_ai.usage.details.cached_content_tokens","value":{"intValue":"984"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"4934"}},{"key":"gen_ai.usage.details.text_cache_tokens","value":{"intValue":"984"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4895"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_2515ded3cb9b4c2181fc15f0e10c00aa\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"message\": \"The AI analysis relies on `issue_snippet` and `target_snippet` to locate problems and apply fixes. The current implementation, however, only uses the *first* occurrence of these snippets found in the document (`positions[0]`). If the same snippet appears multiple times, the diagnostic or code action might be applied to an incorrect location, leading to misinterpretations or unintended code changes. This severely compromises the precision and reliability of the AI-powered LSP features.\", \"severity\": \"error\", \"suggested_fixes\": [{\"replacement_snippet\": \"This issue requires a fundamental change to the `DiagnosticResult` and `CodeIssue` models, as well as the AI agent's prompt, to include line and character information for `issue_snippet` and `target_snippet`. This precise location data should then be used directly to create `Range` objects, rather than relying on string searching and `positions[0]`.\", \"target_snippet\": \"positions[0]\", \"title\": \"Enhance AI output with precise location data\"}], \"issue_snippet\": \"positions[0]\"}, {\"severity\": \"warning\", \"message\": \"Accessing `server.lsp.diagnostics` directly is an architectural anti-pattern. This relies on internal implementation details of `pygls` and can lead to fragile code that breaks if the library's internals change. While `server` itself is a `LanguageServer` instance, `_diagnostics` is a private attribute. A more robust solution would involve `AILanguageServer` managing its own diagnostic state or using a public API if `pygls` were to provide one for retrieving current diagnostics.\", \"suggested_fixes\": [{\"replacement_snippet\": \"server._diagnostics.get(uri, [])\", \"title\": \"Use `server._diagnostics` directly (with caution)\", \"target_snippet\": \"getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\"}], \"issue_snippet\": \"getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\"}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_2515ded3cb9b4c2181fc15f0e10c00aa\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a511f9806f0c4aa746882108cc","spanId":"f8c406283dd30f51","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977452537240000","endTimeUnixNano":"1762977452537240000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a515e3b893fe6abea590c7e2b3","spanId":"a3c1ef6e902082f3","parentSpanId":"176d8d21e12daf4a","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977453540455000","endTimeUnixNano":"1762977453540455000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a5183c63d3e96b77925f092554","spanId":"7c15cdbf94682c9c","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977454140502000","endTimeUnixNano":"1762977454140502000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a515e3b893fe6abea590c7e2b3","spanId":"176d8d21e12daf4a","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977453539358000","endTimeUnixNano":"1762977454143624000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762977454141642000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a5183fb9b09501e595597e66bc","spanId":"2abd7e504335dbb2","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977454143874000","endTimeUnixNano":"1762977454143874000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a515e3b893fe6abea590c7e2b3","spanId":"3da588cbb23c9f09","parentSpanId":"59d4eda758bed9ed","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977453544077000","endTimeUnixNano":"1762977454141059000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a515e3b893fe6abea590c7e2b3","spanId":"59d4eda758bed9ed","parentSpanId":"176d8d21e12daf4a","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977453542334000","endTimeUnixNano":"1762977454141248000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n\\n\\n```\\n\\nFile: models.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a51c2675b089d16fa16062edbb","spanId":"151899dd8b372494","parentSpanId":"ae5d04c4f41d78bc","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977455143811000","endTimeUnixNano":"1762977455143811000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a51c8216fd0836e76d52a32262","spanId":"086d0e5152f22bd0","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977455234789000","endTimeUnixNano":"1762977455234789000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a51c2675b089d16fa16062edbb","spanId":"ae5d04c4f41d78bc","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977455142662000","endTimeUnixNano":"1762977455241249000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"0a5ac19f3ac09d50aed6db0a5940f8b1dc28d859fdfbc0ceb110af6b2780d66d"}}],"events":[{"timeUnixNano":"1762977455236745000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        req, traces=traces, timeout=real_timeout\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<7 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1283, in _wrap_create_connection\n    return await self._loop.create_connection(*args, **kwargs, sock=sock)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/base_events.py\", line 1182, in create_connection\n    transport, protocol = await self._create_connection_transport(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n        ssl_shutdown_timeout=ssl_shutdown_timeout)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/base_events.py\", line 1215, in _create_connection_transport\n    await waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a51c896e9a5722d8f0bf7c25ed","spanId":"7295f4bbb4028941","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977455241459000","endTimeUnixNano":"1762977455241459000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a51c963cf8447c07a49a312237","spanId":"26ac07b031a18b67","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977455254049000","endTimeUnixNano":"1762977455254049000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a51c96519f97a2d1d856a48f1b","spanId":"82b0351206200135","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977455254357000","endTimeUnixNano":"1762977455254357000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a51c9802f77207603d3b2fc70f","spanId":"5e55d1d3cd634933","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977455256299000","endTimeUnixNano":"1762977455256299000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a51c984f95e67faa36f4ff8afb","spanId":"79cf1d6968eb13aa","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977455256562000","endTimeUnixNano":"1762977455256562000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a520812705fcc88dea701cdb7e","spanId":"cdf1317db03f1a9a","parentSpanId":"87cf7fae7deaa15e","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977456257932000","endTimeUnixNano":"1762977456257932000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a51c2675b089d16fa16062edbb","spanId":"8f359f4a3dbe9242","parentSpanId":"76b6326677bda6a3","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977455148842000","endTimeUnixNano":"1762977455235864000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a51c2675b089d16fa16062edbb","spanId":"76b6326677bda6a3","parentSpanId":"ae5d04c4f41d78bc","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977455146377000","endTimeUnixNano":"1762977455236024000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n\\n```\\n\\nFile: models.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a5470bcdf0a18d8a986833472b","spanId":"0db89644e1119fc4","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977466123009000","endTimeUnixNano":"1762977466123009000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a54af3de11451c850e30795840","spanId":"11096e81dcb59d50","parentSpanId":"f8b2634ed95e89e7","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977467123725000","endTimeUnixNano":"1762977467123725000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a54f80d1d4f8e3eb3a2d65cc39","spanId":"b6e5f23a56993666","flags":256,"name":"code_actions","kind":1,"startTimeUnixNano":"1762977468287994000","endTimeUnixNano":"1762977468288241000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"315"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"code_actions"}},{"key":"logfire.msg","value":{"stringValue":"code_actions"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79a520812705fcc88dea701cdb7e","spanId":"5d1169aa1e57484c","parentSpanId":"87cf7fae7deaa15e","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762977468602617000","endTimeUnixNano":"1762977468602617000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 1 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"1"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79a520812705fcc88dea701cdb7e","spanId":"7c6f802510036cf0","parentSpanId":"87cf7fae7deaa15e","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977468603122000","endTimeUnixNano":"1762977468603122000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'http://localhost:431...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"http://localhost:431"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a520812705fcc88dea701cdb7e","spanId":"87cf7fae7deaa15e","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977456257581000","endTimeUnixNano":"1762977468603379000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79a550bbc112e81aace4b6ffc619","spanId":"ee812fadc2b6724d","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762977468603870000","endTimeUnixNano":"1762977468603870000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 1 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"1"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a520812705fcc88dea701cdb7e","spanId":"0164babf434aaf59","parentSpanId":"8605ad4a99b03dcb","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977456259400000","endTimeUnixNano":"1762977468597557000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"795"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"203"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"1911"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"795"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom typing import Literal\\n\\n# pyrefly: ignore [missing-import]\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n```\\n\\nFile: models.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_90d3e5965c71455d8dfa7c5de07bfa42\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"message\": \"The default OTLP exporter endpoint points to `localhost`. In a production or deployed environment, this will likely fail as `localhost` refers to the container or machine itself, not an external OpenTelemetry collector. This default can lead to silent failures in observability data collection if not explicitly overridden for deployment. Consider making this an environment-specific setting or defaulting to `None` if it's mandatory.\", \"suggested_fixes\": [{\"target_snippet\": \"http://localhost:4318\", \"replacement_snippet\": \"None\", \"title\": \"Make OTLP endpoint configurable and default to None\"}, {\"replacement_snippet\": \"\\\"\\\"\", \"title\": \"Use an empty string as a default placeholder to indicate it needs configuration\", \"target_snippet\": \"http://localhost:4318\"}], \"severity\": \"warning\", \"issue_snippet\": \"http://localhost:4318\"}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a520812705fcc88dea701cdb7e","spanId":"8605ad4a99b03dcb","parentSpanId":"87cf7fae7deaa15e","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977456258637000","endTimeUnixNano":"1762977468600466000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"http://localhost:4318\", \"severity\": \"warning\", \"message\": \"The default OTLP exporter endpoint points to `localhost`. In a production or deployed environment, this will likely fail as `localhost` refers to the container or machine itself, not an external OpenTelemetry collector. This default can lead to silent failures in observability data collection if not explicitly overridden for deployment. Consider making this an environment-specific setting or defaulting to `None` if it's mandatory.\", \"suggested_fixes\": [{\"title\": \"Make OTLP endpoint configurable and default to None\", \"target_snippet\": \"http://localhost:4318\", \"replacement_snippet\": \"None\"}, {\"title\": \"Use an empty string as a default placeholder to indicate it needs configuration\", \"target_snippet\": \"http://localhost:4318\", \"replacement_snippet\": \"\\\"\\\"\"}]}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"795"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"203"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"1911"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"795"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom typing import Literal\\n\\n# pyrefly: ignore [missing-import]\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n```\\n\\nFile: models.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_90d3e5965c71455d8dfa7c5de07bfa42\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"message\": \"The default OTLP exporter endpoint points to `localhost`. In a production or deployed environment, this will likely fail as `localhost` refers to the container or machine itself, not an external OpenTelemetry collector. This default can lead to silent failures in observability data collection if not explicitly overridden for deployment. Consider making this an environment-specific setting or defaulting to `None` if it's mandatory.\", \"suggested_fixes\": [{\"target_snippet\": \"http://localhost:4318\", \"replacement_snippet\": \"None\", \"title\": \"Make OTLP endpoint configurable and default to None\"}, {\"replacement_snippet\": \"\\\"\\\"\", \"title\": \"Use an empty string as a default placeholder to indicate it needs configuration\", \"target_snippet\": \"http://localhost:4318\"}], \"severity\": \"warning\", \"issue_snippet\": \"http://localhost:4318\"}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_90d3e5965c71455d8dfa7c5de07bfa42\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a55dfeb1309fe50f3e10e60f0b","spanId":"48769d0f4867c8d6","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977471998971000","endTimeUnixNano":"1762977471998971000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a54af3de11451c850e30795840","spanId":"f8b2634ed95e89e7","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977467123494000","endTimeUnixNano":"1762977472007349000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762977472004849000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a55e07be2ffd7436477871df02","spanId":"7a42f408012bfdef","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977472007686000","endTimeUnixNano":"1762977472007686000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a561e87c0cc70fb08e250b8bdb","spanId":"c7bde25e130f304b","parentSpanId":"f5880d34340bfbea","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977473000968000","endTimeUnixNano":"1762977473000968000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a54af3de11451c850e30795840","spanId":"e205ff0b1ed96e99","parentSpanId":"63dc8eb7d8b5b04a","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977467124566000","endTimeUnixNano":"1762977471999582000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a54af3de11451c850e30795840","spanId":"63dc8eb7d8b5b04a","parentSpanId":"f8b2634ed95e89e7","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977467124193000","endTimeUnixNano":"1762977471999818000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a570e3915cf6700fbc86886880","spanId":"c16efdcbf238d47f","flags":256,"name":"code_actions","kind":1,"startTimeUnixNano":"1762977476835155000","endTimeUnixNano":"1762977476835369000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"315"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"code_actions"}},{"key":"logfire.msg","value":{"stringValue":"code_actions"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a57f481fa5a628a3dcc3ca8cca","spanId":"3778f63b308765d2","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977480520422000","endTimeUnixNano":"1762977480520422000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a561e87c0cc70fb08e250b8bdb","spanId":"f5880d34340bfbea","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977473000654000","endTimeUnixNano":"1762977480528661000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762977480526294000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a57f50a25ee37cc05d2a12cf0c","spanId":"5de5d7b0fbc198eb","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977480528870000","endTimeUnixNano":"1762977480528870000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a58332ba6c9b26ac679c41052a","spanId":"31ca80dc578c44d9","parentSpanId":"9a919228e1fbfcdd","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977481522956000","endTimeUnixNano":"1762977481522956000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a561e87c0cc70fb08e250b8bdb","spanId":"fb43aaf0a239dee4","parentSpanId":"a67ff64c6f3b406f","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977473002153000","endTimeUnixNano":"1762977480521115000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a561e87c0cc70fb08e250b8bdb","spanId":"a67ff64c6f3b406f","parentSpanId":"f5880d34340bfbea","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977473001642000","endTimeUnixNano":"1762977480521389000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom ai_lsp.models import Settings\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a58332ba6c9b26ac679c41052a","spanId":"d97490ed6fbb71ee","parentSpanId":"90d04a22df9be563","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977481525319000","endTimeUnixNano":"1762977502138732000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4705"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"508"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"3196"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4705"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom ai_lsp.models import DiagnosticResult\\nfrom ai_lsp.models import Settings\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_3df229cc429c4e13b750ae9960f6144f\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"warning\", \"message\": \"The `logfire.configure` call sets `send_to_logfire=False` even when `settings.configure_logfire` is true. This might be a misconfiguration, preventing logs from being sent to Logfire as intended by the `configure_logfire` setting. If the intent is to send logs, this should be enabled.\", \"issue_snippet\": \"send_to_logfire=False\", \"suggested_fixes\": [{\"replacement_snippet\": \"send_to_logfire=True\", \"target_snippet\": \"send_to_logfire=False\", \"title\": \"Enable sending logs to Logfire\"}]}, {\"message\": \"When `find_snippet_in_text` returns multiple occurrences of `issue.issue_snippet`, the code only uses the first one (`positions[0]`) to place the diagnostic. This can lead to diagnostics being incorrectly placed if the AI's semantic analysis refers to a later occurrence of the same snippet. The AI output should ideally include precise location (line/column) for issues, or a more sophisticated matching strategy is needed.\", \"suggested_fixes\": null, \"severity\": \"error\", \"issue_snippet\": \"positions[0]\"}, {\"severity\": \"error\", \"suggested_fixes\": null, \"issue_snippet\": \"target_positions[0]\", \"message\": \"When applying code actions, `find_snippet_in_text` is used to locate the `target_snippet` for a fix, and only the first occurrence (`target_positions[0]`) is used. If `target_snippet` is not unique, the code action might modify the wrong part of the code, leading to unintended bugs or corruption.\"}, {\"suggested_fixes\": null, \"issue_snippet\": \"positions[0]\", \"message\": \"During fix regeneration, the new diagnostic's range is determined by `find_snippet_in_text` on the `regenerated_issue.issue_snippet`, taking only `positions[0]`. This combines the problem of ambiguous snippet placement with the risk of losing the original diagnostic's precise location. If the regenerated snippet differs or is not unique, the diagnostic might be incorrectly re-positioned or applied to the wrong instance, despite the original command specifying a precise line/character for the fix.\", \"severity\": \"error\"}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a58332ba6c9b26ac679c41052a","spanId":"90d04a22df9be563","parentSpanId":"9a919228e1fbfcdd","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977481524201000","endTimeUnixNano":"1762977502146472000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"send_to_logfire=False\", \"severity\": \"warning\", \"message\": \"The `logfire.configure` call sets `send_to_logfire=False` even when `settings.configure_logfire` is true. This might be a misconfiguration, preventing logs from being sent to Logfire as intended by the `configure_logfire` setting. If the intent is to send logs, this should be enabled.\", \"suggested_fixes\": [{\"title\": \"Enable sending logs to Logfire\", \"target_snippet\": \"send_to_logfire=False\", \"replacement_snippet\": \"send_to_logfire=True\"}]}, {\"issue_snippet\": \"positions[0]\", \"severity\": \"error\", \"message\": \"When `find_snippet_in_text` returns multiple occurrences of `issue.issue_snippet`, the code only uses the first one (`positions[0]`) to place the diagnostic. This can lead to diagnostics being incorrectly placed if the AI's semantic analysis refers to a later occurrence of the same snippet. The AI output should ideally include precise location (line/column) for issues, or a more sophisticated matching strategy is needed.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"target_positions[0]\", \"severity\": \"error\", \"message\": \"When applying code actions, `find_snippet_in_text` is used to locate the `target_snippet` for a fix, and only the first occurrence (`target_positions[0]`) is used. If `target_snippet` is not unique, the code action might modify the wrong part of the code, leading to unintended bugs or corruption.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"positions[0]\", \"severity\": \"error\", \"message\": \"During fix regeneration, the new diagnostic's range is determined by `find_snippet_in_text` on the `regenerated_issue.issue_snippet`, taking only `positions[0]`. This combines the problem of ambiguous snippet placement with the risk of losing the original diagnostic's precise location. If the regenerated snippet differs or is not unique, the diagnostic might be incorrectly re-positioned or applied to the wrong instance, despite the original command specifying a precise line/character for the fix.\", \"suggested_fixes\": null}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4705"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"508"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"3196"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4705"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom ai_lsp.models import DiagnosticResult\\nfrom ai_lsp.models import Settings\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any, Literal\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_3df229cc429c4e13b750ae9960f6144f\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"warning\", \"message\": \"The `logfire.configure` call sets `send_to_logfire=False` even when `settings.configure_logfire` is true. This might be a misconfiguration, preventing logs from being sent to Logfire as intended by the `configure_logfire` setting. If the intent is to send logs, this should be enabled.\", \"issue_snippet\": \"send_to_logfire=False\", \"suggested_fixes\": [{\"replacement_snippet\": \"send_to_logfire=True\", \"target_snippet\": \"send_to_logfire=False\", \"title\": \"Enable sending logs to Logfire\"}]}, {\"message\": \"When `find_snippet_in_text` returns multiple occurrences of `issue.issue_snippet`, the code only uses the first one (`positions[0]`) to place the diagnostic. This can lead to diagnostics being incorrectly placed if the AI's semantic analysis refers to a later occurrence of the same snippet. The AI output should ideally include precise location (line/column) for issues, or a more sophisticated matching strategy is needed.\", \"suggested_fixes\": null, \"severity\": \"error\", \"issue_snippet\": \"positions[0]\"}, {\"severity\": \"error\", \"suggested_fixes\": null, \"issue_snippet\": \"target_positions[0]\", \"message\": \"When applying code actions, `find_snippet_in_text` is used to locate the `target_snippet` for a fix, and only the first occurrence (`target_positions[0]`) is used. If `target_snippet` is not unique, the code action might modify the wrong part of the code, leading to unintended bugs or corruption.\"}, {\"suggested_fixes\": null, \"issue_snippet\": \"positions[0]\", \"message\": \"During fix regeneration, the new diagnostic's range is determined by `find_snippet_in_text` on the `regenerated_issue.issue_snippet`, taking only `positions[0]`. This combines the problem of ambiguous snippet placement with the risk of losing the original diagnostic's precise location. If the regenerated snippet differs or is not unique, the diagnostic might be incorrectly re-positioned or applied to the wrong instance, despite the original command specifying a precise line/character for the fix.\", \"severity\": \"error\"}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_3df229cc429c4e13b750ae9960f6144f\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a58332ba6c9b26ac679c41052a","spanId":"a11dea23fffc28d4","parentSpanId":"9a919228e1fbfcdd","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762977502151452000","endTimeUnixNano":"1762977502151452000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 4 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"4"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79a58332ba6c9b26ac679c41052a","spanId":"92d65716a12b1d3c","parentSpanId":"9a919228e1fbfcdd","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977502151637000","endTimeUnixNano":"1762977502151637000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'send_to_logfire=Fals...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"send_to_logfire=Fals"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a58332ba6c9b26ac679c41052a","spanId":"2ada615cc2af8b1c","parentSpanId":"9a919228e1fbfcdd","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977502151812000","endTimeUnixNano":"1762977502151812000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 3 occurrences of 'positions[0]...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"3"}},{"key":"snippet[:20]","value":{"stringValue":"positions[0]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a58332ba6c9b26ac679c41052a","spanId":"1753d6630e6e7da3","parentSpanId":"9a919228e1fbfcdd","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977502151929000","endTimeUnixNano":"1762977502151929000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'target_positions[0]...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"target_positions[0]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a58332ba6c9b26ac679c41052a","spanId":"38a699d8fa78be2d","parentSpanId":"9a919228e1fbfcdd","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977502152057000","endTimeUnixNano":"1762977502152057000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 3 occurrences of 'positions[0]...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"3"}},{"key":"snippet[:20]","value":{"stringValue":"positions[0]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a58332ba6c9b26ac679c41052a","spanId":"9a919228e1fbfcdd","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977481522394000","endTimeUnixNano":"1762977502152119000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79a5d3c847bfae1fb1ce095bac89","spanId":"f2b1c45e1bad5a6a","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762977502152269000","endTimeUnixNano":"1762977502152269000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 4 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"4"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a5ec6af89eee6d5b2acf22ec8d","spanId":"6abb4557b644680b","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977508458447000","endTimeUnixNano":"1762977508458447000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a5f05370e9cfe9f8901c9351a5","spanId":"06c0cfaaa766ad18","parentSpanId":"ac9093a56660180d","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977509459468000","endTimeUnixNano":"1762977509459468000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a5f15985f0da7cbffb80391681","spanId":"0017a842cf3b2a7d","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977509721823000","endTimeUnixNano":"1762977509721823000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a5f05370e9cfe9f8901c9351a5","spanId":"ac9093a56660180d","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977509459072000","endTimeUnixNano":"1762977509725977000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762977509723646000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    result = await self.agent.run(prompt)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a5f15e81d6926c0c4487ee20c1","spanId":"bcda18d8355ca47a","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977509726191000","endTimeUnixNano":"1762977509726191000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a5f4ba9343e2702f232496b54b","spanId":"a36f6f356595bd19","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977510586873000","endTimeUnixNano":"1762977510586873000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a5f4bb5daa08f6be916273857e","spanId":"1c25a678524d844e","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977510587287000","endTimeUnixNano":"1762977510587287000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a5f4c87efe9c93b49c72a7b380","spanId":"326ec924799e6dfe","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977510600992000","endTimeUnixNano":"1762977510600992000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a5f4c9dbd429d7ca28f448076a","spanId":"7abdf16d0d1962a2","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977510601328000","endTimeUnixNano":"1762977510601328000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a5f8b26c2442c3b2c14d8c0c60","spanId":"19e183ee6d795b72","parentSpanId":"51330e2238c0d360","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977511602944000","endTimeUnixNano":"1762977511602944000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a5f05370e9cfe9f8901c9351a5","spanId":"fb869b849fb36e03","parentSpanId":"d9f0c1053d50aa83","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977509461540000","endTimeUnixNano":"1762977509722592000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a5f05370e9cfe9f8901c9351a5","spanId":"d9f0c1053d50aa83","parentSpanId":"ac9093a56660180d","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977509460688000","endTimeUnixNano":"1762977509722918000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom typing import Literal\\n\\n# pyrefly: ignore [missing-import]\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n```\\n\\nFile: models.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a608d3c2b4c2b82b16b627b0a0","spanId":"d5359b3ba50c2101","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977515731819000","endTimeUnixNano":"1762977515731819000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a608e0a4ee49b191c66438185a","spanId":"b6590df6454bcb7a","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977515744801000","endTimeUnixNano":"1762977515744801000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a608e15ca281fc5c2bffc20b3b","spanId":"5a9995affbfd5dde","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977515745108000","endTimeUnixNano":"1762977515745108000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a608e1d19e5f55f31fdcae75aa","spanId":"f7142089cfff0ced","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977515745439000","endTimeUnixNano":"1762977515745439000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a608e14a3fb308a6804bccb56b","spanId":"43ca37c5cd56c44b","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977515745728000","endTimeUnixNano":"1762977515745728000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a60ccbd4f3c72da500bdf5a95f","spanId":"89639e9ea9234e57","parentSpanId":"4ef836800eaa7069","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977516747875000","endTimeUnixNano":"1762977516747875000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a62b9a57cb0d41f19242c8f189","spanId":"8b16f358dd66ab54","flags":256,"name":"code_actions","kind":1,"startTimeUnixNano":"1762977524634468000","endTimeUnixNano":"1762977524635158000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"315"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"code_actions"}},{"key":"logfire.msg","value":{"stringValue":"code_actions"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79a63423da053b38a11531731287","spanId":"703da3c7c5bc46d4","flags":256,"name":"code_actions","kind":1,"startTimeUnixNano":"1762977526819079000","endTimeUnixNano":"1762977526819413000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"315"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"code_actions"}},{"key":"logfire.msg","value":{"stringValue":"code_actions"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79a63796b230a1c7ce8926c644e3","spanId":"e96a1b49d5476972","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977527702838000","endTimeUnixNano":"1762977527702838000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a60ccbd4f3c72da500bdf5a95f","spanId":"4ef836800eaa7069","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977516747061000","endTimeUnixNano":"1762977527710032000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"e720c7a9f0107cb9e83d2bbacd62701180fe65009e65cbc31cd5420ef569071b"}}],"events":[{"timeUnixNano":"1762977527707854000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a6379e72e71bbc3ea6ae3a3dc9","spanId":"b47013266ac69e60","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977527710215000","endTimeUnixNano":"1762977527710215000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a5f8b26c2442c3b2c14d8c0c60","spanId":"a67c03bbb982e309","parentSpanId":"51330e2238c0d360","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762977527901876000","endTimeUnixNano":"1762977527901876000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 2 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"2"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79a5f8b26c2442c3b2c14d8c0c60","spanId":"8d4e3a7ef531bfc1","parentSpanId":"51330e2238c0d360","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977527902044000","endTimeUnixNano":"1762977527902044000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'http://localhost:431...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"http://localhost:431"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a5f8b26c2442c3b2c14d8c0c60","spanId":"040676a8c2065aad","parentSpanId":"51330e2238c0d360","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977527902156000","endTimeUnixNano":"1762977527902156000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'True...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"True"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a5f8b26c2442c3b2c14d8c0c60","spanId":"51330e2238c0d360","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977511602535000","endTimeUnixNano":"1762977527902223000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79a6385eb5a7d362fa2e4911c62f","spanId":"64d1917769c83975","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762977527902379000","endTimeUnixNano":"1762977527902379000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 2 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"2"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/models.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a63b80e852a266e1ab090c90ac","spanId":"20bb10fb8738d76e","parentSpanId":"bde33e938a830cca","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977528705071000","endTimeUnixNano":"1762977528705071000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a60ccbd4f3c72da500bdf5a95f","spanId":"1768c87caa3f3753","parentSpanId":"9db0b8a03456d737","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977516750019000","endTimeUnixNano":"1762977527703289000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a60ccbd4f3c72da500bdf5a95f","spanId":"9db0b8a03456d737","parentSpanId":"4ef836800eaa7069","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977516749192000","endTimeUnixNano":"1762977527703478000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom ai_lsp.models import DiagnosticResult\\nfrom ai_lsp.models import Settings\\nimport asyncio\\nimport os\\nfrom pathlib import Path\\nfrom typing import Any\\nfrom importlib.metadata import version\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a5f8b26c2442c3b2c14d8c0c60","spanId":"3e544896f81f049e","parentSpanId":"e210a97fc4c714ae","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977511604712000","endTimeUnixNano":"1762977527898438000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"771"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"272"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"2883"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"771"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom typing import Literal\\n\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n```\\n\\nFile: models.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_c2ddb67aada0442ba10b5241c586c162\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"suggested_fixes\": null, \"issue_snippet\": \"http://localhost:4318\", \"message\": \"The default OTLP exporter endpoint points to `http://localhost:4318`. While this is suitable for local development and testing, relying on this default in a production environment will result in telemetry data not being collected or being sent to an incorrect location. This is a common operational oversight that can lead to a lack of observability in deployed applications, making debugging and monitoring significantly more challenging.\", \"severity\": \"warning\"}, {\"issue_snippet\": \"True\", \"severity\": \"warning\", \"suggested_fixes\": [{\"target_snippet\": \"True\", \"title\": \"Default Logfire configuration to opt-in\", \"replacement_snippet\": \"False\"}], \"message\": \"[Scrubbed due to 'API key']\"}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.scrubbed","value":{"stringValue":"[{\"path\": [\"attributes\", \"events\", 2, \"message\", \"tool_calls\", 0, \"function\", \"arguments\", \"issues\", 1, \"message\"], \"matched_substring\": \"API key\"}]"}}],"status":{}},{"traceId":"019a79a5f8b26c2442c3b2c14d8c0c60","spanId":"e210a97fc4c714ae","parentSpanId":"51330e2238c0d360","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977511603803000","endTimeUnixNano":"1762977527900202000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"http://localhost:4318\", \"severity\": \"warning\", \"message\": \"The default OTLP exporter endpoint points to `http://localhost:4318`. While this is suitable for local development and testing, relying on this default in a production environment will result in telemetry data not being collected or being sent to an incorrect location. This is a common operational oversight that can lead to a lack of observability in deployed applications, making debugging and monitoring significantly more challenging.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"True\", \"severity\": \"warning\", \"message\": \"[Scrubbed due to 'API key']\", \"suggested_fixes\": [{\"title\": \"Default Logfire configuration to opt-in\", \"target_snippet\": \"True\", \"replacement_snippet\": \"False\"}]}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"771"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"272"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"2883"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"771"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom typing import Literal\\n\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai.models import KnownModelName\\nfrom pydantic_settings import BaseSettings\\n\\n\\nclass Settings(BaseSettings):\\n    ai_lsp_model: KnownModelName = Field(default=\\\"google-gla:gemini-2.5-flash\\\")\\n    debounce_ms: int = Field(default=1000)\\n    max_cache_size: int = Field(default=50)\\n    configure_logfire: bool = Field(default=True)\\n    otel_exporter_otlp_endpoint: str = Field(default=\\\"http://localhost:4318\\\")\\n\\n\\nclass SuggestedFix(BaseModel):\\n    title: str\\n    target_snippet: str\\n    replacement_snippet: str\\n\\n\\nclass CodeIssue(BaseModel):\\n    issue_snippet: str\\n    severity: Literal[\\\"error\\\", \\\"warning\\\", \\\"info\\\", \\\"hint\\\"]\\n    message: str\\n    suggested_fixes: list[SuggestedFix] | None = None\\n\\n\\nclass DiagnosticResult(BaseModel):\\n    issues: list[CodeIssue]\\n\\n```\\n\\nFile: models.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_c2ddb67aada0442ba10b5241c586c162\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"suggested_fixes\": null, \"issue_snippet\": \"http://localhost:4318\", \"message\": \"The default OTLP exporter endpoint points to `http://localhost:4318`. While this is suitable for local development and testing, relying on this default in a production environment will result in telemetry data not being collected or being sent to an incorrect location. This is a common operational oversight that can lead to a lack of observability in deployed applications, making debugging and monitoring significantly more challenging.\", \"severity\": \"warning\"}, {\"issue_snippet\": \"True\", \"severity\": \"warning\", \"suggested_fixes\": [{\"target_snippet\": \"True\", \"title\": \"Default Logfire configuration to opt-in\", \"replacement_snippet\": \"False\"}], \"message\": \"[Scrubbed due to 'API key']\"}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_c2ddb67aada0442ba10b5241c586c162\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}},{"key":"logfire.scrubbed","value":{"stringValue":"[{\"path\": [\"attributes\", \"final_result\", \"issues\", 1, \"message\"], \"matched_substring\": \"API key\"}, {\"path\": [\"attributes\", \"all_messages_events\", 2, \"tool_calls\", 0, \"function\", \"arguments\", \"issues\", 1, \"message\"], \"matched_substring\": \"API key\"}]"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a63b80e852a266e1ab090c90ac","spanId":"dcda12b38d9d2320","parentSpanId":"5b56c0a289ca7674","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977528706602000","endTimeUnixNano":"1762977567129452000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4664"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"479"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"7352"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4664"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom importlib.metadata import version\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\nfrom ai_lsp.models import DiagnosticResult, Settings\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_7fc5cba901b54c12898d6d774b5cf146\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"warning\", \"issue_snippet\": \"positions[0]\", \"message\": \"The `analyze_document` function uses `positions[0]` to create a diagnostic, meaning it only flags the very first occurrence of an `issue_snippet` in the document. If the AI detects a problem that appears multiple times (e.g., a repeated anti-pattern or logic error), only the first instance will be shown to the user, reducing the comprehensiveness of the analysis.\"}, {\"severity\": \"warning\", \"issue_snippet\": \"target_positions[0]\", \"message\": \"In the `code_actions` function, when constructing a quick fix, `target_positions[0]` is used to define the range for the `TextEdit`. This means if the `target_snippet` (the code segment to be replaced by the fix) appears multiple times in the document, the quick fix will only be offered for, and applied to, the first instance. This limits the utility of automated fixes for recurring issues. A single `WorkspaceEdit` can contain multiple `TextEdit`s for the same URI to fix all occurrences simultaneously.\"}, {\"severity\": \"error\", \"issue_snippet\": \"positions[0]\", \"suggested_fixes\": [{\"target_snippet\": \"range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            )\", \"title\": \"Keep diagnostic at original range\", \"replacement_snippet\": \"range=diag.range,\"}], \"message\": \"The `regenerate_fix` command targets a specific diagnostic for update based on its original `start.line` and `start.character`. However, it then determines the new diagnostic's `range` by searching for the *newly generated* `regenerated_issue.issue_snippet` and using its *first occurrence* (`positions[0]`). If this new snippet is found at a different location than the original diagnostic, the regenerated diagnostic will \\\"move\\\", confusing the user who expected the *specific* diagnostic they interacted with to be updated in place. The range of the updated diagnostic should ideally remain consistent with the original diagnostic's location.\"}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a63b80e852a266e1ab090c90ac","spanId":"5b56c0a289ca7674","parentSpanId":"bde33e938a830cca","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977528705936000","endTimeUnixNano":"1762977567136068000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"positions[0]\", \"severity\": \"warning\", \"message\": \"The `analyze_document` function uses `positions[0]` to create a diagnostic, meaning it only flags the very first occurrence of an `issue_snippet` in the document. If the AI detects a problem that appears multiple times (e.g., a repeated anti-pattern or logic error), only the first instance will be shown to the user, reducing the comprehensiveness of the analysis.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"target_positions[0]\", \"severity\": \"warning\", \"message\": \"In the `code_actions` function, when constructing a quick fix, `target_positions[0]` is used to define the range for the `TextEdit`. This means if the `target_snippet` (the code segment to be replaced by the fix) appears multiple times in the document, the quick fix will only be offered for, and applied to, the first instance. This limits the utility of automated fixes for recurring issues. A single `WorkspaceEdit` can contain multiple `TextEdit`s for the same URI to fix all occurrences simultaneously.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"positions[0]\", \"severity\": \"error\", \"message\": \"The `regenerate_fix` command targets a specific diagnostic for update based on its original `start.line` and `start.character`. However, it then determines the new diagnostic's `range` by searching for the *newly generated* `regenerated_issue.issue_snippet` and using its *first occurrence* (`positions[0]`). If this new snippet is found at a different location than the original diagnostic, the regenerated diagnostic will \\\"move\\\", confusing the user who expected the *specific* diagnostic they interacted with to be updated in place. The range of the updated diagnostic should ideally remain consistent with the original diagnostic's location.\", \"suggested_fixes\": [{\"title\": \"Keep diagnostic at original range\", \"target_snippet\": \"range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            )\", \"replacement_snippet\": \"range=diag.range,\"}]}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4664"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"479"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"7352"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4664"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom importlib.metadata import version\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\nfrom ai_lsp.models import DiagnosticResult, Settings\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n            output_type=DiagnosticResult,\\n            system_prompt=\\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\"\\\"\\\",\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_7fc5cba901b54c12898d6d774b5cf146\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"severity\": \"warning\", \"issue_snippet\": \"positions[0]\", \"message\": \"The `analyze_document` function uses `positions[0]` to create a diagnostic, meaning it only flags the very first occurrence of an `issue_snippet` in the document. If the AI detects a problem that appears multiple times (e.g., a repeated anti-pattern or logic error), only the first instance will be shown to the user, reducing the comprehensiveness of the analysis.\"}, {\"severity\": \"warning\", \"issue_snippet\": \"target_positions[0]\", \"message\": \"In the `code_actions` function, when constructing a quick fix, `target_positions[0]` is used to define the range for the `TextEdit`. This means if the `target_snippet` (the code segment to be replaced by the fix) appears multiple times in the document, the quick fix will only be offered for, and applied to, the first instance. This limits the utility of automated fixes for recurring issues. A single `WorkspaceEdit` can contain multiple `TextEdit`s for the same URI to fix all occurrences simultaneously.\"}, {\"severity\": \"error\", \"issue_snippet\": \"positions[0]\", \"suggested_fixes\": [{\"target_snippet\": \"range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            )\", \"title\": \"Keep diagnostic at original range\", \"replacement_snippet\": \"range=diag.range,\"}], \"message\": \"The `regenerate_fix` command targets a specific diagnostic for update based on its original `start.line` and `start.character`. However, it then determines the new diagnostic's `range` by searching for the *newly generated* `regenerated_issue.issue_snippet` and using its *first occurrence* (`positions[0]`). If this new snippet is found at a different location than the original diagnostic, the regenerated diagnostic will \\\"move\\\", confusing the user who expected the *specific* diagnostic they interacted with to be updated in place. The range of the updated diagnostic should ideally remain consistent with the original diagnostic's location.\"}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_7fc5cba901b54c12898d6d774b5cf146\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a63b80e852a266e1ab090c90ac","spanId":"2f1c52281c860f4f","parentSpanId":"bde33e938a830cca","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762977567142136000","endTimeUnixNano":"1762977567142136000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 3 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"3"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79a63b80e852a266e1ab090c90ac","spanId":"9187344bcb876429","parentSpanId":"bde33e938a830cca","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977567142435000","endTimeUnixNano":"1762977567142435000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 3 occurrences of 'positions[0]...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"3"}},{"key":"snippet[:20]","value":{"stringValue":"positions[0]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a63b80e852a266e1ab090c90ac","spanId":"9ef3c0e1bd619e7b","parentSpanId":"bde33e938a830cca","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977567142620000","endTimeUnixNano":"1762977567142620000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'target_positions[0]...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"target_positions[0]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a63b80e852a266e1ab090c90ac","spanId":"a0f8f8639a2ecd95","parentSpanId":"bde33e938a830cca","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977567142824000","endTimeUnixNano":"1762977567142824000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 3 occurrences of 'positions[0]...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"3"}},{"key":"snippet[:20]","value":{"stringValue":"positions[0]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a63b80e852a266e1ab090c90ac","spanId":"bde33e938a830cca","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977528704588000","endTimeUnixNano":"1762977567142921000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79a6d1a76e7315ad0239e87d0429","spanId":"8f3badb2b6b26aa7","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762977567143118000","endTimeUnixNano":"1762977567143118000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 3 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"3"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a8db9b7b23e5955c1615035c91","spanId":"fd5289174b0044ab","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977700763228000","endTimeUnixNano":"1762977700763228000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"291"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a8de87ee0f0c4e9a1ff7933bdf","spanId":"6870ad81cc63c24e","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977701511077000","endTimeUnixNano":"1762977701511077000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a8de877441f8bcf456f8d20ca7","spanId":"cf603ba4c0df6ed4","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977701511387000","endTimeUnixNano":"1762977701511387000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a8e2719d0ac23d4bbcefd092f0","spanId":"50284d35dbdb35b2","parentSpanId":"d815dcca3c730c47","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977702514195000","endTimeUnixNano":"1762977702514195000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a8ecf0448cd8bd0cc4c3d5d28d","spanId":"4f768fb755c155f0","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977705199994000","endTimeUnixNano":"1762977705199994000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a8e2719d0ac23d4bbcefd092f0","spanId":"d815dcca3c730c47","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977702513089000","endTimeUnixNano":"1762977705203573000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977705201498000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a8ecf3a0fbd9a564ec5f9603c8","spanId":"9f2314427fb494a8","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977705203758000","endTimeUnixNano":"1762977705203758000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a8f0d9104624d9bf489a819842","spanId":"2bd89bd6b69c0475","parentSpanId":"943ea179358fa16a","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977706202173000","endTimeUnixNano":"1762977706202173000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a8e2719d0ac23d4bbcefd092f0","spanId":"9805ca9ddb6e86e5","parentSpanId":"1dfaf9efcdf21773","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977702517757000","endTimeUnixNano":"1762977705200524000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a8e2719d0ac23d4bbcefd092f0","spanId":"1dfaf9efcdf21773","parentSpanId":"d815dcca3c730c47","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977702516356000","endTimeUnixNano":"1762977705200682000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .settings import settings  # adjust import to your layout\\nfrom .models import DiagnosticResult  # wherever DiagnosticResult lives\\nfrom .agent_lib import Agent  # whatever module actually defines Agent\\n\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a909c779d7df87bd07e7f0c724","spanId":"a0985444aad2148f","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977712583884000","endTimeUnixNano":"1762977712583884000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a8f0d9104624d9bf489a819842","spanId":"943ea179358fa16a","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977706201731000","endTimeUnixNano":"1762977712588122000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977712585829000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a909cccf8630ab391c77d8e36a","spanId":"f755506ccc2c3cbc","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977712588334000","endTimeUnixNano":"1762977712588334000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a90db3272f7b05a96495dfa480","spanId":"cbba1026acbe0395","parentSpanId":"57372057a27f3101","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977713588179000","endTimeUnixNano":"1762977713588179000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a90fde7c2028a5af60964908b8","spanId":"af518bd57496375d","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977714142801000","endTimeUnixNano":"1762977714142801000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a90db3272f7b05a96495dfa480","spanId":"57372057a27f3101","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977713587309000","endTimeUnixNano":"1762977714146746000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977714144505000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a90fe3c097731cf13da4977dc6","spanId":"c6c9c61e5719047d","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977714147079000","endTimeUnixNano":"1762977714147079000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a910e39c4437a7c95fd6c65b38","spanId":"47802c56fe55f38f","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977714403573000","endTimeUnixNano":"1762977714403573000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a910e4790cc5f666a962e0f9a6","spanId":"58dc59921d936c16","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977714404124000","endTimeUnixNano":"1762977714404124000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a8f0d9104624d9bf489a819842","spanId":"e2285dc52aa72b37","parentSpanId":"561af3fb597f0ca6","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977706203665000","endTimeUnixNano":"1762977712584650000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a8f0d9104624d9bf489a819842","spanId":"561af3fb597f0ca6","parentSpanId":"943ea179358fa16a","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977706203002000","endTimeUnixNano":"1762977712584897000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .settings import settings  # adjust import to your layout\\nfrom .models import DiagnosticResult  # wherever DiagnosticResult lives\\nfrom .agent_lib import Agent  # whatever module actually defines Agent\\n\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a90db3272f7b05a96495dfa480","spanId":"1cd33036acdfd1a1","parentSpanId":"1ed421015ef013ec","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977713591027000","endTimeUnixNano":"1762977714143350000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a90db3272f7b05a96495dfa480","spanId":"1ed421015ef013ec","parentSpanId":"57372057a27f3101","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977713589644000","endTimeUnixNano":"1762977714143621000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult  # wherever DiagnosticResult lives\\nfrom .agent_lib import Agent  # whatever module actually defines Agent\\n\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a91179458e3176f0abae4598c0","spanId":"07c8ec4e14ce4467","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977714553012000","endTimeUnixNano":"1762977714553012000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a91179ee9827e9796576d852ff","spanId":"3d92b6e935406e24","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977714553478000","endTimeUnixNano":"1762977714553478000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9120e0c70c3f541a1dd464113","spanId":"34a59c13eb891e08","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977714702911000","endTimeUnixNano":"1762977714702911000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9120fc4a46c64bd41937caa36","spanId":"b9ea5e2e1bed0ace","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977714703364000","endTimeUnixNano":"1762977714703364000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a912a55dbd1e717befa7825ec7","spanId":"fd8908318f4193e4","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977714853014000","endTimeUnixNano":"1762977714853014000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a912a5fd114dd438a8479f9491","spanId":"1f63719f78035874","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977714853349000","endTimeUnixNano":"1762977714853349000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9133a2cbd6ece268fc478a05c","spanId":"28ceaf311a76f32a","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977715002849000","endTimeUnixNano":"1762977715002849000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9133b95d4051bbc35ce6e882c","spanId":"1533da21bce604b7","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977715003188000","endTimeUnixNano":"1762977715003188000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a913d17527b81bb5a20a54c515","spanId":"f9674a6ee049b89f","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977715153142000","endTimeUnixNano":"1762977715153142000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a913d10ea29295abd0c8fe22e8","spanId":"6476aa908e11a49e","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977715153414000","endTimeUnixNano":"1762977715153414000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a91467143fff1f6189c293ea80","spanId":"020f316478b9e69a","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977715303235000","endTimeUnixNano":"1762977715303235000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a91467a4c41776472980bef8e9","spanId":"857a0ae97fc139e8","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977715303591000","endTimeUnixNano":"1762977715303591000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a914fdba8d11462eb0e1c4ac0d","spanId":"18ea4d954510f9a5","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977715453390000","endTimeUnixNano":"1762977715453390000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a914fde7e67599248ab53441ee","spanId":"085535544c5c1ae8","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977715453827000","endTimeUnixNano":"1762977715453827000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a91594037e3cc5c3d0924b3caf","spanId":"c58486e4185cb423","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977715604752000","endTimeUnixNano":"1762977715604752000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9159502c27b4bfee3689fb88b","spanId":"41794b125a01ccbd","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977715605184000","endTimeUnixNano":"1762977715605184000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9162b80be19738f53090a920a","spanId":"575a805624879a5d","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977715755636000","endTimeUnixNano":"1762977715755636000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9162cbf6997e1d3aac29b6955","spanId":"2a624953c77b0699","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977715756116000","endTimeUnixNano":"1762977715756116000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a916c1763afafe0435aaffe5cf","spanId":"91d4bd8ed9e6b353","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977715905335000","endTimeUnixNano":"1762977715905335000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a916c13b010e8dc1046dea0c46","spanId":"ecae8190e87c5fa8","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977715905762000","endTimeUnixNano":"1762977715905762000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a917562f0997963ba408ba5ef1","spanId":"593822c175ce887e","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977716054169000","endTimeUnixNano":"1762977716054169000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a917561b20553ed468082882f9","spanId":"2ddfb7acd60da808","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977716054620000","endTimeUnixNano":"1762977716054620000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9180f2b4621dfe1aca1ed91a9","spanId":"9df4c60e680df693","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977716239008000","endTimeUnixNano":"1762977716239008000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9180fa43287f746305a582975","spanId":"383eb460cba5553d","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977716239540000","endTimeUnixNano":"1762977716239540000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a91bf90acae69d2e22cbc9732c","spanId":"577b8ea13c8ded59","parentSpanId":"9dcf1c2c051fbdb2","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977717241669000","endTimeUnixNano":"1762977717241669000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a923b1574a414fe64e4ef545d1","spanId":"dddfd52ee262ffe3","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977719217624000","endTimeUnixNano":"1762977719217624000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a91bf90acae69d2e22cbc9732c","spanId":"9dcf1c2c051fbdb2","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977717241043000","endTimeUnixNano":"1762977719224459000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977719221107000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a923b8b685028f4c5aa52e4551","spanId":"ffc91d438f6710b7","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977719224833000","endTimeUnixNano":"1762977719224833000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a923bb01b6d67902eff4eecf03","spanId":"343ae9ddcf10a140","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977719227797000","endTimeUnixNano":"1762977719227797000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a923bc683d0cb388d683c3638a","spanId":"4619e10c9ce4ef49","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977719228144000","endTimeUnixNano":"1762977719228144000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a91bf90acae69d2e22cbc9732c","spanId":"88a7d5986256e81c","parentSpanId":"330e94a35d3bfc62","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977717243398000","endTimeUnixNano":"1762977719219491000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a91bf90acae69d2e22cbc9732c","spanId":"330e94a35d3bfc62","parentSpanId":"9dcf1c2c051fbdb2","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977717242511000","endTimeUnixNano":"1762977719219802000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResul\\nfrom .agent_lib import Agent  # whatever module actually defines Agent\\n\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a925a019301a46e744c9afbe95","spanId":"23491be8b2411da7","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977719712896000","endTimeUnixNano":"1762977719712896000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a925a11f0e6cf742874e0087b5","spanId":"adfd696b9eb3062b","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977719713752000","endTimeUnixNano":"1762977719713752000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a926f6f86854f5e19909d1b022","spanId":"0d850d85c7047a36","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977720054165000","endTimeUnixNano":"1762977720054165000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a926f6b299223c7c9eb9156d53","spanId":"97c323b87f51f251","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977720054835000","endTimeUnixNano":"1762977720054835000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a928df595b637b68b885664edc","spanId":"ac248b5fe7c6b985","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977720543519000","endTimeUnixNano":"1762977720543519000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a928e03fecf7204545372061a1","spanId":"1513cfa7be20d88d","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977720544663000","endTimeUnixNano":"1762977720544663000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a928ee6bf76c0aa39c998c3da2","spanId":"27c06c81cda33c02","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977720558569000","endTimeUnixNano":"1762977720558569000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a928ee413f254967c2c1457ed4","spanId":"0b1ec029bb4074a0","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977720558964000","endTimeUnixNano":"1762977720558964000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a92cd85242d98719b76f48652d","spanId":"a7841ffd441f01ef","parentSpanId":"29186ab5e9c08926","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977721560438000","endTimeUnixNano":"1762977721560438000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a92d5efe79a7b4d9587660cbb5","spanId":"054764a75574e5de","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977721694940000","endTimeUnixNano":"1762977721694940000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a92cd85242d98719b76f48652d","spanId":"29186ab5e9c08926","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977721560100000","endTimeUnixNano":"1762977721698646000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977721696504000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a92d6290835a82d956db683efb","spanId":"48915af5299a6335","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977721698832000","endTimeUnixNano":"1762977721698832000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a92d751647c17506a443720a09","spanId":"33f43db66b976ee7","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977721717292000","endTimeUnixNano":"1762977721717292000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a92d75a27c66c13e6dbab1451b","spanId":"77c8c2409f2e0264","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977721717577000","endTimeUnixNano":"1762977721717577000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9315ec28e8feea61ed4c40d94","spanId":"452511c9ed1185c4","parentSpanId":"2c9da725f11322a1","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977722719140000","endTimeUnixNano":"1762977722719140000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a92cd85242d98719b76f48652d","spanId":"d621f8e50823e2c5","parentSpanId":"c4a19be6df91dd73","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977721561686000","endTimeUnixNano":"1762977721695473000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a92cd85242d98719b76f48652d","spanId":"c4a19be6df91dd73","parentSpanId":"29186ab5e9c08926","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977721561098000","endTimeUnixNano":"1762977721695670000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult, Settings, SuggestedFix\\nfrom .agent_lib import Agent  # whatever module actually defines Agent\\n\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a93b0020fc2474b8c281757e16","spanId":"9a86f376be981916","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977725184930000","endTimeUnixNano":"1762977725184930000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9315ec28e8feea61ed4c40d94","spanId":"2c9da725f11322a1","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977722718763000","endTimeUnixNano":"1762977725188861000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977725186801000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a93b05a361ea932d11eff6be16","spanId":"075dbd7ae76d801e","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977725188997000","endTimeUnixNano":"1762977725188997000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a93eea9577f9a264b7bc842c9b","spanId":"63fabd86c7fb56fd","parentSpanId":"445b7381df12c802","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977726187645000","endTimeUnixNano":"1762977726187645000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a93f11ad95ecb6599b580d5e70","spanId":"ef168c0ec1523280","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977726225552000","endTimeUnixNano":"1762977726225552000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a93eea9577f9a264b7bc842c9b","spanId":"445b7381df12c802","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977726186906000","endTimeUnixNano":"1762977726231035000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"f49cff508f46786ad5391d474233285d00fc1ad6af034b5c176c80cd24576785"}}],"events":[{"timeUnixNano":"1762977726227525000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        req, traces=traces, timeout=real_timeout\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<7 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1283, in _wrap_create_connection\n    return await self._loop.create_connection(*args, **kwargs, sock=sock)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/base_events.py\", line 1182, in create_connection\n    transport, protocol = await self._create_connection_transport(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n        ssl_shutdown_timeout=ssl_shutdown_timeout)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/base_events.py\", line 1215, in _create_connection_transport\n    await waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a93f1796280a7ca3b395b4c201","spanId":"f9249d8387c9392f","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977726231216000","endTimeUnixNano":"1762977726231216000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a93fdacb0ee913fa2f1722cc9c","spanId":"bdb5fa96e7b452b5","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977726426209000","endTimeUnixNano":"1762977726426209000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a93fda08c193f9f9c26836e7b8","spanId":"de7922b3a6f3c97c","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977726426745000","endTimeUnixNano":"1762977726426745000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9400bbcb8ba3d2f74873dfd6c","spanId":"2ebd94733b91d7d9","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977726475965000","endTimeUnixNano":"1762977726475965000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9400c496a08dc710b53e4d07a","spanId":"2b9e6d62e6d02950","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977726476283000","endTimeUnixNano":"1762977726476283000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a940a3fec7294d63e691068823","spanId":"1c296121c781574c","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977726627198000","endTimeUnixNano":"1762977726627198000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a940a33de089fd1e02bb2ff696","spanId":"885e57f9f5f01aaa","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977726627623000","endTimeUnixNano":"1762977726627623000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9448d0f4d7105b8ce51aac6e2","spanId":"a500f0ae94bf090a","parentSpanId":"ff3a48bbc36dae04","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977727629840000","endTimeUnixNano":"1762977727629840000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a94af6bb58edb54519b1d2a647","spanId":"dc3d0bda005beae9","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977729270938000","endTimeUnixNano":"1762977729270938000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9448d0f4d7105b8ce51aac6e2","spanId":"ff3a48bbc36dae04","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977727629012000","endTimeUnixNano":"1762977729277370000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977729274203000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a94afd08cd85cedee2961b09ef","spanId":"cd74856fe89eb361","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977729277628000","endTimeUnixNano":"1762977729277628000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a9315ec28e8feea61ed4c40d94","spanId":"c2ce5c2b007eaca1","parentSpanId":"49a0549613eb6753","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977722720495000","endTimeUnixNano":"1762977725185659000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a9315ec28e8feea61ed4c40d94","spanId":"49a0549613eb6753","parentSpanId":"2c9da725f11322a1","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977722719885000","endTimeUnixNano":"1762977725185887000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom .agent_lib import Agent  # whatever module actually defines Agent\\n\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a93eea9577f9a264b7bc842c9b","spanId":"4d105152d79af493","parentSpanId":"f49b091cc193c0af","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977726190514000","endTimeUnixNano":"1762977726226232000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a93eea9577f9a264b7bc842c9b","spanId":"f49b091cc193c0af","parentSpanId":"445b7381df12c802","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977726189265000","endTimeUnixNano":"1762977726226444000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\n\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a9448d0f4d7105b8ce51aac6e2","spanId":"8281934fc2209c3b","parentSpanId":"23ced189850585ac","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977727633245000","endTimeUnixNano":"1762977729272616000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a9448d0f4d7105b8ce51aac6e2","spanId":"23ced189850585ac","parentSpanId":"ff3a48bbc36dae04","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977727631635000","endTimeUnixNano":"1762977729272981000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a94d5c444734c71c5eacd96a17","spanId":"e14026316a3609c0","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977729884729000","endTimeUnixNano":"1762977729884729000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a94d5fba330c9a95efb6ca180e","spanId":"7d9b9e91b03f640c","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977729887876000","endTimeUnixNano":"1762977729887876000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a94d6fa7c8d2e7d869d1017d5d","spanId":"8b1652a19e0c9233","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977729903253000","endTimeUnixNano":"1762977729903253000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a94d6f03454e4d3334c2b33e9e","spanId":"5fe85c24ef06cb83","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977729903626000","endTimeUnixNano":"1762977729903626000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a95158a69abfae4769b4327b08","spanId":"96fe95f036f35ca5","parentSpanId":"e97f57ea567b6e93","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977730905142000","endTimeUnixNano":"1762977730905142000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9529c8aedd0f07b620d55b294","spanId":"cf43e9f35a202048","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977731228642000","endTimeUnixNano":"1762977731228642000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a95158a69abfae4769b4327b08","spanId":"e97f57ea567b6e93","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977730904759000","endTimeUnixNano":"1762977731232740000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977731230364000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a952a0104e0d63115e273cc2ae","spanId":"433c49a01d96ee0a","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977731232942000","endTimeUnixNano":"1762977731232942000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9568578042b69278f85af878d","spanId":"2b0e028ddeabf232","parentSpanId":"0936638e9784d412","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977732230363000","endTimeUnixNano":"1762977732230363000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a95158a69abfae4769b4327b08","spanId":"f4b4dc481646827a","parentSpanId":"18dc12dd982c4765","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977730906341000","endTimeUnixNano":"1762977731229293000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a95158a69abfae4769b4327b08","spanId":"18dc12dd982c4765","parentSpanId":"e97f57ea567b6e93","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977730905795000","endTimeUnixNano":"1762977731229490000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a9568578042b69278f85af878d","spanId":"0465a7462eece4dc","parentSpanId":"0d40ec939c3c3ee5","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977732231562000","endTimeUnixNano":"1762977745995286000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"930"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"212"}},{"key":"gen_ai.usage.details.cached_content_tokens","value":{"intValue":"844"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"2254"}},{"key":"gen_ai.usage.details.text_cache_tokens","value":{"intValue":"844"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"930"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_cbb37ae422bc4f3697f1376fb8fba0a1\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"issue_snippet\": \"create_diagnostic_agent\", \"message\": \"The `create_diagnostic_agent` function directly accesses `settings.ai_lsp_model` without it being passed as an argument. This introduces a global dependency, tightly coupling the function to the `settings` object. This pattern reduces testability, makes the function harder to reuse in different contexts, and goes against the principle of dependency injection.\", \"severity\": \"warning\", \"suggested_fixes\": [{\"replacement_snippet\": \"def create_diagnostic_agent(ai_model: Any) -> Agent[DiagnosticResult, Any]:\", \"title\": \"Inject the AI model as an argument\", \"target_snippet\": \"def create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\"}, {\"replacement_snippet\": \"model=ai_model,\", \"target_snippet\": \"model=settings.ai_lsp_model,\", \"title\": \"Use the injected AI model\"}]}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a9568578042b69278f85af878d","spanId":"0d40ec939c3c3ee5","parentSpanId":"0936638e9784d412","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977732231042000","endTimeUnixNano":"1762977746000464000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"create_diagnostic_agent\", \"severity\": \"warning\", \"message\": \"The `create_diagnostic_agent` function directly accesses `settings.ai_lsp_model` without it being passed as an argument. This introduces a global dependency, tightly coupling the function to the `settings` object. This pattern reduces testability, makes the function harder to reuse in different contexts, and goes against the principle of dependency injection.\", \"suggested_fixes\": [{\"title\": \"Inject the AI model as an argument\", \"target_snippet\": \"def create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\", \"replacement_snippet\": \"def create_diagnostic_agent(ai_model: Any) -> Agent[DiagnosticResult, Any]:\"}, {\"title\": \"Use the injected AI model\", \"target_snippet\": \"model=settings.ai_lsp_model,\", \"replacement_snippet\": \"model=ai_model,\"}]}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"930"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"212"}},{"key":"gen_ai.usage.details.cached_content_tokens","value":{"intValue":"844"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"2254"}},{"key":"gen_ai.usage.details.text_cache_tokens","value":{"intValue":"844"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"930"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_cbb37ae422bc4f3697f1376fb8fba0a1\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"issue_snippet\": \"create_diagnostic_agent\", \"message\": \"The `create_diagnostic_agent` function directly accesses `settings.ai_lsp_model` without it being passed as an argument. This introduces a global dependency, tightly coupling the function to the `settings` object. This pattern reduces testability, makes the function harder to reuse in different contexts, and goes against the principle of dependency injection.\", \"severity\": \"warning\", \"suggested_fixes\": [{\"replacement_snippet\": \"def create_diagnostic_agent(ai_model: Any) -> Agent[DiagnosticResult, Any]:\", \"title\": \"Inject the AI model as an argument\", \"target_snippet\": \"def create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\"}, {\"replacement_snippet\": \"model=ai_model,\", \"target_snippet\": \"model=settings.ai_lsp_model,\", \"title\": \"Use the injected AI model\"}]}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_cbb37ae422bc4f3697f1376fb8fba0a1\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a9568578042b69278f85af878d","spanId":"4981cc39fa85c976","parentSpanId":"0936638e9784d412","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762977746004243000","endTimeUnixNano":"1762977746004243000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 1 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"1"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79a9568578042b69278f85af878d","spanId":"966ce9f2ee8eb0b8","parentSpanId":"0936638e9784d412","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977746004804000","endTimeUnixNano":"1762977746004804000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'create_diagnostic_ag...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"create_diagnostic_ag"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79a9568578042b69278f85af878d","spanId":"0936638e9784d412","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977732229979000","endTimeUnixNano":"1762977746005090000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79a98c55b6130de6074af34037db","spanId":"1f416ec486e814d9","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762977746005572000","endTimeUnixNano":"1762977746005572000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 1 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"1"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a9d5fa63dff0f62fc1c3d7b090","spanId":"68fa572d03de3d8a","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977764858340000","endTimeUnixNano":"1762977764858340000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9d68670685fa31d9adbe82dc4","spanId":"e7b6ea3d69a86ec9","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977764998015000","endTimeUnixNano":"1762977764998015000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9d686b9c1a0cf9dddb42996f0","spanId":"c9f43b692c669b19","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977764998430000","endTimeUnixNano":"1762977764998430000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9da3a70f49f91f371ab582b07","spanId":"95db4f6d314a1149","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977765946665000","endTimeUnixNano":"1762977765946665000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9da3dadfdeb719effb584892d","spanId":"927796aec612a7f1","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977765949077000","endTimeUnixNano":"1762977765949077000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9da48318c8fa5b6fb87848814","spanId":"a5e3b13d0404ab9c","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977765960227000","endTimeUnixNano":"1762977765960227000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9da48e1be69faa937fe2b4b04","spanId":"4f7d397b26059a46","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977765960757000","endTimeUnixNano":"1762977765960757000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9de31706343d6a98b46a72cf0","spanId":"8af2bda1a48d20f5","parentSpanId":"0103962b1faf41d2","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977766962427000","endTimeUnixNano":"1762977766962427000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9dfa74a99ecf392689b8a3102","spanId":"44063ede33f390be","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977767335120000","endTimeUnixNano":"1762977767335120000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9de31706343d6a98b46a72cf0","spanId":"0103962b1faf41d2","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977766961906000","endTimeUnixNano":"1762977767342296000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977767339009000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a9dfae206278cb092c02b07fa9","spanId":"afdf2ab72aec94f6","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977767342591000","endTimeUnixNano":"1762977767342591000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9e3919df99023612c60f375bf","spanId":"cd4918f1dd9adb7c","parentSpanId":"c8c3f067211f021e","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977768338227000","endTimeUnixNano":"1762977768338227000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9e486270f55b2270ee3dd9ec7","spanId":"a9c16dceb004a505","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977768582136000","endTimeUnixNano":"1762977768582136000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9e3919df99023612c60f375bf","spanId":"c8c3f067211f021e","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977768337827000","endTimeUnixNano":"1762977768589157000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977768585386000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a9e48dad96e9fbdda83a98797f","spanId":"f94a12c99e67cf94","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977768589582000","endTimeUnixNano":"1762977768589582000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9e4f4c3c2a7929b61259e43ce","spanId":"9125d80b9bc324da","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977768692315000","endTimeUnixNano":"1762977768692315000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9e4f401516053d7e0fe59303c","spanId":"1ab6c9b1032c5f93","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977768692643000","endTimeUnixNano":"1762977768692643000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9e775dd439ea557c1d2a6e0a5","spanId":"bfa89937ae611091","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977769332987000","endTimeUnixNano":"1762977769332987000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9e776593bd57fd8190c43cd0a","spanId":"599fdc5b3f66ab18","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977769334906000","endTimeUnixNano":"1762977769334906000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9e85e2f7a5ce19779f2a2c3be","spanId":"376367da2d79786c","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977769566763000","endTimeUnixNano":"1762977769566763000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9e85f48d17b80df5d07a947dd","spanId":"7e8347fb70e34537","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977769567308000","endTimeUnixNano":"1762977769567308000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a9de31706343d6a98b46a72cf0","spanId":"5e878cdf5aa83533","parentSpanId":"4f82a6f1414ce2ab","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977766964758000","endTimeUnixNano":"1762977767337228000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a9de31706343d6a98b46a72cf0","spanId":"4f82a6f1414ce2ab","parentSpanId":"0103962b1faf41d2","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977766963392000","endTimeUnixNano":"1762977767337543000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent(model: Any) -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a9e3919df99023612c60f375bf","spanId":"9534a764b4bf840d","parentSpanId":"a86b01acef5e4543","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977768339748000","endTimeUnixNano":"1762977768583750000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a9e3919df99023612c60f375bf","spanId":"a86b01acef5e4543","parentSpanId":"c8c3f067211f021e","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977768339019000","endTimeUnixNano":"1762977768584163000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent() -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a9ea5871da9d4705e2c003ae27","spanId":"72d5130f237491df","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977770072455000","endTimeUnixNano":"1762977770072455000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9ea598b03deb1e7d22c8b25a9","spanId":"32007b411e6755c7","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977770073386000","endTimeUnixNano":"1762977770073386000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9ee42e47be24aab964f37de9f","spanId":"8c2d527354c047c2","parentSpanId":"48231ce52a88e7a1","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977771076018000","endTimeUnixNano":"1762977771076018000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9f015370d19bc92b4da9f406a","spanId":"20df669a8b40db60","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977771541677000","endTimeUnixNano":"1762977771541677000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9ee42e47be24aab964f37de9f","spanId":"48231ce52a88e7a1","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977771074959000","endTimeUnixNano":"1762977771548072000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977771544468000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a9f01cbea2b2dfca60cef2fe39","spanId":"0c058440fde05e5f","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977771548282000","endTimeUnixNano":"1762977771548282000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9f1e67c211a684979450199a4","spanId":"4ed05c8b03d2c9bb","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977772006813000","endTimeUnixNano":"1762977772006813000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9f1e762ef89be9994cb1bac56","spanId":"b297af1365998760","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977772007927000","endTimeUnixNano":"1762977772007927000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9f5d150ab9898690a4d26fb32","spanId":"99de978891e03a32","parentSpanId":"825c9a7c46308d2c","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977773009753000","endTimeUnixNano":"1762977773009753000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9fa4304373c790118b255eb4d","spanId":"39854bbee8bb0570","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977774147477000","endTimeUnixNano":"1762977774147477000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9f5d150ab9898690a4d26fb32","spanId":"825c9a7c46308d2c","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977773009028000","endTimeUnixNano":"1762977774153478000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977774150327000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79a9fa49ea3248126d93f1ee016b","spanId":"adce3d615395af59","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977774153756000","endTimeUnixNano":"1762977774153756000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9fb4a7992864d8585100d3c75","spanId":"c6a8c698c3b5e11e","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977774410841000","endTimeUnixNano":"1762977774410841000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9fb4ba3ae8129ba7e281aa9d0","spanId":"f2943d875ef1efb0","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977774411264000","endTimeUnixNano":"1762977774411264000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9fba9eb208fd9bc97398c6d3d","spanId":"4de69d4f692286f3","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977774505020000","endTimeUnixNano":"1762977774505020000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9fba977cd6872dcc547c6748a","spanId":"4be70710fb5eb9d2","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977774505372000","endTimeUnixNano":"1762977774505372000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a9ee42e47be24aab964f37de9f","spanId":"05f8b9c5ab4b8951","parentSpanId":"4128f4f7681d0669","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977771079490000","endTimeUnixNano":"1762977771542628000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a9ee42e47be24aab964f37de9f","spanId":"4128f4f7681d0669","parentSpanId":"48231ce52a88e7a1","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977771078107000","endTimeUnixNano":"1762977771542918000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent(model) -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79a9f5d150ab9898690a4d26fb32","spanId":"707ade1766a1cf94","parentSpanId":"6e68cd2f360f5ddc","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977773012799000","endTimeUnixNano":"1762977774148773000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a9f5d150ab9898690a4d26fb32","spanId":"6e68cd2f360f5ddc","parentSpanId":"825c9a7c46308d2c","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977773011467000","endTimeUnixNano":"1762977774149077000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent(model: ) -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79a9ff929381b4390aa6d92f40f0","spanId":"8ce57d35aa32fea3","parentSpanId":"894e01b162d4e367","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977775507661000","endTimeUnixNano":"1762977775507661000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa01e8d978e93737f04ff4b1bb","spanId":"39188a73fb3f8a87","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977776104166000","endTimeUnixNano":"1762977776104166000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79a9ff929381b4390aa6d92f40f0","spanId":"894e01b162d4e367","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977775506395000","endTimeUnixNano":"1762977776111001000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977776107633000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79aa01efd109a956aa1bcd638a93","spanId":"6cd1419a6cbe4c93","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977776111302000","endTimeUnixNano":"1762977776111302000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa0277332daa8f81cee396b8fe","spanId":"82c63857568e6038","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977776247078000","endTimeUnixNano":"1762977776247078000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa02777034be45e15845600f83","spanId":"1e83974fe2cb88de","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977776247423000","endTimeUnixNano":"1762977776247423000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa0447384933020409571f4b50","spanId":"97a9b335abc938ed","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977776711277000","endTimeUnixNano":"1762977776711277000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa04480dbb7920ede34e69885c","spanId":"1358893098a94439","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977776712113000","endTimeUnixNano":"1762977776712113000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa04fe05579d6b11846e99d582","spanId":"1d31d413dda5cc52","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977776894078000","endTimeUnixNano":"1762977776894078000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa04fe56f3cf0aa8998fffe7b1","spanId":"dc0b52c3107c0364","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977776894747000","endTimeUnixNano":"1762977776894747000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa0593b8f489b33562a48d5de6","spanId":"e5c7ae84dc75ce41","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977777043481000","endTimeUnixNano":"1762977777043481000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa0593c7749aff43f1798d49ed","spanId":"82412df2ba47fcda","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977777043785000","endTimeUnixNano":"1762977777043785000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa097d00893d98fb717ab29ae7","spanId":"d339a13071bc3f9b","parentSpanId":"8e17a0267f21f049","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977778046242000","endTimeUnixNano":"1762977778046242000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa09c9cbcf4bdab33eb6f54c1c","spanId":"1fa89fa0af7342b1","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977778121704000","endTimeUnixNano":"1762977778121704000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa097d00893d98fb717ab29ae7","spanId":"8e17a0267f21f049","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977778045321000","endTimeUnixNano":"1762977778128354000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"f49cff508f46786ad5391d474233285d00fc1ad6af034b5c176c80cd24576785"}}],"events":[{"timeUnixNano":"1762977778123953000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 725, in _connect_and_send_request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        req, traces=traces, timeout=real_timeout\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 642, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1209, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1550, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<7 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/connector.py\", line 1283, in _wrap_create_connection\n    return await self._loop.create_connection(*args, **kwargs, sock=sock)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/base_events.py\", line 1182, in create_connection\n    transport, protocol = await self._create_connection_transport(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n        ssl_shutdown_timeout=ssl_shutdown_timeout)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/asyncio/base_events.py\", line 1215, in _create_connection_transport\n    await waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79aa09d0639024e82141267c7f4e","spanId":"628481f16964af0d","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977778128707000","endTimeUnixNano":"1762977778128707000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa09d1c1f9a95187fcdcc65015","spanId":"df799c1db39137e3","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977778129497000","endTimeUnixNano":"1762977778129497000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa09d1cf4e4d4d64796ce50154","spanId":"84afcbaf35755ce6","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977778129821000","endTimeUnixNano":"1762977778129821000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa0dbbf45d3bc2ed9dc49cc56c","spanId":"023f1afc58c82d9d","parentSpanId":"378435a49f6b9ca9","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977779131439000","endTimeUnixNano":"1762977779131439000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa0e14d2c5f425764cd3d7f9eb","spanId":"ba5e2ada3af1f339","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977779220474000","endTimeUnixNano":"1762977779220474000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa0dbbf45d3bc2ed9dc49cc56c","spanId":"378435a49f6b9ca9","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977779131008000","endTimeUnixNano":"1762977779224323000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977779222024000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79aa0e18e927b4191cb102bec405","spanId":"e2db9e1c2e7cad5d","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977779224510000","endTimeUnixNano":"1762977779224510000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79a9ff929381b4390aa6d92f40f0","spanId":"aad2a8d1e9a004cc","parentSpanId":"2c9871061c0d3566","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977775511352000","endTimeUnixNano":"1762977776105890000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79a9ff929381b4390aa6d92f40f0","spanId":"2c9871061c0d3566","parentSpanId":"894e01b162d4e367","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977775509953000","endTimeUnixNano":"1762977776106396000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent(model: Kno) -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79aa097d00893d98fb717ab29ae7","spanId":"bf8a4e89dfaf1fdf","parentSpanId":"260769d5ebce7db0","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977778049695000","endTimeUnixNano":"1762977778122504000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79aa097d00893d98fb717ab29ae7","spanId":"260769d5ebce7db0","parentSpanId":"8e17a0267f21f049","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977778048314000","endTimeUnixNano":"1762977778122748000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent(model: KnownMod) -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79aa0dbbf45d3bc2ed9dc49cc56c","spanId":"e0679c7f78d55129","parentSpanId":"e085f2d110c91abc","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977779133013000","endTimeUnixNano":"1762977779220938000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79aa0dbbf45d3bc2ed9dc49cc56c","spanId":"e085f2d110c91abc","parentSpanId":"378435a49f6b9ca9","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977779132307000","endTimeUnixNano":"1762977779221120000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom pydantic_ai.models import KnownModelName\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent(model: KnownModelName) -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79aa11fe6e40e65e7eb15237d2a2","spanId":"92e5549223d46e2f","parentSpanId":"05b40dd82899cf51","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977780222521000","endTimeUnixNano":"1762977780222521000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa176450e1a4595db2fea49e40","spanId":"4f60faedbbf44592","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977781604978000","endTimeUnixNano":"1762977781604978000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa11fe6e40e65e7eb15237d2a2","spanId":"05b40dd82899cf51","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977780222136000","endTimeUnixNano":"1762977781610218000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977781607356000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79aa176ae199177c37731b5ff23e","spanId":"b640442877f63cdd","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977781610461000","endTimeUnixNano":"1762977781610461000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1871adec465529b8c9d33f40","spanId":"c66fc055f55d29b0","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977781873334000","endTimeUnixNano":"1762977781873334000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa18727e7fcb08b864d3b1732e","spanId":"381180a80e06e70f","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977781874027000","endTimeUnixNano":"1762977781874027000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa189d08faf60dcc409116030f","spanId":"2cf0030bfe3b81e0","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977781917324000","endTimeUnixNano":"1762977781917324000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa189d44274742509b96ada94f","spanId":"7941df2bc413ee5c","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977781917689000","endTimeUnixNano":"1762977781917689000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa18be25f47cec14641a5db069","spanId":"e2270ae3be336c96","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977781950124000","endTimeUnixNano":"1762977781950124000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa18be3e0fb2cd305a31dcde2f","spanId":"43563fa79323942c","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977781950457000","endTimeUnixNano":"1762977781950457000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa18e1f45c91dc5697aa0f16c2","spanId":"553a9fc35cc730b7","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977781985221000","endTimeUnixNano":"1762977781985221000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa18e1cca0b29fc4277059e693","spanId":"299ad3b05ee0cc82","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977781985495000","endTimeUnixNano":"1762977781985495000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa19010257b2348475a2710358","spanId":"b06ef02d441d8fce","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977782017754000","endTimeUnixNano":"1762977782017754000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa190284b582dddc14dd21d729","spanId":"08c24f5c57d741b1","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977782018038000","endTimeUnixNano":"1762977782018038000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa192270f3b2ef6c54860c39fa","spanId":"2c94b28880992e0d","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977782050912000","endTimeUnixNano":"1762977782050912000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1923cb1006adf8eb5c55c7d8","spanId":"218d39481e821ea8","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977782051225000","endTimeUnixNano":"1762977782051225000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa19448280013b76e45a765fa0","spanId":"6d146dd9875e82b3","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977782084373000","endTimeUnixNano":"1762977782084373000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa19440b6b44b3d17c005ef95a","spanId":"0dcebb33edae3b90","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977782084674000","endTimeUnixNano":"1762977782084674000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1967446f883c39c3f74edf08","spanId":"b3155b66c37302af","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977782119215000","endTimeUnixNano":"1762977782119215000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1967063f2707a8e79825830d","spanId":"1fead70fcf27ca1a","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977782119540000","endTimeUnixNano":"1762977782119540000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1987affe6600056fdd7ba525","spanId":"4df0ba6d3bf0f0ec","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977782151061000","endTimeUnixNano":"1762977782151061000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1987b15c071c7be730b7cddd","spanId":"dba1bd8b6097119a","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977782151296000","endTimeUnixNano":"1762977782151296000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa19a9f050ce8f4cf21eb44ce4","spanId":"a413a9ae82b4ff98","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977782185164000","endTimeUnixNano":"1762977782185164000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa19a9f8732d2f66b6fc78dda4","spanId":"96c131e93b67c486","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977782185404000","endTimeUnixNano":"1762977782185404000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1a407378f7f3ad3f23780cdf","spanId":"01da58bf4cfa56b6","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977782336380000","endTimeUnixNano":"1762977782336380000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1a40ffcb4ad99ed46d0a1d30","spanId":"1b1c305317be1a11","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977782336710000","endTimeUnixNano":"1762977782336710000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1b1f4b60f6bea56ce129d8cd","spanId":"a1cb77368c924cf6","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977782559282000","endTimeUnixNano":"1762977782559282000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1b1fa188afd2eef57944a7dd","spanId":"b387f502c6df00c2","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977782559733000","endTimeUnixNano":"1762977782559733000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1bb7c1bb79aa2e0810fa4e4f","spanId":"6cf7ecb907057476","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977782711901000","endTimeUnixNano":"1762977782711901000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1bb8a9a81403c7f8a650f8b0","spanId":"934c2648553f0f9e","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977782712472000","endTimeUnixNano":"1762977782712472000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1c6761a4e1a08c30a23e7f1a","spanId":"6cf9725760063dbd","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977782886973000","endTimeUnixNano":"1762977782886973000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1c68bbadd6fb0b1ba9aea6d7","spanId":"91b67a3f1265b25d","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977782888159000","endTimeUnixNano":"1762977782888159000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1cfc5649cb980f5eb14e9e82","spanId":"55883789db25eca7","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977783036393000","endTimeUnixNano":"1762977783036393000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1cfcaed24fc43d39fdb83732","spanId":"89b0b5100895a68e","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977783036971000","endTimeUnixNano":"1762977783036971000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1dde04427fe77f5be3073fc1","spanId":"affba44f963e97a9","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977783262290000","endTimeUnixNano":"1762977783262290000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1ddec2611af4fc63613ef96b","spanId":"e22cafa2af27d04e","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977783262824000","endTimeUnixNano":"1762977783262824000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1e695e9579e7601bd95b8c95","spanId":"af5c5b90569461eb","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977783401913000","endTimeUnixNano":"1762977783401913000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1e6aa32a8390bbd407381c9d","spanId":"3ec5c40bbf0da5b4","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977783402446000","endTimeUnixNano":"1762977783402446000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1eca9655febc61cf55144096","spanId":"b455600c4232baa4","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977783498147000","endTimeUnixNano":"1762977783498147000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1ecad3b74e692f30882c17d0","spanId":"4f5d59449879fb8c","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977783498515000","endTimeUnixNano":"1762977783498515000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1fa975dc31cef0b53a72c444","spanId":"830ceaa9f3a01421","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977783721332000","endTimeUnixNano":"1762977783721332000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa1fa92eb240c03767760b4089","spanId":"ae69bab79945bb91","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977783721754000","endTimeUnixNano":"1762977783721754000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa20395e4888c93aa3d2b5ee92","spanId":"a0567288eeebe70d","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977783865459000","endTimeUnixNano":"1762977783865459000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa20398d5c306433d69666ade9","spanId":"2e5b873e8778023a","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977783865880000","endTimeUnixNano":"1762977783865880000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa20dc17695238b3ad4302c22e","spanId":"69cb4d12e6910749","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977784028501000","endTimeUnixNano":"1762977784028501000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa20dcbc5a41cdc67bbbbd132b","spanId":"163dcc666ea202b4","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977784028738000","endTimeUnixNano":"1762977784028738000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa21f10a8ad4dfc74404eff97a","spanId":"9c691605430c7708","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977784305369000","endTimeUnixNano":"1762977784305369000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa21f1f88391520f68ae09fb72","spanId":"9556796d5cda702e","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977784305902000","endTimeUnixNano":"1762977784305902000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa2201b5b0a4d4ddf06720623f","spanId":"98b96a53f79f5b26","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977784321345000","endTimeUnixNano":"1762977784321345000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa2201d7e79fd971f42d237f1f","spanId":"017281faf59bb615","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977784321639000","endTimeUnixNano":"1762977784321639000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79aa11fe6e40e65e7eb15237d2a2","spanId":"5ef4e1c030e8b74a","parentSpanId":"16bce1229423de14","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977780223799000","endTimeUnixNano":"1762977781605817000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79aa11fe6e40e65e7eb15237d2a2","spanId":"16bce1229423de14","parentSpanId":"05b40dd82899cf51","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977780223230000","endTimeUnixNano":"1762977781606096000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom pydantic_ai.models import KnownModelName\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent(model: KnownModelName) -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=settings.ai_lsp_model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79aa23c2ed3f400291c3503e60c9","spanId":"5eacd1c1ff46e6f2","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977784770952000","endTimeUnixNano":"1762977784770952000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa23c42adfcbe39b757f6a8f21","spanId":"45703678b926bd67","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977784772123000","endTimeUnixNano":"1762977784772123000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa23cf13ff0d98d2c2aa7c721e","spanId":"bcc98281df55d8b5","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977784783628000","endTimeUnixNano":"1762977784783628000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa23d02d38c1c27a3692a2dabd","spanId":"bdd5d0122bc32f76","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977784784040000","endTimeUnixNano":"1762977784784040000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa26c48f86c698f2468f1702ae","spanId":"05da2ce4e00b5015","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977785540508000","endTimeUnixNano":"1762977785540508000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa26c5f7b4e87a826c5dcc1120","spanId":"3b88dd9e0d5c3d47","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977785541030000","endTimeUnixNano":"1762977785541030000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa2aae1a1bf5fa7f52e2632d8f","spanId":"7de95888a566837c","parentSpanId":"4384269c00294bfd","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977786542292000","endTimeUnixNano":"1762977786542292000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79aa476be0bc9459ebf04b5ed65d","spanId":"9f49b654e703b2ac","flags":256,"name":"code_actions","kind":1,"startTimeUnixNano":"1762977793899072000","endTimeUnixNano":"1762977793899364000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"315"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"code_actions"}},{"key":"logfire.msg","value":{"stringValue":"code_actions"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79aa5109f09772c99d99c9e74977","spanId":"9337535898bb85ff","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977796361124000","endTimeUnixNano":"1762977796361124000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa2aae1a1bf5fa7f52e2632d8f","spanId":"4384269c00294bfd","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977786542012000","endTimeUnixNano":"1762977796365118000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977796362821000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79aa510ddef2a09ad157871a41ba","spanId":"186519c85e4845d6","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977796365328000","endTimeUnixNano":"1762977796365328000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aa54f1396462ffe7c3e552992a","spanId":"0c1ec1f7c92e009e","parentSpanId":"7d45b92586a428f3","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977797362081000","endTimeUnixNano":"1762977797362081000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79aa2aae1a1bf5fa7f52e2632d8f","spanId":"5bd94624a91d2c35","parentSpanId":"d4f114f1ac03af32","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977786543353000","endTimeUnixNano":"1762977796361794000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79aa2aae1a1bf5fa7f52e2632d8f","spanId":"d4f114f1ac03af32","parentSpanId":"4384269c00294bfd","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977786542864000","endTimeUnixNano":"1762977796362003000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom pydantic_ai.models import KnownModelName\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent(model: KnownModelName) -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79aa54f1396462ffe7c3e552992a","spanId":"497b99d71b684a00","parentSpanId":"0fd5d79d52ed8d58","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977797362783000","endTimeUnixNano":"1762977819427027000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"943"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"503"}},{"key":"gen_ai.usage.details.cached_content_tokens","value":{"intValue":"846"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"3591"}},{"key":"gen_ai.usage.details.text_cache_tokens","value":{"intValue":"846"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"943"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom pydantic_ai.models import KnownModelName\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent(model: KnownModelName) -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_df80f3666c944d3cb9e31579af9426a5\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"message\": \"The instruction for `issue_snippet` to be 'ONLY the problematic token/value' might be overly restrictive for effectively communicating complex semantic issues. Many deep-seated problems in logic, architecture, or performance involve interactions across multiple tokens or even lines of code. Limiting the snippet to a single token can strip away crucial context, making the diagnostic less clear and harder for a human to act upon. Consider allowing a slightly broader `issue_snippet` when necessary to fully convey the problem.\", \"issue_snippet\": \"ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"'\", \"suggested_fixes\": [{\"title\": \"Allow more flexible `issue_snippet` for complex problems\", \"target_snippet\": \"ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"'\", \"replacement_snippet\": \"the most concise and relevant code snippet that captures the problem, which could be a single token or a small expression, e.g., 'command' if the variable name is the issue, or 'list.append(item)' if the entire method call is problematic.\"}], \"severity\": \"warning\"}, {\"suggested_fixes\": [{\"replacement_snippet\": \"Focus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\\nIf you are uncertain about an issue, please state your confidence level or highlight it as a potential concern requiring further investigation.\", \"target_snippet\": \"Focus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"title\": \"Add guidance for handling uncertainty and confidence\"}], \"message\": \"The `AI_LSP_SYSTEM_PROMPT` defines the AI agent's behavior but lacks explicit instructions on how to handle situations of uncertainty, low confidence, or potential false positives. For a diagnostic AI, guiding it to express confidence levels, ask clarifying questions, or indicate when an issue is speculative can significantly enhance the trustworthiness and utility of its reports for human users. Without such guidance, the AI might report issues with absolute certainty even when its confidence is low, potentially leading to wasted human effort or overlooked actual problems.\", \"severity\": \"info\", \"issue_snippet\": \"AI_LSP_SYSTEM_PROMPT\"}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79aa54f1396462ffe7c3e552992a","spanId":"0fd5d79d52ed8d58","parentSpanId":"7d45b92586a428f3","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977797362469000","endTimeUnixNano":"1762977819431455000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"'\", \"severity\": \"warning\", \"message\": \"The instruction for `issue_snippet` to be 'ONLY the problematic token/value' might be overly restrictive for effectively communicating complex semantic issues. Many deep-seated problems in logic, architecture, or performance involve interactions across multiple tokens or even lines of code. Limiting the snippet to a single token can strip away crucial context, making the diagnostic less clear and harder for a human to act upon. Consider allowing a slightly broader `issue_snippet` when necessary to fully convey the problem.\", \"suggested_fixes\": [{\"title\": \"Allow more flexible `issue_snippet` for complex problems\", \"target_snippet\": \"ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"'\", \"replacement_snippet\": \"the most concise and relevant code snippet that captures the problem, which could be a single token or a small expression, e.g., 'command' if the variable name is the issue, or 'list.append(item)' if the entire method call is problematic.\"}]}, {\"issue_snippet\": \"AI_LSP_SYSTEM_PROMPT\", \"severity\": \"info\", \"message\": \"The `AI_LSP_SYSTEM_PROMPT` defines the AI agent's behavior but lacks explicit instructions on how to handle situations of uncertainty, low confidence, or potential false positives. For a diagnostic AI, guiding it to express confidence levels, ask clarifying questions, or indicate when an issue is speculative can significantly enhance the trustworthiness and utility of its reports for human users. Without such guidance, the AI might report issues with absolute certainty even when its confidence is low, potentially leading to wasted human effort or overlooked actual problems.\", \"suggested_fixes\": [{\"title\": \"Add guidance for handling uncertainty and confidence\", \"target_snippet\": \"Focus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"replacement_snippet\": \"Focus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\\nIf you are uncertain about an issue, please state your confidence level or highlight it as a potential concern requiring further investigation.\"}]}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"943"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"503"}},{"key":"gen_ai.usage.details.cached_content_tokens","value":{"intValue":"846"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"3591"}},{"key":"gen_ai.usage.details.text_cache_tokens","value":{"intValue":"846"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"943"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\n# agent.py\\nfrom pydantic_ai.models import KnownModelName\\nfrom __future__ import annotations\\n\\nfrom typing import Any\\n\\nfrom .models import DiagnosticResult\\nfrom pydantic_ai import Agent\\n\\nAI_LSP_SYSTEM_PROMPT = \\\"\\\"\\\"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\\\"\\\"\\\"\\n\\n\\ndef create_diagnostic_agent(model: KnownModelName) -> Agent[DiagnosticResult, Any]:\\n    \\\"\\\"\\\"Factory for the AI diagnostic agent.\\\"\\\"\\\"\\n    return Agent(\\n        model=model,\\n        output_type=DiagnosticResult,\\n        system_prompt=AI_LSP_SYSTEM_PROMPT,\\n    )\\n\\n```\\n\\nFile: agent.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_df80f3666c944d3cb9e31579af9426a5\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"message\": \"The instruction for `issue_snippet` to be 'ONLY the problematic token/value' might be overly restrictive for effectively communicating complex semantic issues. Many deep-seated problems in logic, architecture, or performance involve interactions across multiple tokens or even lines of code. Limiting the snippet to a single token can strip away crucial context, making the diagnostic less clear and harder for a human to act upon. Consider allowing a slightly broader `issue_snippet` when necessary to fully convey the problem.\", \"issue_snippet\": \"ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"'\", \"suggested_fixes\": [{\"title\": \"Allow more flexible `issue_snippet` for complex problems\", \"target_snippet\": \"ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"'\", \"replacement_snippet\": \"the most concise and relevant code snippet that captures the problem, which could be a single token or a small expression, e.g., 'command' if the variable name is the issue, or 'list.append(item)' if the entire method call is problematic.\"}], \"severity\": \"warning\"}, {\"suggested_fixes\": [{\"replacement_snippet\": \"Focus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\\\nIf you are uncertain about an issue, please state your confidence level or highlight it as a potential concern requiring further investigation.\", \"target_snippet\": \"Focus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"title\": \"Add guidance for handling uncertainty and confidence\"}], \"message\": \"The `AI_LSP_SYSTEM_PROMPT` defines the AI agent's behavior but lacks explicit instructions on how to handle situations of uncertainty, low confidence, or potential false positives. For a diagnostic AI, guiding it to express confidence levels, ask clarifying questions, or indicate when an issue is speculative can significantly enhance the trustworthiness and utility of its reports for human users. Without such guidance, the AI might report issues with absolute certainty even when its confidence is low, potentially leading to wasted human effort or overlooked actual problems.\", \"severity\": \"info\", \"issue_snippet\": \"AI_LSP_SYSTEM_PROMPT\"}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_df80f3666c944d3cb9e31579af9426a5\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79aa54f1396462ffe7c3e552992a","spanId":"066d6b448aaf6ec4","parentSpanId":"7d45b92586a428f3","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762977819434474000","endTimeUnixNano":"1762977819434474000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 2 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"203"}},{"key":"len(result.output.issues)","value":{"intValue":"2"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79aa54f1396462ffe7c3e552992a","spanId":"f8dfb2e595e45ab0","parentSpanId":"7d45b92586a428f3","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977819434917000","endTimeUnixNano":"1762977819434917000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 1 occurrences of 'ONLY the problematic...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"1"}},{"key":"snippet[:20]","value":{"stringValue":"ONLY the problematic"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79aa54f1396462ffe7c3e552992a","spanId":"9a3d7c199e17c5a8","parentSpanId":"7d45b92586a428f3","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762977819435524000","endTimeUnixNano":"1762977819435524000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 2 occurrences of 'AI_LSP_SYSTEM_PROMPT...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"151"}},{"key":"len(positions)","value":{"intValue":"2"}},{"key":"snippet[:20]","value":{"stringValue":"AI_LSP_SYSTEM_PROMPT"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79aa54f1396462ffe7c3e552992a","spanId":"7d45b92586a428f3","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977797361883000","endTimeUnixNano":"1762977819435754000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79aaab2ca1d7a40382bac7adcbce","spanId":"2cb72c437d686050","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762977819436088000","endTimeUnixNano":"1762977819436088000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 2 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"181"}},{"key":"len(diagnostics)","value":{"intValue":"2"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/agent.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79aaad203d2efab09c52f3be0344","spanId":"7eaea7ded96badd8","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977819936842000","endTimeUnixNano":"1762977819936842000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab10af62645ce3dad6b5a578f","spanId":"7233cd2dfd75f808","parentSpanId":"d58767f9efab6a72","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977820939992000","endTimeUnixNano":"1762977820939992000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab526564ea2a02235013640f9","spanId":"151285d667cb1c30","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977821990912000","endTimeUnixNano":"1762977821990912000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab10af62645ce3dad6b5a578f","spanId":"d58767f9efab6a72","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977820938740000","endTimeUnixNano":"1762977822001846000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977821998794000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79aab5321715c376b2ad05acd036","spanId":"5b69500139127f54","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977822002291000","endTimeUnixNano":"1762977822002291000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab5cad3939dee7384be1df6d7","spanId":"7b9865142a7afa28","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977822154146000","endTimeUnixNano":"1762977822154146000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab5cad6c8e1e120930108c1bd","spanId":"bee4059cdbe64a25","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977822154568000","endTimeUnixNano":"1762977822154568000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab68abab0040602443bd26d7b","spanId":"c44c5c90a99bb529","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977822346111000","endTimeUnixNano":"1762977822346111000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab68ab472f8aa7ca9f6641d0f","spanId":"85814e4880606094","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977822346508000","endTimeUnixNano":"1762977822346508000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab7293f80ee16a13a4d0a2ea6","spanId":"67d5ce25d32ac675","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977822505462000","endTimeUnixNano":"1762977822505462000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab729cc01a90ef3c5793bef33","spanId":"4b5b0f885a252a66","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977822505945000","endTimeUnixNano":"1762977822505945000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab7c75f3bab029a38ec4fe36e","spanId":"0de368e3541ec9e0","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977822663814000","endTimeUnixNano":"1762977822663814000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab7c880d5d34e0263689757f4","spanId":"dd0803fdfb05bb44","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977822664188000","endTimeUnixNano":"1762977822664188000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab87b9e51f1bfe68526d80347","spanId":"d34ac094d94577ec","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977822842998000","endTimeUnixNano":"1762977822842998000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab87cb464dc931e5240cf4ca5","spanId":"5c8b0c0a45f89ab3","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977822844360000","endTimeUnixNano":"1762977822844360000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab9f99fe4957478912592c2a9","spanId":"c85533a16fdfb786","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977823225152000","endTimeUnixNano":"1762977823225152000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aab9f9d0994199e98d059acd76","spanId":"b2d3e0ab4fbbd1f3","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977823225760000","endTimeUnixNano":"1762977823225760000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aaba8c67599c1bd4f7253cfd9c","spanId":"7fbc4f83c4b49200","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977823372523000","endTimeUnixNano":"1762977823372523000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aaba8dd25433e69c8c9c88820b","spanId":"4ef1bd881002c9c5","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977823373074000","endTimeUnixNano":"1762977823373074000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aabbc1bce611c2383106d0b723","spanId":"4b860691785f845f","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977823681180000","endTimeUnixNano":"1762977823681180000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aabbc1cdaebc7461c76715db7e","spanId":"d6d5c93044286d8a","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977823681644000","endTimeUnixNano":"1762977823681644000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aabc6b1b4209848c9131daa8c1","spanId":"bcdac22395335ec2","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977823851956000","endTimeUnixNano":"1762977823851956000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aabc6caf0801753d1e3c3d9da7","spanId":"838fbb01171325b5","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977823852272000","endTimeUnixNano":"1762977823852272000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aabe99b5826e04286b033709cb","spanId":"2a5a4a6aa4024da4","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977824409494000","endTimeUnixNano":"1762977824409494000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aabe9ae39d29d09d97ee90a59e","spanId":"be9284a0534e5450","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977824410051000","endTimeUnixNano":"1762977824410051000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aabeb1c3331900e91c4de560ff","spanId":"9cc2312773cc26e0","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977824433854000","endTimeUnixNano":"1762977824433854000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aabeb28f96786989445f2ba6de","spanId":"c1f5bb32c268c94b","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977824434197000","endTimeUnixNano":"1762977824434197000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79aab10af62645ce3dad6b5a578f","spanId":"88218511bcd1526b","parentSpanId":"61117ea8656c4063","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977820942852000","endTimeUnixNano":"1762977821992584000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79aab10af62645ce3dad6b5a578f","spanId":"61117ea8656c4063","parentSpanId":"d58767f9efab6a72","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977820941707000","endTimeUnixNano":"1762977821993060000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom importlib.metadata import version\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\nfrom ai_lsp.models import DiagnosticResult, Settings\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = Agent(\\n            model=settings.ai_lsp_model,\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79aac29b1cc3994d02e05d1b56c2","spanId":"2ccd63b86216dc5d","parentSpanId":"7623100c5e543bc6","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977825436793000","endTimeUnixNano":"1762977825436793000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79aadc9e97d1d5c8a6cbdf150af9","spanId":"f06bc82edddc8237","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977832094430000","endTimeUnixNano":"1762977832094430000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aac29b1cc3994d02e05d1b56c2","spanId":"7623100c5e543bc6","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977825435706000","endTimeUnixNano":"1762977832104054000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977832101418000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    \"target_snippet\": fix.target_snippet,\n    ^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79aadca80727261890b4dfa65e86","spanId":"2c44d371f4e2ca4b","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977832104290000","endTimeUnixNano":"1762977832104290000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aaddc34f9bac44dce61f186025","spanId":"bc5216eb2ac54f8a","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977832387934000","endTimeUnixNano":"1762977832387934000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aaddc4caea3a2f34eb9f044349","spanId":"b1463e5cf4efec02","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977832388408000","endTimeUnixNano":"1762977832388408000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aae1ade5b9171b8d738d51a231","spanId":"d4436873ca899231","parentSpanId":"92e846933a5350f8","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977833389813000","endTimeUnixNano":"1762977833389813000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aae228aa07c16c2b9397894caf","spanId":"7180483ee9ef78a5","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977833512969000","endTimeUnixNano":"1762977833512969000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aae1ade5b9171b8d738d51a231","spanId":"92e846933a5350f8","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977833389532000","endTimeUnixNano":"1762977833519122000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"7f429e5a4babd9eb9e62b565de96896c8aacd7b82d7c18cfaaaed36bde14a75b"}}],"events":[{"timeUnixNano":"1762977833517095000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    match file_path.suffix:\n                         ^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79aae22fb648d732c4b89a7411a6","spanId":"f13344bd7878ebf4","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977833519323000","endTimeUnixNano":"1762977833519323000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aae236052e95bedb1f581f12bc","spanId":"c4c7eee471b71634","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977833526110000","endTimeUnixNano":"1762977833526110000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aae236e56e07ef7721c6c3ce20","spanId":"17e68bb26566372b","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977833526390000","endTimeUnixNano":"1762977833526390000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aae620fbb27311cfa8fe58fa49","spanId":"354d4d89bfc4c818","parentSpanId":"5a8151a766f8c60d","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977834529034000","endTimeUnixNano":"1762977834529034000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79aac29b1cc3994d02e05d1b56c2","spanId":"ef73ce6eb61ce990","parentSpanId":"1eb87467c0abd46a","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977825440218000","endTimeUnixNano":"1762977832095880000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79aac29b1cc3994d02e05d1b56c2","spanId":"1eb87467c0abd46a","parentSpanId":"7623100c5e543bc6","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977825438838000","endTimeUnixNano":"1762977832096211000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom importlib.metadata import version\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\nfrom ai_lsp.models import DiagnosticResult, Settings\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = create_diagnostic_agent(\\n            model=settings.ai_lsp_model,\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79aae1ade5b9171b8d738d51a231","spanId":"8683c7681d45e79c","parentSpanId":"d787a4d529679a7d","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977833390756000","endTimeUnixNano":"1762977833513604000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79aae1ade5b9171b8d738d51a231","spanId":"d787a4d529679a7d","parentSpanId":"92e846933a5350f8","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977833390339000","endTimeUnixNano":"1762977833513889000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom importlib.metadata import version\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\nfrom ai_lsp.models import DiagnosticResult, Settings\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = create_diagnostic_agent(\\n            model=settings.ai_lsp_model)\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79aaee9550ab7ef9a015fb7befb4","spanId":"b87e8607b274b3e4","flags":256,"name":"code_actions","kind":1,"startTimeUnixNano":"1762977836693656000","endTimeUnixNano":"1762977836693984000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"code_actions"}},{"key":"code.lineno","value":{"intValue":"315"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"code_actions"}},{"key":"logfire.msg","value":{"stringValue":"code_actions"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79aaf7af70efed92e77d0ca03eca","spanId":"1d785553ee405ff5","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977839023707000","endTimeUnixNano":"1762977839023707000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aae620fbb27311cfa8fe58fa49","spanId":"5a8151a766f8c60d","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977834527973000","endTimeUnixNano":"1762977839030692000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"2d79df47d9d644193c5359fedc726949ddf10cd8fdd65980d2983e25db6d28aa"}}],"events":[{"timeUnixNano":"1762977839028535000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    match file_path.suffix:\n                         ^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79aaf7b6bcb5c4f542662e8847a9","spanId":"19d1206088e29e1d","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977839030906000","endTimeUnixNano":"1762977839030906000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79aae620fbb27311cfa8fe58fa49","spanId":"96cea68e752b7b5c","parentSpanId":"0d9dd68be9d181d4","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977834533300000","endTimeUnixNano":"1762977839024240000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79aae620fbb27311cfa8fe58fa49","spanId":"0d9dd68be9d181d4","parentSpanId":"5a8151a766f8c60d","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977834531346000","endTimeUnixNano":"1762977839024445000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nimport asyncio\\nimport os\\nfrom importlib.metadata import version\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\nfrom ai_lsp.models import DiagnosticResult, Settings\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = create_diagnostic_agent(\\n            model=settings.ai_lsp_model\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79aafb99ab9a0b9fcc7a667c6732","spanId":"f10b9c0a0556a93d","parentSpanId":"c3abb5a0dd699360","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977840026427000","endTimeUnixNano":"1762977840026427000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79aafd6d00dabd523617798ddd3d","spanId":"6f111f07a267fec7","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977840493529000","endTimeUnixNano":"1762977840493529000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79aafb99ab9a0b9fcc7a667c6732","spanId":"c3abb5a0dd699360","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977840025602000","endTimeUnixNano":"1762977840500202000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"2d79df47d9d644193c5359fedc726949ddf10cd8fdd65980d2983e25db6d28aa"}}],"events":[{"timeUnixNano":"1762977840497740000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    def _detect_language(self, file_path: Path) -> str:\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79aafd7477097cf1896ab9f5d2d3","spanId":"d38f78f710c3f9e9","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977840500431000","endTimeUnixNano":"1762977840500431000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}},{"traceId":"019a79ab0157c90c6debaa0150d61e8b","spanId":"7a4792f2b87cac64","parentSpanId":"780ecc5eecb59df0","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977841495863000","endTimeUnixNano":"1762977841495863000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79aafb99ab9a0b9fcc7a667c6732","spanId":"0b14603560888628","parentSpanId":"c69b87d77823ed8f","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977840029091000","endTimeUnixNano":"1762977840494186000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79aafb99ab9a0b9fcc7a667c6732","spanId":"c69b87d77823ed8f","parentSpanId":"c3abb5a0dd699360","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977840027926000","endTimeUnixNano":"1762977840494395000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom ai_lsp.agent import create_diagnostic_agent\\nimport asyncio\\nimport os\\nfrom importlib.metadata import version\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\nfrom ai_lsp.models import DiagnosticResult, Settings\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = create_diagnostic_agent(\\n            model=settings.ai_lsp_model\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79ab3236a17d2746f3f2acb1ffc3","spanId":"de94e31fbe829a73","flags":256,"name":"Document changed: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977854006638000","endTimeUnixNano":"1762977854006638000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Document changed: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document changed: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_change"}},{"key":"code.lineno","value":{"intValue":"305"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79ab0157c90c6debaa0150d61e8b","spanId":"780ecc5eecb59df0","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977841495397000","endTimeUnixNano":"1762977854014058000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"8a49ee111e0b409577b0f2de9e71cfedb29ea65de096b7ff515ca3b157bfe513"}}],"events":[{"timeUnixNano":"1762977854011678000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    def _detect_language(self, file_path: Path) -> str:\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79ab323e0f3fac2e7c749834a125","spanId":"176e33db1deb26d8","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977854014291000","endTimeUnixNano":"1762977854014291000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79ab0157c90c6debaa0150d61e8b","spanId":"d269913c6517d2de","parentSpanId":"377f30884738f022","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977841497628000","endTimeUnixNano":"1762977854007108000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79ab0157c90c6debaa0150d61e8b","spanId":"377f30884738f022","parentSpanId":"780ecc5eecb59df0","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977841496767000","endTimeUnixNano":"1762977854007365000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom ai_lsp.agent import create_diagnostic_agent\\nimport asyncio\\nimport os\\nfrom importlib.metadata import version\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\nfrom ai_lsp.models import DiagnosticResult, Settings\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n\\n        self.agent: Agent[DiagnosticResult, Any] = create_diagnostic_agent(\\n            model=settings.ai_lsp_model\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79ab361f0b4b0f6da3ca8f9f398b","spanId":"b5812930f3af6c0e","parentSpanId":"2005b16b752ffa55","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762977855007900000","endTimeUnixNano":"1762977855007900000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"188"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"47b0e83af18e4e4e8d19d3df5af62e59"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"15750"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79ac1ebfca53f90a3849aa224690","spanId":"463d004ff0457c35","flags":256,"name":"Document saved: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762977914559410000","endTimeUnixNano":"1762977914559410000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document saved: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document saved: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_save"}},{"key":"code.lineno","value":{"intValue":"298"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79ab361f0b4b0f6da3ca8f9f398b","spanId":"2005b16b752ffa55","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762977855007674000","endTimeUnixNano":"1762977914564520000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"187"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.level_num","value":{"intValue":"17"}},{"key":"logfire.exception.fingerprint","value":{"stringValue":"8a49ee111e0b409577b0f2de9e71cfedb29ea65de096b7ff515ca3b157bfe513"}}],"events":[{"timeUnixNano":"1762977914562832000","name":"exception","attributes":[{"key":"exception.type","value":{"stringValue":"asyncio.exceptions.CancelledError"}},{"key":"exception.message","value":{"stringValue":""}},{"key":"exception.stacktrace","value":{"stringValue":"Traceback (most recent call last):\n  File \"/Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py\", line 202, in analyze_document\n    match file_path.suffix:\n                         ^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 562, in run\n    async for _ in agent_run:\n        pass\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 2178, in __anext__\n    next_node = await self._graph_run.__anext__()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 809, in __anext__\n    return await self.next(self._next_node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_graph/graph.py\", line 782, in next\n    self._next_node = await node.run(ctx)\n                      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 299, in run\n    return await self._make_request(ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py\", line 359, in _make_request\n    model_response = await ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/instrumented.py\", line 215, in request\n    response = await super().request(messages, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/wrapper.py\", line 30, in request\n    return await self.wrapped.request(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 181, in request\n    response = await self._generate_content(messages, False, model_settings, model_request_parameters)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/pydantic_ai/models/google.py\", line 304, in _generate_content\n    return await func(model=self._model_name, contents=contents, config=config)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 7428, in generate_content\n    response = await self._generate_content(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        model=model, contents=contents, config=parsed_config\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/models.py\", line 6381, in _generate_content\n    response = await self._api_client.async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        'post', path, request_dict, http_options\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1211, in async_request\n    result = await self._async_request(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n        http_request=http_request, http_options=http_options, stream=False\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1156, in _async_request\n    return await self._async_retry(  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self._async_request_once, http_request, stream\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ~~~~~~~~~~~~~~~~~^^\n  File \"/Users/benomahony/Library/Application Support/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/google/genai/_api_client.py\", line 1089, in _async_request_once\n    response = await session.request(\n               ^^^^^^^^^^^^^^^^^^^^^^\n    ...<6 lines>...\n    )\n    ^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 770, in _request\n    resp = await handler(req)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client.py\", line 748, in _connect_and_send_request\n    await resp.start(conn)\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/client_reqrep.py\", line 532, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/benomahony/Code/open_source/ai-lsp/.venv/lib/python3.13/site-packages/aiohttp/streams.py\", line 672, in read\n    await self._waiter\nasyncio.exceptions.CancelledError\n"}},{"key":"exception.escaped","value":{"stringValue":"True"}}]}],"status":{"message":"CancelledError: ","code":2}},{"traceId":"019a79ac1ec4237546464a603e199b2e","spanId":"c4a74651654f9bff","flags":256,"name":"Analysis cancelled for {uri}","kind":1,"startTimeUnixNano":"1762977914564728000","endTimeUnixNano":"1762977914564728000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Analysis cancelled for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Analysis cancelled for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.debounced_analyze"}},{"key":"code.lineno","value":{"intValue":"169"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]},{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79ab361f0b4b0f6da3ca8f9f398b","spanId":"503d76bf01d2920c","parentSpanId":"86f3fe5246d76b8e","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762977855008809000","endTimeUnixNano":"1762977914559972000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"model_request_parameters\": {\"type\": \"object\"}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}}],"status":{}},{"traceId":"019a79ab361f0b4b0f6da3ca8f9f398b","spanId":"86f3fe5246d76b8e","parentSpanId":"2005b16b752ffa55","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762977855008452000","endTimeUnixNano":"1762977914560204000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom ai_lsp.agent import create_diagnostic_agent\\nimport asyncio\\nimport os\\nfrom importlib.metadata import version\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\nfrom ai_lsp.models import DiagnosticResult, Settings\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n        self.agent: Agent[DiagnosticResult, Any] = create_diagnostic_agent(\\n            model=settings.ai_lsp_model\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"0987554b2c94454388250d29f8d98abd"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"33711"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79ae68ca64544ef592c00fea73cc","spanId":"74218fbb2c1e1eaa","flags":256,"name":"Initializing AI Language Server","kind":1,"startTimeUnixNano":"1762978064586450000","endTimeUnixNano":"1762978064586450000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Initializing AI Language Server"}},{"key":"logfire.msg","value":{"stringValue":"Initializing AI Language Server"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"52"}}],"status":{}},{"traceId":"019a79ae6abbcba81882e47078c2071c","spanId":"50a2e17e118df3a5","flags":256,"name":"AI agent initialized successfully","kind":1,"startTimeUnixNano":"1762978065083607000","endTimeUnixNano":"1762978065083607000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI agent initialized successfully"}},{"key":"logfire.msg","value":{"stringValue":"AI agent initialized successfully"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.__init__"}},{"key":"code.lineno","value":{"intValue":"58"}}],"status":{}},{"traceId":"019a79ae6add060b38ec192467f9a888","spanId":"13e224fd4ac5902a","flags":256,"name":"Document opened: {params.text_document.uri}","kind":1,"startTimeUnixNano":"1762978065117308000","endTimeUnixNano":"1762978065117308000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Document opened: {params.text_document.uri}"}},{"key":"logfire.msg","value":{"stringValue":"Document opened: file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"did_open"}},{"key":"code.lineno","value":{"intValue":"234"}},{"key":"params.text_document.uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"params.text_document.uri\":{}}}"}}],"status":{}},{"traceId":"019a79ae6ec92b27aa2061249d5709d4","spanId":"5366dd6a5e4cd16a","parentSpanId":"82f717034d300651","flags":256,"name":"Starting AI analysis for {uri}","kind":1,"startTimeUnixNano":"1762978066122816000","endTimeUnixNano":"1762978066122816000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"Starting AI analysis for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Starting AI analysis for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"131"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}}],"status":{}}]}]}]}
{"resourceSpans":[{"resource":{"attributes":[{"key":"service.instance.id","value":{"stringValue":"0987554b2c94454388250d29f8d98abd"}},{"key":"telemetry.sdk.language","value":{"stringValue":"python"}},{"key":"telemetry.sdk.name","value":{"stringValue":"opentelemetry"}},{"key":"telemetry.sdk.version","value":{"stringValue":"1.38.0"}},{"key":"service.name","value":{"stringValue":"ai-lsp"}},{"key":"process.pid","value":{"intValue":"33711"}},{"key":"process.runtime.name","value":{"stringValue":"cpython"}},{"key":"process.runtime.version","value":{"stringValue":"3.13.0"}},{"key":"process.runtime.description","value":{"stringValue":"3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]"}},{"key":"service.version","value":{"stringValue":"3dc7b113904374f8e28a0dedc730aa08936d5814"}}]},"scopeSpans":[{"scope":{"name":"pydantic-ai","version":"0.4.11"},"spans":[{"traceId":"019a79ae6ec92b27aa2061249d5709d4","spanId":"289a3ae8d2f7c270","parentSpanId":"fbf6e29aa3946c3d","flags":256,"name":"chat gemini-2.5-flash","kind":1,"startTimeUnixNano":"1762978066127006000","endTimeUnixNano":"1762978093802083000","attributes":[{"key":"gen_ai.operation.name","value":{"stringValue":"chat"}},{"key":"gen_ai.system","value":{"stringValue":"google-gla"}},{"key":"gen_ai.request.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"server.address","value":{"stringValue":"generativelanguage.googleapis.com"}},{"key":"model_request_parameters","value":{"stringValue":"{\"function_tools\": [], \"output_mode\": \"tool\", \"output_object\": null, \"output_tools\": [{\"name\": \"final_result\", \"parameters_json_schema\": {\"properties\": {\"issues\": {\"items\": {\"properties\": {\"issue_snippet\": {\"type\": \"string\"}, \"severity\": {\"enum\": [\"error\", \"warning\", \"info\", \"hint\"], \"type\": \"string\"}, \"message\": {\"type\": \"string\"}, \"suggested_fixes\": {\"items\": {\"properties\": {\"title\": {\"type\": \"string\"}, \"target_snippet\": {\"type\": \"string\"}, \"replacement_snippet\": {\"type\": \"string\"}}, \"required\": [\"title\", \"target_snippet\", \"replacement_snippet\"], \"type\": \"object\"}, \"type\": \"array\", \"nullable\": true}}, \"required\": [\"issue_snippet\", \"severity\", \"message\"], \"type\": \"object\"}, \"type\": \"array\"}}, \"required\": [\"issues\"], \"type\": \"object\"}, \"description\": \"The final response which ends this conversation\", \"outer_typed_dict_key\": null, \"strict\": true, \"kind\": \"output\"}], \"allow_text_output\": false}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"logfire.msg","value":{"stringValue":"chat gemini-2.5-flash"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4377"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"479"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"4676"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4377"}},{"key":"gen_ai.response.model","value":{"stringValue":"gemini-2.5-flash"}},{"key":"events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom ai_lsp.agent import create_diagnostic_agent\\nimport asyncio\\nimport os\\nfrom importlib.metadata import version\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\nfrom ai_lsp.models import DiagnosticResult, Settings\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n        self.agent: Agent[DiagnosticResult, Any] = create_diagnostic_agent(\\n            model=settings.ai_lsp_model\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.system\": \"google-gla\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_8898e2a43d8247c0b62817881e5b2d27\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"issue_snippet\": \"find_snippet_in_text\", \"severity\": \"error\", \"suggested_fixes\": null, \"message\": \"The `find_snippet_in_text` method is used to locate AI-generated snippets (e.g., `issue_snippet` for diagnostics, `target_snippet` for fixes) within the document. This method performs a simple string search and returns the first match. If the AI provides a snippet that is not unique or is a common substring, the diagnostic or quick-fix will be applied to the wrong location. This can lead to mis-highlighted issues and potentially incorrect application of fixes. A more robust approach is required, such as the AI providing exact line/column ranges in its output, or a context-aware matching algorithm that considers the surrounding code.\"}, {\"message\": \"Accessing `server.lsp.diagnostics` via `getattr` relies on an internal, non-public attribute of the `pygls` LanguageServer. This is an architectural anti-pattern that makes the code fragile and prone to breaking with future updates to the `pygls` library. Diagnostics should be managed through the public API or within the `AILanguageServer` class's own state (e.g., a dedicated dictionary `self._diagnostics_cache`) and only published via `server.publish_diagnostics`.\", \"issue_snippet\": \"getattr\", \"suggested_fixes\": null, \"severity\": \"warning\"}, {\"issue_snippet\": \"file_path.name\", \"message\": \"The `file_path.name` variable is directly interpolated into the LLM prompt. While less critical than code content, unvalidated external input in a prompt can sometimes lead to prompt injection or unintended LLM behavior if the filename contains special characters or patterns recognized by the LLM as instructions or delimiters. It's a good security practice to sanitize or escape all user-controlled inputs provided to an LLM, even for seemingly innocuous fields like filenames.\", \"suggested_fixes\": [{\"replacement_snippet\": \"`{file_path.name}`\", \"target_snippet\": \"{file_path.name}\", \"title\": \"Escape filename in prompt to prevent misinterpretation\"}], \"severity\": \"info\"}]}}}]}, \"gen_ai.system\": \"google-gla\", \"event.name\": \"gen_ai.choice\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"events\": {\"type\": \"array\"}, \"model_request_parameters\": {\"type\": \"object\"}}}"}}],"status":{}},{"traceId":"019a79ae6ec92b27aa2061249d5709d4","spanId":"fbf6e29aa3946c3d","parentSpanId":"82f717034d300651","flags":256,"name":"agent run","kind":1,"startTimeUnixNano":"1762978066125965000","endTimeUnixNano":"1762978093811554000","attributes":[{"key":"model_name","value":{"stringValue":"gemini-2.5-flash"}},{"key":"agent_name","value":{"stringValue":"agent"}},{"key":"logfire.msg","value":{"stringValue":"agent run"}},{"key":"logfire.span_type","value":{"stringValue":"span"}},{"key":"final_result","value":{"stringValue":"{\"issues\": [{\"issue_snippet\": \"find_snippet_in_text\", \"severity\": \"error\", \"message\": \"The `find_snippet_in_text` method is used to locate AI-generated snippets (e.g., `issue_snippet` for diagnostics, `target_snippet` for fixes) within the document. This method performs a simple string search and returns the first match. If the AI provides a snippet that is not unique or is a common substring, the diagnostic or quick-fix will be applied to the wrong location. This can lead to mis-highlighted issues and potentially incorrect application of fixes. A more robust approach is required, such as the AI providing exact line/column ranges in its output, or a context-aware matching algorithm that considers the surrounding code.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"getattr\", \"severity\": \"warning\", \"message\": \"Accessing `server.lsp.diagnostics` via `getattr` relies on an internal, non-public attribute of the `pygls` LanguageServer. This is an architectural anti-pattern that makes the code fragile and prone to breaking with future updates to the `pygls` library. Diagnostics should be managed through the public API or within the `AILanguageServer` class's own state (e.g., a dedicated dictionary `self._diagnostics_cache`) and only published via `server.publish_diagnostics`.\", \"suggested_fixes\": null}, {\"issue_snippet\": \"file_path.name\", \"severity\": \"info\", \"message\": \"The `file_path.name` variable is directly interpolated into the LLM prompt. While less critical than code content, unvalidated external input in a prompt can sometimes lead to prompt injection or unintended LLM behavior if the filename contains special characters or patterns recognized by the LLM as instructions or delimiters. It's a good security practice to sanitize or escape all user-controlled inputs provided to an LLM, even for seemingly innocuous fields like filenames.\", \"suggested_fixes\": [{\"title\": \"Escape filename in prompt to prevent misinterpretation\", \"target_snippet\": \"{file_path.name}\", \"replacement_snippet\": \"`{file_path.name}`\"}]}]}"}},{"key":"gen_ai.usage.input_tokens","value":{"intValue":"4377"}},{"key":"gen_ai.usage.output_tokens","value":{"intValue":"479"}},{"key":"gen_ai.usage.details.thoughts_tokens","value":{"intValue":"4676"}},{"key":"gen_ai.usage.details.text_prompt_tokens","value":{"intValue":"4377"}},{"key":"all_messages_events","value":{"stringValue":"[{\"role\": \"system\", \"content\": \"You are an AI code analyzer that provides semantic insights that traditional LSPs cannot detect.\\n\\nONLY flag issues that require deep semantic understanding:\\n- Logic errors in algorithms or business logic\\n- Subtle race conditions or concurrency issues  \\n- Architectural problems and design pattern violations\\n- Security vulnerabilities requiring context understanding\\n- Performance anti-patterns that need semantic analysis\\n- Complex data flow issues\\n- Domain-specific best practice violations\\n- Accessibility issues requiring UX understanding\\n\\nDO NOT flag issues that normal LSPs handle:\\n- Syntax errors\\n- Basic type errors  \\n- Undefined variables\\n- Import issues\\n- Basic formatting problems\\n- Simple linting rules\\n\\nFor each issue, provide:\\n- The exact issue_snippet that has the problem (ONLY the problematic token/value, e.g. just \\\"python\\\" not 'command = \\\"python\\\"')\\n- Clear explanation of WHY this needs human attention in the message\\n- Use severity: \\\"info\\\" for suggestions, \\\"warning\\\" for concerns, \\\"error\\\" for serious logic issues\\n- When possible, provide suggested_fixes with:\\n  - target_snippet: the exact code to replace (same as issue_snippet usually)\\n  - replacement_snippet: the replacement code\\n  - title: clear description of the fix\\n\\nFocus on insights that require understanding code intent and context. Keep issue_snippet to the absolute minimum - just the problematic token.\\n\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.system.message\"}, {\"content\": \"Analyze this python code for issues:\\n\\n```python\\nfrom ai_lsp.agent import create_diagnostic_agent\\nimport asyncio\\nimport os\\nfrom importlib.metadata import version\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nimport logfire\\n\\n# pyrefly: ignore [missing-import]\\nfrom lsprotocol.types import (\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    TEXT_DOCUMENT_DID_CHANGE,\\n    TEXT_DOCUMENT_DID_OPEN,\\n    TEXT_DOCUMENT_DID_SAVE,\\n    CodeAction,\\n    CodeActionKind,\\n    CodeActionOptions,\\n    CodeActionParams,\\n    Command,\\n    Diagnostic,\\n    DiagnosticSeverity,\\n    DidChangeTextDocumentParams,\\n    DidOpenTextDocumentParams,\\n    DidSaveTextDocumentParams,\\n    Position,\\n    Range,\\n    TextEdit,\\n    WorkspaceEdit,\\n)\\nfrom pydantic_ai import Agent\\n\\n# pyrefly: ignore [missing-import]\\nfrom pygls.server import LanguageServer\\n\\nfrom ai_lsp.models import DiagnosticResult, Settings\\n\\nsettings = Settings()\\n\\n\\nos.environ[\\\"OTEL_EXPORTER_OTLP_ENDPOINT\\\"] = settings.otel_exporter_otlp_endpoint\\n\\nif settings.configure_logfire:\\n    logfire.configure(send_to_logfire=False, service_name=\\\"ai-lsp\\\")  # pyright: ignore[reportUnusedCallResult]\\n    logfire.instrument_pydantic_ai()\\n    logfire.instrument_httpx(capture_all=True)\\n\\n\\nclass AILanguageServer(LanguageServer):\\n    def __init__(self):\\n        super().__init__(\\\"ai-lsp\\\", version(\\\"ai-lsp\\\"))\\n        logfire.info(\\\"Initializing AI Language Server\\\")\\n        self._pending_tasks: dict[str, asyncio.Task[Any]] = {}\\n        self._analysis_locks: dict[str, asyncio.Lock] = {}\\n        self.agent: Agent[DiagnosticResult, Any] = create_diagnostic_agent(\\n            model=settings.ai_lsp_model\\n        )\\n        logfire.info(\\\"AI agent initialized successfully\\\")\\n\\n    def find_snippet_in_text(\\n        self, text: str, snippet: str\\n    ) -> list[tuple[int, int, int, int]]:\\n        \\\"\\\"\\\"Find all occurrences of code snippet in text, return positions as (start_line, start_col, end_line, end_col)\\\"\\\"\\\"\\n        snippet = snippet.strip()\\n        positions = []\\n\\n        # Search for all occurrences of snippet\\n        start_pos = 0\\n        while True:\\n            start_pos = text.find(snippet, start_pos)\\n            if start_pos == -1:\\n                break\\n\\n            # Convert character position to line/column\\n            lines = text[:start_pos].split(\\\"\\\\n\\\")\\n            start_line = len(lines) - 1\\n            start_col = len(lines[-1])\\n\\n            # Calculate end position\\n            snippet_lines = snippet.split(\\\"\\\\n\\\")\\n            if len(snippet_lines) == 1:\\n                end_line = start_line\\n                end_col = start_col + len(snippet)\\n            else:\\n                end_line = start_line + len(snippet_lines) - 1\\n                end_col = len(snippet_lines[-1])\\n\\n            positions.append((start_line, start_col, end_line, end_col))\\n            start_pos += 1  # Move past this occurrence\\n\\n        if not positions:\\n            logfire.warn(f\\\"Could not find snippet '{snippet}' in text\\\")\\n        else:\\n            logfire.debug(f\\\"Found {len(positions)} occurrences of '{snippet[:20]}...'\\\")\\n\\n        return positions\\n\\n    async def debounced_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Debounced analysis that cancels previous calls\\\"\\\"\\\"\\n        # Cancel any existing task for this URI\\n        if uri in self._pending_tasks:\\n            self._pending_tasks[uri].cancel()\\n            _ = self._pending_tasks.pop(uri, None)\\n\\n        # Create new task\\n        task: asyncio.Task[None] = asyncio.create_task(self._delayed_analyze(uri, text))\\n        self._pending_tasks[uri] = task\\n\\n        try:\\n            await task\\n        except asyncio.CancelledError:\\n            logfire.debug(f\\\"Analysis cancelled for {uri}\\\")\\n        finally:\\n            # Clean up completed task\\n            if uri in self._pending_tasks and self._pending_tasks[uri] == task:\\n                _ = self._pending_tasks.pop(uri)\\n\\n    async def _delayed_analyze(self, uri: str, text: str):\\n        \\\"\\\"\\\"Wait for debounce period then analyze\\\"\\\"\\\"\\n        await asyncio.sleep(settings.debounce_ms / 1000.0)\\n        diagnostics = await self.analyze_document(uri, text)\\n        # pyrefly: ignore [missing-attribute]\\n        self.publish_diagnostics(uri, diagnostics)\\n        logfire.debug(f\\\"Published {len(diagnostics)} diagnostics for {uri}\\\")\\n\\n    async def analyze_document(self, uri: str, text: str) -> list[Diagnostic]:\\n        # Ensure we have a lock for this URI\\n        lock = self._analysis_locks.setdefault(uri, asyncio.Lock())\\n        async with lock:\\n            with logfire.span(\\\"analyze_document\\\", uri=uri):\\n                logfire.info(f\\\"Starting AI analysis for {uri}\\\")\\n\\n                try:\\n                    file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n                    language = self._detect_language(file_path)\\n\\n                    prompt = f\\\"\\\"\\\"Analyze this {language} code for issues:\\n\\n```{language}\\n{text}\\n```\\n\\nFile: {file_path.name}\\\"\\\"\\\"\\n\\n                    result = await self.agent.run(prompt)\\n                    logfire.info(\\n                        f\\\"AI analysis completed, found {len(result.output.issues)} issues\\\"\\n                    )\\n\\n                    diagnostics = []\\n                    for issue in result.output.issues:\\n                        positions = self.find_snippet_in_text(text, issue.issue_snippet)\\n\\n                        if positions:\\n                            start_line, start_col, end_line, end_col = positions[0]\\n                            diagnostic = Diagnostic(\\n                                range=Range(\\n                                    start=Position(\\n                                        line=start_line, character=start_col\\n                                    ),\\n                                    end=Position(line=end_line, character=end_col),\\n                                ),\\n                                message=f\\\"AI LSP: {issue.message}\\\",\\n                                severity=self._get_diagnostic_severity(issue.severity),\\n                                source=\\\"ai-lsp\\\",\\n                                data={\\n                                    \\\"issue_snippet\\\": issue.issue_snippet,\\n                                    \\\"suggested_fixes\\\": [\\n                                        {\\n                                            \\\"title\\\": fix.title,\\n                                            \\\"target_snippet\\\": fix.target_snippet,\\n                                            \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                        }\\n                                        for fix in (issue.suggested_fixes or [])\\n                                    ],\\n                                },\\n                            )\\n                            diagnostics.append(diagnostic)\\n\\n                    return diagnostics\\n\\n                except Exception as e:\\n                    logfire.error(\\n                        f\\\"AI analysis failed for {uri}: {str(e)}\\\", _exc_info=True\\n                    )\\n                    return []\\n\\n    def _get_diagnostic_severity(self, severity: str) -> DiagnosticSeverity:\\n        match severity:\\n            case \\\"error\\\":\\n                return DiagnosticSeverity.Error\\n            case \\\"warning\\\":\\n                return DiagnosticSeverity.Warning\\n            case \\\"info\\\":\\n                return DiagnosticSeverity.Information\\n            case \\\"hint\\\":\\n                return DiagnosticSeverity.Hint\\n            case _:\\n                return DiagnosticSeverity.Information\\n\\n    def _detect_language(self, file_path: Path) -> str:\\n        match file_path.suffix:\\n            case \\\".py\\\":\\n                return \\\"python\\\"\\n            case \\\".js\\\":\\n                return \\\"javascript\\\"\\n            case \\\".ts\\\":\\n                return \\\"typescript\\\"\\n            case \\\".jsx\\\":\\n                return \\\"jsx\\\"\\n            case \\\".tsx\\\":\\n                return \\\"tsx\\\"\\n            case \\\".rs\\\":\\n                return \\\"rust\\\"\\n            case \\\".go\\\":\\n                return \\\"go\\\"\\n            case \\\".java\\\":\\n                return \\\"java\\\"\\n            case \\\".cpp\\\":\\n                return \\\"cpp\\\"\\n            case \\\".c\\\":\\n                return \\\"c\\\"\\n            case \\\".lua\\\":\\n                return \\\"lua\\\"\\n            case _:\\n                return \\\"text\\\"\\n\\n\\nserver = AILanguageServer()\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_OPEN)\\nasync def did_open(params: DidOpenTextDocumentParams):\\n    logfire.info(f\\\"Document opened: {params.text_document.uri}\\\")\\n    doc = params.text_document\\n    await server.debounced_analyze(doc.uri, doc.text)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_SAVE)\\nasync def did_save(params: DidSaveTextDocumentParams):\\n    logfire.info(f\\\"Document saved: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(TEXT_DOCUMENT_DID_CHANGE)\\nasync def did_change(params: DidChangeTextDocumentParams):\\n    logfire.debug(f\\\"Document changed: {params.text_document.uri}\\\")\\n    doc = server.workspace.get_text_document(params.text_document.uri)\\n    await server.debounced_analyze(doc.uri, doc.source)\\n\\n\\n@server.feature(\\n    TEXT_DOCUMENT_CODE_ACTION,\\n    CodeActionOptions(code_action_kinds=[CodeActionKind.QuickFix]),\\n)\\nasync def code_actions(params: CodeActionParams) -> list[CodeAction]:\\n    with logfire.span(\\\"code_actions\\\", uri=params.text_document.uri):\\n        actions: list[CodeAction] = []\\n        uri = params.text_document.uri\\n        doc = server.workspace.get_text_document(uri)\\n\\n        for diagnostic in params.context.diagnostics:\\n            if diagnostic.source != \\\"ai-lsp\\\" or not diagnostic.data:\\n                continue\\n\\n            suggested_fixes = diagnostic.data.get(\\\"suggested_fixes\\\", [])\\n\\n            for fix_data in suggested_fixes:\\n                target_positions = server.find_snippet_in_text(\\n                    doc.source, fix_data[\\\"target_snippet\\\"]\\n                )\\n\\n                if target_positions:\\n                    with logfire.span(\\\"creating_quick_fix\\\", title=fix_data[\\\"title\\\"]):\\n                        start_line, start_col, end_line, end_col = target_positions[0]\\n                        fix_range = Range(\\n                            start=Position(line=start_line, character=start_col),\\n                            end=Position(line=end_line, character=end_col),\\n                        )\\n\\n                        edit = WorkspaceEdit(\\n                            changes={\\n                                uri: [\\n                                    TextEdit(\\n                                        range=fix_range,\\n                                        new_text=fix_data[\\\"replacement_snippet\\\"],\\n                                    )\\n                                ]\\n                            }\\n                        )\\n\\n                        action = CodeAction(\\n                            title=f\\\"Apply: {fix_data['title']}\\\",\\n                            kind=CodeActionKind.QuickFix,\\n                            edit=edit,\\n                            diagnostics=[diagnostic],\\n                            is_preferred=True,\\n                        )\\n                        actions.append(action)\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Regenerate AI fix\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Regenerate fix\\\",\\n                        command=\\\"ai-lsp.regenerateFix\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.data.get(\\\"issue_snippet\\\"),\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n            actions.append(\\n                CodeAction(\\n                    title=\\\"Dismiss AI suggestion\\\",\\n                    kind=CodeActionKind.QuickFix,\\n                    command=Command(\\n                        title=\\\"Dismiss\\\",\\n                        command=\\\"ai-lsp.dismiss\\\",\\n                        arguments=[\\n                            uri,\\n                            diagnostic.range.start.line,\\n                            diagnostic.range.start.character,\\n                        ],\\n                    ),\\n                    diagnostics=[diagnostic],\\n                )\\n            )\\n\\n        return actions\\n\\n\\n@server.command(\\\"ai-lsp.regenerateFix\\\")\\nasync def regenerate_fix(*args):\\n    if len(args) < 4:\\n        return\\n\\n    uri, issue_snippet, line, character = args[0], args[1], args[2], args[3]\\n\\n    with logfire.span(\\\"regenerate_fix\\\", uri=uri, issue_snippet=issue_snippet):\\n        logfire.info(\\n            f\\\"Regenerating fix for '{issue_snippet}' at {uri}:{line}:{character}\\\"\\n        )\\n\\n        doc = server.workspace.get_text_document(uri)\\n        file_path = Path(uri.replace(\\\"file://\\\", \\\"\\\"))\\n        language = server._detect_language(file_path)\\n\\n        prompt = f\\\"\\\"\\\"Analyze this specific code issue and provide NEW suggested fixes.\\n\\nIssue snippet: {issue_snippet}\\n\\nFull code context:\\n```{language}\\n{doc.source}\\n```\\n\\nFile: {file_path.name}\\n\\nFocus ONLY on the issue at the snippet shown. Provide fresh, alternative fixes if possible.\\\"\\\"\\\"\\n\\n        try:\\n            result = await server.agent.run(prompt)\\n\\n            if not result.output.issues:\\n                logfire.warn(\\\"No issues returned for regeneration\\\")\\n                return\\n\\n            regenerated_issue = result.output.issues[0]\\n\\n            current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n            updated_diags = []\\n\\n            for diag in current_diags:\\n                if (\\n                    diag.source == \\\"ai-lsp\\\"\\n                    and diag.range.start.line == line\\n                    and diag.range.start.character == character\\n                ):\\n                    positions = server.find_snippet_in_text(\\n                        doc.source, regenerated_issue.issue_snippet\\n                    )\\n                    if positions:\\n                        start_line, start_col, end_line, end_col = positions[0]\\n                        updated_diag = Diagnostic(\\n                            range=Range(\\n                                start=Position(line=start_line, character=start_col),\\n                                end=Position(line=end_line, character=end_col),\\n                            ),\\n                            message=f\\\"AI LSP: {regenerated_issue.message}\\\",\\n                            severity=server._get_diagnostic_severity(\\n                                regenerated_issue.severity\\n                            ),\\n                            source=\\\"ai-lsp\\\",\\n                            data={\\n                                \\\"issue_snippet\\\": regenerated_issue.issue_snippet,\\n                                \\\"suggested_fixes\\\": [\\n                                    {\\n                                        \\\"title\\\": fix.title,\\n                                        \\\"target_snippet\\\": fix.target_snippet,\\n                                        \\\"replacement_snippet\\\": fix.replacement_snippet,\\n                                    }\\n                                    for fix in (regenerated_issue.suggested_fixes or [])\\n                                ],\\n                            },\\n                        )\\n                        updated_diags.append(updated_diag)\\n                        logfire.info(\\n                            f\\\"Regenerated diagnostic with {len(regenerated_issue.suggested_fixes or [])} new fixes\\\"\\n                        )\\n                else:\\n                    updated_diags.append(diag)\\n\\n            server.publish_diagnostics(uri, updated_diags)\\n\\n        except Exception as e:\\n            logfire.error(f\\\"Failed to regenerate fix: {str(e)}\\\", _exc_info=True)\\n\\n\\n@server.command(\\\"ai-lsp.dismiss\\\")\\ndef dismiss_suggestion(*args):\\n    if len(args) < 3:\\n        return\\n\\n    uri, line, character = args[0], args[1], args[2]\\n\\n    with logfire.span(\\\"dismiss_suggestion\\\", uri=uri, line=line, character=character):\\n        logfire.info(f\\\"Dismissing suggestion at {uri}:{line}:{character}\\\")\\n\\n        doc = server.workspace.get_text_document(uri)\\n        diagnostics = []\\n\\n        current_diags = getattr(server.lsp, \\\"diagnostics\\\", {}).get(uri, [])\\n        for diag in current_diags:\\n            if (\\n                diag.source == \\\"ai-lsp\\\"\\n                and diag.range.start.line == line\\n                and diag.range.start.character == character\\n            ):\\n                continue\\n            diagnostics.append(diag)\\n\\n        server.publish_diagnostics(uri, diagnostics)\\n\\n\\ndef app():\\n    server.start_io()\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    app()\\n\\n```\\n\\nFile: main.py\", \"role\": \"user\", \"gen_ai.message.index\": 0, \"event.name\": \"gen_ai.user.message\"}, {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"pyd_ai_8898e2a43d8247c0b62817881e5b2d27\", \"type\": \"function\", \"function\": {\"name\": \"final_result\", \"arguments\": {\"issues\": [{\"issue_snippet\": \"find_snippet_in_text\", \"severity\": \"error\", \"suggested_fixes\": null, \"message\": \"The `find_snippet_in_text` method is used to locate AI-generated snippets (e.g., `issue_snippet` for diagnostics, `target_snippet` for fixes) within the document. This method performs a simple string search and returns the first match. If the AI provides a snippet that is not unique or is a common substring, the diagnostic or quick-fix will be applied to the wrong location. This can lead to mis-highlighted issues and potentially incorrect application of fixes. A more robust approach is required, such as the AI providing exact line/column ranges in its output, or a context-aware matching algorithm that considers the surrounding code.\"}, {\"message\": \"Accessing `server.lsp.diagnostics` via `getattr` relies on an internal, non-public attribute of the `pygls` LanguageServer. This is an architectural anti-pattern that makes the code fragile and prone to breaking with future updates to the `pygls` library. Diagnostics should be managed through the public API or within the `AILanguageServer` class's own state (e.g., a dedicated dictionary `self._diagnostics_cache`) and only published via `server.publish_diagnostics`.\", \"issue_snippet\": \"getattr\", \"suggested_fixes\": null, \"severity\": \"warning\"}, {\"issue_snippet\": \"file_path.name\", \"message\": \"The `file_path.name` variable is directly interpolated into the LLM prompt. While less critical than code content, unvalidated external input in a prompt can sometimes lead to prompt injection or unintended LLM behavior if the filename contains special characters or patterns recognized by the LLM as instructions or delimiters. It's a good security practice to sanitize or escape all user-controlled inputs provided to an LLM, even for seemingly innocuous fields like filenames.\", \"suggested_fixes\": [{\"replacement_snippet\": \"`{file_path.name}`\", \"target_snippet\": \"{file_path.name}\", \"title\": \"Escape filename in prompt to prevent misinterpretation\"}], \"severity\": \"info\"}]}}}], \"gen_ai.message.index\": 1, \"event.name\": \"gen_ai.assistant.message\"}, {\"content\": \"Final result processed.\", \"role\": \"tool\", \"id\": \"pyd_ai_8898e2a43d8247c0b62817881e5b2d27\", \"name\": \"final_result\", \"gen_ai.message.index\": 2, \"event.name\": \"gen_ai.tool.message\"}]"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\": \"object\", \"properties\": {\"all_messages_events\": {\"type\": \"array\"}, \"final_result\": {\"type\": \"object\"}}}"}}],"status":{}}]},{"scope":{"name":"logfire","version":"4.14.2"},"spans":[{"traceId":"019a79ae6ec92b27aa2061249d5709d4","spanId":"e53578fa6297cbe8","parentSpanId":"82f717034d300651","flags":256,"name":"AI analysis completed, found {len(result.output.issues)} issues","kind":1,"startTimeUnixNano":"1762978093818880000","endTimeUnixNano":"1762978093818880000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"9"}},{"key":"logfire.msg_template","value":{"stringValue":"AI analysis completed, found {len(result.output.issues)} issues"}},{"key":"logfire.msg","value":{"stringValue":"AI analysis completed, found 3 issues"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"146"}},{"key":"len(result.output.issues)","value":{"intValue":"3"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(result.output.issues)\":{}}}"}}],"status":{}},{"traceId":"019a79ae6ec92b27aa2061249d5709d4","spanId":"104d0d72478be1d0","parentSpanId":"82f717034d300651","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762978093820452000","endTimeUnixNano":"1762978093820452000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 4 occurrences of 'find_snippet_in_text...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"94"}},{"key":"len(positions)","value":{"intValue":"4"}},{"key":"snippet[:20]","value":{"stringValue":"find_snippet_in_text"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79ae6ec92b27aa2061249d5709d4","spanId":"6b560d662ff2b795","parentSpanId":"82f717034d300651","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762978093820822000","endTimeUnixNano":"1762978093820822000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 2 occurrences of 'getattr...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"94"}},{"key":"len(positions)","value":{"intValue":"2"}},{"key":"snippet[:20]","value":{"stringValue":"getattr"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79ae6ec92b27aa2061249d5709d4","spanId":"f2c3ef5a237fa3be","parentSpanId":"82f717034d300651","flags":256,"name":"Found {len(positions)} occurrences of '{snippet[:20]}...'","kind":1,"startTimeUnixNano":"1762978093821037000","endTimeUnixNano":"1762978093821037000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Found {len(positions)} occurrences of '{snippet[:20]}...'"}},{"key":"logfire.msg","value":{"stringValue":"Found 2 occurrences of 'file_path.name...'"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.find_snippet_in_text"}},{"key":"code.lineno","value":{"intValue":"94"}},{"key":"len(positions)","value":{"intValue":"2"}},{"key":"snippet[:20]","value":{"stringValue":"file_path.name"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(positions)\":{},\"snippet[:20]\":{}}}"}}],"status":{}},{"traceId":"019a79ae6ec92b27aa2061249d5709d4","spanId":"82f717034d300651","flags":256,"name":"analyze_document","kind":1,"startTimeUnixNano":"1762978066121949000","endTimeUnixNano":"1762978093821154000","attributes":[{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer.analyze_document"}},{"key":"code.lineno","value":{"intValue":"130"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.msg_template","value":{"stringValue":"analyze_document"}},{"key":"logfire.msg","value":{"stringValue":"analyze_document"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"uri\":{}}}"}},{"key":"logfire.span_type","value":{"stringValue":"span"}}],"status":{}},{"traceId":"019a79aedb00a6216137c9ad79ca156b","spanId":"66bc26c955f31a3b","flags":256,"name":"Published {len(diagnostics)} diagnostics for {uri}","kind":1,"startTimeUnixNano":"1762978093824267000","endTimeUnixNano":"1762978093824267000","attributes":[{"key":"logfire.span_type","value":{"stringValue":"log"}},{"key":"logfire.level_num","value":{"intValue":"5"}},{"key":"logfire.msg_template","value":{"stringValue":"Published {len(diagnostics)} diagnostics for {uri}"}},{"key":"logfire.msg","value":{"stringValue":"Published 3 diagnostics for file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"code.filepath","value":{"stringValue":"src/ai_lsp/main.py"}},{"key":"code.function","value":{"stringValue":"AILanguageServer._delayed_analyze"}},{"key":"code.lineno","value":{"intValue":"124"}},{"key":"len(diagnostics)","value":{"intValue":"3"}},{"key":"uri","value":{"stringValue":"file:///Users/benomahony/Code/open_source/ai-lsp/src/ai_lsp/main.py"}},{"key":"logfire.json_schema","value":{"stringValue":"{\"type\":\"object\",\"properties\":{\"len(diagnostics)\":{},\"uri\":{}}}"}}],"status":{}}]}]}]}
